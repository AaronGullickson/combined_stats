[
["index.html", "Statistical Analysis in Sociology Preface", " Statistical Analysis in Sociology Aaron Gullickson 2020-07-29 Preface This online textbook combines all of the information taught in my undergraduate statistics course as well as both terms of my introductory graduate statistics course. The undergraduate course consists of the first five chapters/modules up to and including Building Models. I have put a lot of work into this book, but it is still very much a work in progress, so you may notice typos and other errors on occasion. This textbook is designed to be used with the R statistical software program. If you are taking this course from me, then you will be running R through the RStudio Cloud. Various snippets of R code are interspersed throughout the book in order to show you how to do things. You will also find useful information in the appendices of this book. © Aaron Gullickson, 2017 This material is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International Public License. Users are free to remix tweak, and build upon this work for non-commercial purposes, although new work must acknowledge the original author and use the same license. Full license is available here. "],
["understanding-data.html", "Understanding Data", " Understanding Data In this first module, we will cover what it actually means to have “data” and give a broad overview of what kinds of things we can do with data. Data are the foundation of any statistical analysis and most data that we use in the social sciences consist of variables measured on some observations. In the next two sections, we will learn more about these concepts. Slides for this module can be found here. "],
["what-does-data-look-like.html", "What Does Data Look Like?", " What Does Data Look Like? The data that we look at typically take the format of a “spreadsheet” with rows and columns. The table below shows some characteristics of four randomly drawn passengers from the Titanic, in this type of spreadsheet format. Table 1: Data on four passengers from the Titanic survival sex age agegroup pclass fare family Survived Female 24.0000 Adult First 69.3000 0 Died Male 24.0000 Adult Third 7.7958 0 Survived Male 0.9167 Child First 151.5500 3 Died Male 60.0000 Adult First 26.5500 0 Clearly, we can see variation in who survived and died, the passenger classes they were housed in, gender, and age. We also have a measure of the fare they paid for the trip (in English pounds) and the number of family members traveling with them. To understand how to think about data, we need to understand the concepts of an observation and a variable and the distinction between them. The observations The observations are what you have on the rows of your dataset. In the Titanic example, the observations are individual passengers on board the Titanic, but observations can take many different forms. We use the term unit of analysis to distinguish what kind of observation you have in your dataset. If you are interviewing individual people and recording their responses, then the unit of analysis is individual people. If you are collecting cross-national data by country, then the unit of analysis would be a country. If you are analyzing data on the “best colleges in the US” then the unit of analysis is a university/college. The most common unit of analysis that we will see in this course is an individual person, but several of our datasets involve other units of analysis and it is important to keep in mind that an observation can be many different kinds of things. The variables The variables are what you have on the columns of your dataset. Variables measure specific attributes of your observations. If you conduct a survey of individual people and ask them for their age, gender, and education, then these three attributes would be recorded as variables in your dataset. We refer to them as “variables” because they can take different values across the observations. If you were to conduct a survey of individual people and ask your respondents if they are human, then you probably wouldn’t have a proper variable because everyone would likely respond “yes” and there would be no variation (although we can’t necessarily rule out jedis. There are two major types of variables. Some variables measure quantities of something and thus can be represented by a number. We refer to these as quantitative variables. Other variables indicate a category to which the observation belongs. We refer to these as categorical variables. Quantitative variables Quantitative variables measure quantities of something. A person’s height, a worker’s hourly wage, the number of children that a woman has given birth to, a country’s gross domestic product, a US State’s poverty rate, and the percent of a university’s student body that are women are all examples of quantitative variables. They can all be represented by a number which indicates how much of the thing the observation has. There are two important sub-types of quantitative variables. Discrete variables can logically only take certain values within a range, while continuous variables can logically take any value within a range. The most common example of a discrete variable is a count variable. The number of children that a woman has given birth to is an example of a count variable. This number can only take the value of whole numbers (integers) such as 0, 1, 2, 3, and so on. It makes no sense if a respondent says they have given birth to 2.5 children. Count variables are discrete variables because only whole numbers are logical responses. A person’s height is an example of a continuous variable. It is true that we typically measure height only down to a certain level of precision, typically inches in the United States. We might think that if we were to measure a person’s height in inches, it would only take whole number values and therefore it is discrete. But limitations in measurement don’t define whether a variable is continuous or discrete. Rather the distinction is whether the value could be logically measured to any degree of accuracy. We often measure height out to half inches and we could imagine that if we have a precise enough measurement instrument, we could measure a person’s height out to any decimal level that we desired. So, it is perfectly sensible for someone to say they were 69.825467 inches tall, even though we might think they are being a bit tedious. Note that in both the height and the number of children examples, there are logical limits to the values. You can’t have negative children or height. There are no exact upper limits to the values that either can take, but we would likely think we have a data coding error if we saw a report of a 20 foot person or a woman who gave birth to 50 children. This is what I mean by the statement “within a range” above. Both discrete and continuous variables can be limited in the range of values that they can take. What distinguishes them from each other is what values they can logically take within that limited range. Categorical variables Categorical variables are not represented by numerical quantities but rather by a set of mutually exclusive categories to which observations can belong. The gender, race, political party affiliation, and highest educational degree of a person, the public/private status of a university, and the passenger class of a passenger on the Titanic are all examples of categorical variables. There are also two sub-types of categorical variables. Ordinal variables are categorical variables where the categories have an explicit ordered structure, while nominal variables are categorical variables where the categories are unordered. Highest educational degree is an example of an ordinal variables because it is ordered such that Graduate Degree &gt; BA Degree &gt; AA Degree &gt; High School Diploma &gt; No degree. Passenger class is also an ordinal variables that starts in Third class (or steerage - Think Leonardo DiCaprio) and ends in First class (think Kate Winslet), with a Second class in between. Race, gender, and political party affiliation are all examples of nominal variables. The categories here have no ordering to them. While some people might have their own political party preferences, these sort of normative evaluations of categories are irrelevant. For the same reason, even the variable of survival on the Titanic is a nominal variable. We don’t judge the value of life and death. "],
["what-can-we-do-with-data.html", "What Can We Do With Data?", " What Can We Do With Data? We now know what data looks like, but what do social scientists do with data? in the first part of this course, we will learn three fundamental data analysis tasks: analysis of the distribution of a single variable, measuring association, and statistical inference. In the final part of the course, we will build on these fundamentals to learn how to build more complex statistical models. How is a variable distributed? Sometimes, we just want to understand what a single variable “looks like.” We may simply be interested in its “average” or we may want to know something else, like how spread out the values of the variable are. In these cases, we calculate univariate (\"one variable) statistics on the distribution of a variable. Typically, univariate statistics aren’t as interesting to social scientists as the measures of association discussed below, but even then its often a good idea to look at univariate statistics to understand all of the variables in your research. In some cases, the calculation of a univariate statistics is the important question at hand. For example, when poll researchers try to figure out who is going to win an election, they are very much interested in the univariate distribution of support for each candidate, which gives the proportions of likely voters who intend to vote for each candidate. Here are some other questions we could ask in our data: How much variability is there in the amount of money that movies make? What percent of passengers survived the Titanic disaster? What is the average age of voters in the United States? Measuring association Social scientists are often most interested in the relationships, or association, between two or more variables. These associations allow us to test hypotheses about causal relationships between underlying social constructs. For example, we might be interested in whether divorce affected children’s well-being. In this case, we would want to look at the relationship between the categorical variable indicating whether a child’s parents were divorced and some measure of their well-being, such as feelings of stress, academic performance, etc. There are different ways of measuring association depending on the types of variables involved.. Here are some questions about association we could ask in our data: Did the probability of surviving the Titanic depend on passenger class? (categorical and categorical) Do the earnings of movies vary by genre? (quantitative and categorical) Is income inequality in a state related to its crime rate? (quantitative and quantitative) In the first part of the course, we will learn how basic measures of association between two variables, depending on what kind of variables we are using. In the final part of the course, we will return to this topic when we learn how to build more complex statistical models discussed below. Making statistical inferences If I told you that in my sample of twenty people, brown-eyed individuals make $5 more than all other eye colors combined, would you believe me? You probably shouldn’t, because in a sample of twenty people, even when drawn without systematic bias, weird results like this are not unlikely just by random chance. If I told you I observed this phenomenon on a well-drawn sample of 20,000 individuals, you would probably be more likely to believe me. The underlying concept here is called statistical inference. We often want to draw conclusions about the larger populations from which our samples are drawn. Statistical inference is the technique of quantifying how uncertain we are about whether our data are similar to the population or not. When you hear press reports on political polls with the term “margin of error,” they are referring to statistical inference. Many introductory statistics course focus most of their attention on statistical inference, partly because it is more abstract and complex. However, statistical inference is always secondary to the basic descriptive measures of univariate, bivariate, and multivariate statistics. Therefore, I spend considerably less time on this topic than in most statistics courses, so that we can focus on the more important stuff. Building Models Although our basic measures of association are useful, the most common tool in social science analysis is a statistical model in which the user can specify the relationships between variables by some kind of mathematical functions. In the final module of the course, we will learn how to build basic versions of these models that allow us to look at the relationships between multiple quantitative and categorical variables. This section will build on our prior work in all of the previous modules. We will specifically focus on two uses of statistical models. First, statistical models will allow us to “control” for other variables when we look at the association between any two variables. Controlling for other variables is important because they may be confounded with the relationship we want to measure. For example, we may be interested in the relationship between marital status (e.g. never married, married, widowed, divorced) and sexual frequency in our GSS data. However, these different groups vary significantly in their age. Never married individuals are much younger than all of the other groups and widowed individuals much older. Given the fact that sexual frequency tends to decline with age (something we will show later in this term), it seems problematic to just compare the average sexual frequency across these groups. This is because age confounds the relationship between marital status and sexual frequency. Statistical models will gives us tools to account for this problem and to get a better estimate of the relationship between marital status and sexual frequency, net of this confounder. Second, statistical models will allow us to account for how the relationship between two variables might differ depending on the context of a third variable. This is what we call an interaction. For example, lets say we were interested in the relationship between the number of sports played and a student’s popularity (measured by friend nominations) in our Add Health data. Because of gender norms, we might expect that this relationship is different for boys and girls. We can use statistical models to empirically examine whether this is true. This kind of contextualization is an important component of sociological practice. Observational Data, Experimental Thinking Much of the data that we use in sociology is observational rather than experimental. In an experimental design, the researcher randomly selects subjects to receive some sort of treatment and then observes how this treatment affects some outcome. Thus, the research engages in systematic manipulation to observe a response. In observational data, the researcher does not directly manipulate the environment but rather just observes and records the social setting as it is. Experimental data can be more powerful than observational data because the random assignment of a treatment through researcher manipulation strengthens claims of causation. If there is a relationship between treatment and response it can only come through a causal relationship or random chance. In observational data, the relationship between any two variables can be a result of a causal relationship, random chance,spuriousness. Spuriousness occurs when other variables produce the relationship between two other variables rather than them directly causing each other. The example above about marital status and sexual frequency is a simple example. If we note that widows have less sex than other people, we may be tempted to think that something about being widowed reduces someone’s sexual drive or their interactions with others. However, the more obvious explanation is that widows tend to be quite a bit older than other marital status groups and older people have less sex. Age is generating a spurious relationship between widowhood and sexual frequency. This kind of spuriousness is the reason for the frequent claim that “correlation is not causation.” There are two different philosophical approaches to the statistical analysis of observational data where spuriousness can be a problem. The first approach approaches it as pseudo-experimental data. The goal of this approach is to try to find ways to mimic the experimental design approach with observational data. At a basic level this can include “controlling” for other variables (which we will learn) and can extend to a variety of techniques of causal modeling that are intending to use some feature of the data to recover causation (which we will not learn). The second approach treats statistical analysis as a way to describe observed data in a formal, systematic, and replicable way. The goal is to establish to what extent the data are consistent with competing theories that seek to understand the outcome in question, rather than to mimic the experimental approach. Although quantitative and qualitative approaches are often seen as philosophically different approaches, this approach to observational data shares many features with more purely qualitative approaches to data analysis. This is the approach that I take in this course. "],
["the-distribution-of-a-variable.html", "The Distribution of a Variable", " The Distribution of a Variable The distribution of a variable refers to how the different values of that variable are spread out across observations. This distribution gives us a sense of what the most common values are for a given variable and how much these values vary. It can also alert us to unusual patterns in the data. Intuitively, we think about distributions in our personal life any time we think about how we “measure” up relative to everyone else on something (income, number of Facebook friends, GRE scores, etc.). We want to know where we “fall” in the distribution of one of these variables. In this chapter we will first learn graphical techniques that allow us to visualize what the distribution of a variable looks like. For quantitative variables we will then move on to calculate important summary statistics that measure the center and spread of a distribution. Slides for this module can be found here. "],
["looking-at-distributions.html", "Looking at Distributions", " Looking at Distributions One of the best ways to understand the distribution of a variable is to visualize that distribution with a graph. However, the technique we use to graph the distribution will depend on whether we have a categorical or a quantitative variable. For categorical variables, we will use a barplot, while for quantitative variables, we will use a histogram. Looking at the distribution of a categorical variable Calculating the frequency In order to graph the distribution of a categorical variable, we first need to calculate its frequency. The frequency is just the number of observations that fall into a given category of a categorical variable. We could, for example, count up the number of passengers who were in the various passenger classes on the Titanic. Doing so would give us the following: Table 2: Passengers on the Titanic by passenger class Passenger class Frequency First 323 Second 277 Third 709 Total 1309 There were 323 first class passengers, 277 second class passengers, and 709 third class passengers. Adding those numbers up gives us 1,309 total passengers. R will calculate these numbers for us easily using the table command: table(titanic$pclass) ## ## First Second Third ## 323 277 709 Frequency, proportion, and percent The frequency we just calculated is sometimes called the absolute frequency because it just counts the raw number of observations. However such raw numbers are not usually very helpful because they will vary by the overall number of observations. Instead, we typically want to calculate the proportion of observations that fall within each category. These proportions are sometimes also called the relative frequency. We can calculate the proportion by simply dividing our absolute frequency by the total number of observations: Table 3: Passengers on the Titanic by passenger class Passenger class Frequency Proportion First 323 323/1309=0.247 Second 277 277/1309=0.212 Third 709 709/1309=0.542 Total 1309 1.0 R provides a nice shorthand function titled prop.table to conduct this operation. The prop.table command should be run on the output from a table command. prop.table(table(titanic$pclass)) ## ## First Second Third ## 0.2467532 0.2116119 0.5416348 Note that I have “wrapped” the prop.table command around the table command here to do this calculation in a single line. We often convert proportions to percents which are more familiar to most people. To convert a proportion to a percent, just multiply by 100: Table 4: Passengers on the Titanic by passenger class Passenger class Frequency Proportion Percent First 323 323/1309=0.247 0.247*100=24.7% Second 277 277/1309=0.212 0.212*100=21.2% Third 709 709/1309=0.542 0.542*100=54.2% Total 1309 1.0 100% 24.7% of passengers were first class, 21.2% of passengers were second class, and 54.2% of passengers were third class. Just over half of passengers were third class and the remaining passengers were fairly evenly split between first and second class. Try it out In the interactive R session below, try to calculate the proportion or percent of the voting age population that voted for each candidate (or didn’t vote) in the 21016 election, based on our politics dataset. Keep the following in mind: The politics dataset is already loaded. Remember that to access a variable in a dataset, you need to use the $ sign. In this case, the variable you want is called president so you need to use the syntax politics$president. You need to run the table command on your variable before you can run prop.table. Is the result surprising to you? Notice that no candidate received even close to 50%. Why is this the case? Don’t make a piechart Now that we have proportions/percents, we can use these values to construct a graphical display of the distribution. One of the most common techniques for doing this is a piechart. Figure 1 shows a piechart of the distribution of passengers on the Titanic. Figure 1: Piechart of passenger class distribution on Titanic You will notice that I do not show you the code for constructing this piechart. I hid this code because I don’t ever want you to construct a piechart. Despite their popularity, piecharts are a poor tool for visualizing distributions. In order to judge the relative size of the slices on a piechart, your eye has to make judgments in two dimensions and with an unusual pie-shaped slice (\\(\\pi*r^2\\) anyone?). As a result, it can often be difficult to decide which slice is bigger and to properly evaluate the relative sizes of each of the slices. In this case, for example, the relative size of first and second class are quite close and it is not immediately obvious which category is larger. Make a barplot A better way to display the distribution is by using a barplot in which vertical or horizontal bars give the proportion or percent. Figure 2 shows a barplot for the distribution of passengers on the Titanic. ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ labs(x=&quot;passenger class&quot;, y=&quot;percent&quot;)+ scale_y_continuous(labels=scales::percent)+ coord_flip()+ theme_bw() Figure 2: Barplot of passenger class distribution on the Titanic Figure 2 is our first code example of the ggplot library that we will use to make figures in R. I will discuss how this code works below, but first I want to focus on the figure itself. Unlike the piechart, our eye only has to work in one dimension (in this case, horizontal). We can clearly see that third class is the largest passenger class with slightly more than 50% of all passengers. We also can see easily determine visually that third class passengers are at more than twice as common as either of the other two categories, simply by comparing the height of the bars. We can also see that slightly more passengers were in first class than second class simply by comparing the heights of those two bars. So how did I create this graph? The ggplot syntax is a little different than most of the other R syntax we will look at this term. With ggplot, we add several commands together with the + sign to create our overall plot. Each command adds a “layer” to our plot. These layers are: ggplot(titanic, aes(x=pclass, y=..prop.., group=1)): The first command is always the ggplot command itself which defines the dataset we will use (in this case, titanic) and the “aesthetics” that are listed in the aes argument. In this case, we defined an x (horizontal) variable as the pclass variable itself and the y (vertical) variable as a proportion (which in ggplot-speak is ..prop..). The group=1 argument is a trick for barplots that makes sure our proportions add up to 1 across all categories. geom_bar(): This command defines what geometric shape (in this case a bar) to actually plot. These two layers would be enough for a figure, but I also add four more layers that make for a nicer looking figure. You can try the command above with only the first two elements to see how it is different. labs(x=\"passenger class\", y=\"percent\"): The labs command allows me to add a variety of nice labels to my graph. In this case I labeled my x and y axes. scale_y_continuous(labels=scales::percent): This is not a necessary command but it is useful to get better labels on my y-axis tickmarks. Without this command, the tickmarks would just show proportions (e.g. 0.2, 0.4). This command allows me define custom labels for those tickmarks that show them as percents rather than proportions. coord_flip: This does exactly what it sounds like. It flips the x and y axes of the graph. This causes the bars to display horizontally rather than vertically. This is not necessary, but is often a useful feature to avoid problems with long category names overlapping on the x-axis. Note that even though passenger class looks like it is on the y-axis, ggplot still treats it as the “x” variable for things like defining aesthetics and labeling with the labs layer. theme_bw(): This last command is for the “theme” layer that just defines a variety of characteristics for the overall look of the figure. I like the theme_bw (bw for black and white) theme over the default theme that comes with ggplot. Ggplot is a very flexible system that repeats these same basic layer elements in all the graphs it creates. In an appendix to this book, you can see cookbook examples of all the ways we will we use it to create figures throughout the term. Try it out In the interactive R session below, use ggplot to construct a barplot of the percent of respondents in each category of presidential election voting from the president variable. Keep the following in mind: You must combine together different elements of ggplot, in this case ggplot itself and geom_bar with a plus sign to display correctly. Be sure to add nice labels for your x and y axes by adding a labs command. Keep in mind that if you use coord_flip the axes will be reversed on your graph (i.e. the x-axis will represent the vertical axis). Looking at the distribution of a quantitative variable Barplots won’t work for quantitative variables because quantitative variables don’t have categories. However, we can do something quite similar with the histogram. One way to think about the histogram is that we are imposing a set of categories on a quantitative variable by breaking our quantitative variable into a set of equally wide intervals that we call bins. The most important decision with a quantitative variable is how wide to make these bins. As an example, lets take the age variable from our politics dataset. I could break this variable into five-year intervals that go from 0-5, 5-10, 10-15, 15-20, and so on. Alternatively, I could use 10-year intervals from 0-9, 10-19, 20-29, and so on. I could also use any other interval I like such as 1-year, 3-year, and so on. Lets use 10-year intervals for this example. I then just have to count up the number of observations that fall into each 10 year interval. Table 5: Age distribution in politics datatset in 10 year groups Age Group Frequency 0-9 0 10-19 127 20-29 616 30-39 756 40-49 650 50-59 822 60-69 745 70-79 369 80-89 153 Because the survey was only administered to adults, I have zero individuals from 0-9 and only a few in the 10-19 age range. Now that we have the frequencies for each bin, we can plot the histogram. The histogram looks much like a barplot except for two important differences. First, on the x-axis we have a numeric scale for the quantitative variable rather than categories. Second, we don’t put any space between our bars. Below I show some R code for producing the histogram shown in Figure 3 using ggplot. This code is simpler than the barplot code. We only need to define an x aesthetic in our ggplot command. Instead of a geom_bar we use a geom_histogram. I can specify different bin widths in this command with the binwidth argument. Here I have specified a binwidth of ten years. I have also specified two different colors. The col argument is the color of the border for each bar and the fill argument is the color of the bars themselves. ggplot(politics, aes(x=age))+ geom_histogram(binwidth=10, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ theme_bw() Figure 3: Histogram of age in the politics data Looking at Figure 3, I can see the peak in the distribution in the 50’s (the baby boomers) with a long fat tail to the right and a steeper drop off at older ages, due to smaller cohorts and old age mortality.I can also sort of see a smaller peak in the 25-34 range which is largely the children of the baby boomers. What would this histogram look like if I had used 5-year bin widths instead of 10-year bins? Lets try it: ggplot(politics, aes(x=age))+ geom_histogram(binwidth=5, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ theme_bw() Figure 4: Histogram of age in the politics data with five year bins As Figure 4 shows, I get more or less the same overall impression but a more fine-grained view. I can more easily pick out the two distinct “humps” in the distribution that correspond to the baby boomers and their children. Sometimes adjusting bin width can reveal or hide important trends and sometimes it can just make it more difficult to visualize the distribution. As an exercise, you can play around with the interactive example below. Try it out In the interactive R session below, use ggplot to construct a histogram of the distribution of family income from the politics dataset using the income variable. Keep the following in mind: You must combine together different elements of ggplot, in this case ggplot itself and geom_histogram with a plus sign to display correctly. Play around with the size of bins in your histogram by adding a binwidth argument to geom_histogram. Play around with the border and fill color of your histogram bars with the col and fill arguments to geom_histogram. Be sure to add nice labels for your x axis by adding a labs command. What kinds of things should you be looking for in your histogram? There are four general things to be on the lookout for when you examine a histogram. Center Where is the center of the distribution? Loosely we can think of the center as the peak in the data, although we will develop some more technical terms for center in the next section. Some distributions might have more than one distinct peak. When a distribution has one peak, we call it a unimodal distribution. Figure 5 shows a clear unimodal distribution for the runtime of movies. We can clearly see that the peak is somewhere between 90 and 100 minutes. When a distribution has two distinct peaks, we call it a bimodal distribution. Figure 6 shows that the distribution of violent crime rates across states is bimodal. The first peak is around 2200-2700 crimes per 100,000 and the second is around 3500 crimes per 100,000. This suggests that there are two distinctive clusters of states: in the first cluster are states with a moderate crime rate and in the second cluster, states with high crime rate states. You can probably guess what we call a distribution with three peaks (and so on), but its fairly rare to see more than two distinct peaks in a distribution. Figure 5: The distribution of movie runtimes is unimodal with one clear peak around 90-100 minutes Figure 6: The distribution of property crimes by states is bimodal with a two separate peaks. Shape Is the shape symmetric or is one of the tails longer than the other? When the long tail is on the right, we refer to this distribution as right skewed. When the long tail is on the left, we refer to the distribution as left skewed. Figures 7 and 8 show examples of roughly symmetric and heavily right-skewed distributions, respectively, from the movies dataset. The Tomato Rating that movies receive (a score from 1-10) is roughly symmetric with about equal numbers of movies above and below the peak. Box office returns on the other hand are heavily right-skewed. Most movies make less than $100 million at the box office but there are few “blockbusters” that rake in far more. Right-skewness to some degree or another is common in social science data, partially because many variables can’t logically have values below zero and thus the left tail of the distribution is truncated. Left-skewed distributions are rare. In fact, they are so rare that I don’t really have a very good example to show you from our datasets. Figure 7: The distribution of movie tomato ratings is roughly symmetric Figure 8: The distribution of movie box office returns is heavily right skewed Spread How spread out are the values around the center? Do they cluster tightly around the center or are they spread out more widely? Typically this is a question that can only be asked in relative terms. We can only say that the spread of a distribution is larger or smaller than some comparable distribution. We might be interested for example in the spread of the income distribution in the US compared to Sweden, because this spread is one measure of income inequality. Figure 9 compares the distribution of movie runtime for comedy movies and sci-fi/fantasy movies. The figure clearly shows that the spread of movie runtime is much greater for sci-fi/fantasy movies. Comedy movies are all tightly clustered between 90 and 120 minutes while longer movies are more common for sci-fi/fantasy movies leading to a longer right tail and a correspondingly higher spread. Figure 9: The distribution of movie runtime is much more spread out for sci-fi/fantasy films than it is for comedies. Outliers Are there extreme values which fall outside the range of the rest of the data? We want to pay attention to these values because they may have a strong influence on the statistics that we will learn to calculate to summarize a distribution. They might also influence some of the measures of association that we will learn later. Finally, extreme outliers may help identify data coding errors. In Figure 6 above, we can see a clear outlier in the violent crime rate for Washington DC. Washington DC’s crime rate is such an outlier relative to the other states that we will pay attention to it throughout this term as we conduct our analysis. Figure 9 above shows that Peter Jackson’s Return of the King was an outlier for the runtime of sci-fi/fantasy movies, although it doesn’t seem quite as extreme as for the case of Washington DC. In large datasets, it can sometimes be very difficult to detect a single outlier on a histogram because the height of its bar will be so small. Later in this chapter, we will learn another graphical technique called the boxplot that can be more useful for detecting outliers in a quantitative variable. "],
["measuring-the-center-of-a-distribution.html", "Measuring the Center of a Distribution", " Measuring the Center of a Distribution When we look at a distribution, we often can get an intuitive sense of where its “center” is. But what do we really mean by the term “center?” The notion of center often allows us to think about the value we expect a typical or “average” observation to have, but there are multiple ways of defining this center. In statistics, three different measures of center are used: the mean, median, and mode. The mean The mean is the measure most frequently referred to as the “average” although that term could apply to the median and mode as well. The mean is the balancing point of a distribution. Imagine trying to balance a distribution on your finger like a basketball. Where along the distribution would you place your finger to achieve this balance? This point is the mean. It is equivalent to the concept of “center of mass” from physics. The calculation of the mean is straightforward: Sum up all the values of your variable across all observations. Divide this sum by the number of observations. As an example, lets take a sub-sample of our movie data. I am going to select all the romances (not including rom-coms which are coded as comedies) produced in 2013. There were nine “pure” romances in 2013. I want to know their mean Tomato Meter rating. Table 6: Tomato meter of pure romance movies released in 2013 Title Tomato Meter Kill Your Darlings 77 I’m in Love with a Church Girl 6 Safe Haven 12 The Face of Love 43 Love and Honor 13 Before Midnight 98 Drinking Buddies 83 Ain’t Them Bodies Saints 79 The Great Gatsby 49 Sum 460 In the very last row, I show the sum of the Tomato Meter rating which simply sums up the Tomato Meter rating of each movie. To calculate the mean, I simply divide this sum by the number of movies which is 9. \\[\\bar{x}=\\frac{460}{9}=51.11\\] The mean Tomato Meter rating for romantic movies in 2013 was 51.11. Notice the funny \\(x\\) with a bar over it (“x bar”). Mathematically, we often represent variables with lower-case roman letters like \\(x\\) or \\(y\\). Putting the bar above the letter is the way to mathematically signify the mean of that variable. Since we are discussing math symbols, lets talk about creating a mathematical formula for what we just did. In order to do that, I need to introduce a variety of mathematical symbols, but don’t get frightened. We are just formalizing the method of calculating the mean that I just demonstrated above. First, as I said we represent a given variable by a lower-case roman letter, such as \\(x\\). If we want to specify a particular observation of \\(x\\), we use a subscript number. So \\(x_1\\) is the value of the first observation of \\(x\\) in our data and \\(x_{25}\\) is the value of the 25th observation of \\(x\\) in our data. We use the letter n to signify the number of observations so \\(x_n\\) is always the last observation of \\(x\\) in our data. Now we need some way to indicate “sum up all the values of \\(x\\).” This is given by the summation sign which looks as follows: \\[\\sum_{i=1}^n x_i\\] In English, this just means “sum up all the values of \\(x\\), starting at \\(x_1\\) and ending at \\(x_n\\).” That gives us our sum, which we just need to divide by the number of observations, \\(n\\). \\[\\bar{x}=\\frac{\\sum_{i=1}^n x_i}{n}\\] In R, the mean is very straightforward to calculate. Lets calculate the mean of the Tomato Meter rating for all movies in our dataset: sum(movies$TomatoMeter)/nrow(movies) ## [1] 47.77595 The sum command calculates the sum and the nrow command calculates the number of rows of our dataset, which is equivalent to the number of observations. Or we could go even simpler and just use the mean command: mean(movies$TomatoMeter) ## [1] 47.77595 We can see that the mean Tomato Meter of romantic movies in 2013 (51.1) were slightly above the mean for all the movies in our dataset (47.8), but not by much. One last thing to note is that it only makes sense to calculate the mean of quantitative variables. You cannot add up the values for categorical variables because categorical values don’t have numeric values. Rather, they have categories. Notice that you will get an “NA” value and a warning if you try to do this using the mean command: mean(movies$Rating) ## Warning in mean.default(movies$Rating): argument is not numeric or logical: ## returning NA ## [1] NA The median The median is almost as widely used as the mean as a measure of center, for reasons I will discuss below. The median is the midpoint of the distribution. It is the point at which 50% of observations have lower values and 50% of the observations have higher values. In order to calculate the median, we need to first order our observations from lowest to highest. Lets do that with the romantic movie data above. Table 7: Tomato meter of pure romance movies released in 2013, sorted from worst to best Title Tomato Meter Order I’m in Love with a Church Girl 6 1 Safe Haven 12 2 Love and Honor 13 3 The Face of Love 43 4 The Great Gatsby 49 5 Kill Your Darlings 77 6 Ain’t Them Bodies Saints 79 7 Drinking Buddies 83 8 Before Midnight 98 9 To find the median, we have to find the observation right in the middle. When we have an odd number of observations, finding the exact middle is easy. In this case, it is given by the 5th observation because it has four observations lower than it and four observations higher than it. So the median Tomato Meter rating for romantic movies in 2013 was 49. When you have an even number of observations, then finding the median is slightly more tricky because you have no single observation that is exactly in the middle. In this case, you find the two observations that are closest to the middle and calculate their mean (sum them up and divide by two). For example, if we had ten observations, we would take the mean of the 5th and 6th observations to calculate the median. The median command in R will calculate the median for you. median(movies$TomatoMeter) ## [1] 47 In this particular case, the mean (47.8) and median (47) of the Tomato Meter produced very similar measures of center, but this isn’t always the case as I will demonstrate below. As for the mean, it makes no sense to calculate the median of a categorical variable. The mode The mode is the least used of the three measures of center. In fact, it is so infrequently used that R does not even have a built-in function to calculate it. The mode is the high point or peak of the distribution. When you look at a distribution graphically, the mode is what your eye is drawn to as a measure of center. Calculating the mode however is much trickier. Simply speaking, the mode is the most common value in the data. However, when data are recorded down to very precise decimal levels, this can be a misleading number. Furthermore, there may be multiple “peaks” in the data and so speaking of a single mode can be misleading. In the sample of 2013 romance movies, no value was repeated and so there is no legitimate value for the mode. In the case of the tomato meter distribution for the movies dataset, we have a tie for the most common value. 41 movies had Tomato Meter ratings of 29, 33, and 51, respectively. Figure 10 shows that the distribution of tomato meter ratings lacks any real peak, with a relatively even distribution across most values. ggplot(movies, aes(x=TomatoMeter))+ geom_histogram(binwidth=5, col=&quot;black&quot;, fill=&quot;grey&quot;)+ labs(x=&quot;age&quot;)+ geom_vline(xintercept = c(29,33,51), size=2, color=&quot;red&quot;)+ theme_bw() Figure 10: Histogram of Tomato Meter ratings from politics dataset with three candidates for the mode drawn in red Interestingly, while it does not make sense to think about a mean or a median for quantitative variables, we can use the idea of the mode for categorical variables. The most common category is often referred to as the modal category. If you want to quickly identify the modal category for a categorical variable, you can wrap a table command inside a sort command with the decreasing=TRUE argument to see the top category at the right. sort(table(movies$Rating), decreasing = TRUE) ## ## R PG-13 PG G ## 1143 991 363 56 R-Rated movies are the modal category for maturity rating in our movies dataset. Comparing the mean and median How can the mean and median give different results? Remember that the mean defines the balancing point and the median defines the midpoint. If you have a perfectly symmetric distribution, then these two points are the same because you would balance the distribution at the midpoint. However, when the distribution is skewed in one direction or another, the mean and the median will be different. In order to maintain balance, the mean will be pulled in the direction of the skew. When you have heavily skewed distributions, this can lead to dramatically different values for the mean and median. Lets look at this phenomenon for a couple of variables in the movie dataset. As the histogram above showed, the Tomato Meter variable is fairly symmetric and as a result we end up with a mean (47.8) and median (47) that are pretty close. Figure 11 shows the distribution of movie runtime which is somewhat more right-skewed. Figure 11: Distribution of movie runtime with mean (105.2) and median (102) shown as vertical lines The skew here is not too dramatic, but it pulls the mean about 3 minutes higher than the median. Figure 12 shows the distribution of box office returns to movies which is heavily right skewed. Most movies make moderate amounts of money, and then there are a few star performers that make bucket loads of cash. Figure 12: Distribution of movie box office returns with mean (45.2) and median (21.6) shown as vertical lines As a result of this skew, the mean box office returns are about $45.2 million, while the median box office returns are about $21.6 million. The mean here is more than double the median! Note that neither estimate is in some fundamental way incorrect. They are both correctly estimating what they were intended to estimate. It is up to us to understand and interpret these numbers correctly and to understand their limitations. In many cases, we are actually more interested in the median as a measure of “average” experience than the mean, even though we think of the mean as the “average.” This is, for example, why you see home prices in an area always reported in terms of medians rather than means. Mean home prices tend to be much higher than median home prices because of the relatively few very expensive homes in a given area. Try it out In the interactive R session below, Calculate the mean and median of hourly wages from the earnings dataset. Keep the following in mind: To identify the variable you want for the mean and median you need to follow the syntax of dataset$variable_name. How do the results for the mean and median compare? What does this suggest about the underlying shape of the distribution? "],
["percentiles-and-the-five-number-summary.html", "Percentiles and the Five Number Summary", " Percentiles and the Five Number Summary In this section, we will learn about the concept of percentiles. Percentiles will allow us to calculate a five number summary of a distribution and introduce a new kind of graph for describing a distribution called the boxplot. Percentiles We have already seen one example of a percentile. The median is the 50th percentile of the distribution. It is the point at which 50% of the observations are lower and 50% are higher. We can actually use this same logic to calculate other percentiles. We could calculate the 25th percentile of the distribution by finding the point where 25% of the observations are below and 75% are above. We could even calculate something like the 43rd percentile if we were so inclined. We calculate percentiles in a fashion similar to the median. First, sort the data from lowest to highest. Then, find the exact observation where X% of the observations fall below to find the Xth percentile. In some cases, there might not be an exact observation that fits this description and so you may have to take the mean across the two closest numbers. The quantile command in R will calculate percentiles for us in this fashion (quantile is a synonym for percentile). In addition to telling the quantile command which variable we want the percentiles of, we need to tell it which percentiles we want. In the command below, I ask for the 27th and 57th percentile of age in our sexual frequency data. quantile(sex$age, p=c(.27,.57)) ## 27% 57% ## 32 46 27% of the sample were younger than 32 years of age and 57% of the sample were younger than 46 years of age. The five number summary We can split our distribution into quarters by calculating the minimum(0th percentile), the 25th percentile, the 50th percentile (the median), the 75th percentile, and the maximum (100th percentile). Collectively, these percentiles are known at the quartiles of the distribution (not to be confused with quantile) and are also described as the five number summary of the distribution. We can calculate these quartiles with the quantile command. If I don’t enter in specific percentiles, the quantile command will give me the quartiles by default: quantile(sex$age) ## 0% 25% 50% 75% 100% ## 18 31 43 56 89 The bottom 25% of respondents are between the ages of 18-31. The next 25% are between the ages of 31-43. The next 25% are between the ages of 43-56. The top 25% are between the ages of 56-89. We can also use this five number summary to calculate the interquartile range (IQR) which is just the difference between the 25th and 75th percentile. This gives us a sense of how spread out observations are. In this data: \\[IQR=56-31=25\\] So, the 25th and 75th percentile of age are separated by 25 years. Boxplots We can also use this five number summary to create another graphical representation of the distribution called the boxplot. Figure 13 below shows a boxplot for the age variable from the sexual frequency data. Figure 13: Boxplot of respondent’s age in sexual frequency data The “box” in the boxplot is drawn from the 25th to the 75th percentile. The height of this box is equal to the interquartile range. The median is drawn as a thick bar within the box. Finally, “whiskers” are then drawn to the minimum and maximum of the data. Sometimes, the whiskers are drawn to less than the minimum and maximum if these values are very extreme and instead the whiskers are drawn out to 1.5xIQR in length and then individual points are plotted. In this case there were no extreme values, so the whiskers were drawn all the way out to the actual maximum and minimum. The boxplot provides many pieces of information. It shows the center of the distribution as measured by the median. It also gives a sense of the spread of the distribution and extreme values by the height of the box and whiskers. It can also show skewness in the distribution depending on where the median is drawn within the box and the size of the whiskers. If the median is in the center of the box, then that indicates a symmetric distribution. If the median is towards the bottom of the box, then the distribution is right-skewed. If the median is towards the top of the box, then the distribution is left-skewed. I have not yet shown you how to make a boxplot using ggplot. The code for Figure 13 is shown below. ggplot(sex, aes(x=&quot;&quot;, y=age))+ geom_boxplot(fill=&quot;skyblue&quot;)+ labs(x=NULL)+ theme_bw() Most of this code is straightforward. We use the y aesthetic to indicate the variable we want the boxplot for and we use the geom_boxplot command to graph the boxplot (in this case the fill argument can be used to specify a color choice for the box of the boxplot). The only unusual thing here is the use of x=\"\" in the top-level aesthetics and the use of x=NULL in the labs command. These additions are not strictly necessary but they do cause the horizontal x-scale on the graph to be suppressed. Otherwise we would see some non-intuitive numbers here. The exercise below allows you to adjust a slider to see different percentiles on both a histogram and a boxplot. In general, boxplots for a single variable do not contain as much information as a histogram and so are generally inferior for understanding the full shape of the distribution. The real advantage of boxplots will come in the next module when we learn to use comparative boxplots to make comparisons of the distribution of a quantitative variable across different categories of a categorical variable. "],
["measuring-the-spread-of-a-distribution.html", "Measuring the Spread of a Distribution", " Measuring the Spread of a Distribution The second most important measure of a distribution is its spread. Spread indicates how far individual values tend to fall from the center of the distribution. As Figure 14 below shows, two distributions can have the same center and general shape (in this case, a bell curve) but have very different spreads. Figure 14: Two different distributions with the same mean but very different spreads, based on simulated data. Range and interquartile range One of the simplest measures of spread is to calculate the range. The range is the distance between the highest and lowest value. Lets take a look at the range in the fare paid (in British pounds) for tickets on the Titanic. The summary command, will give us the information we need: summary(titanic$fare) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 7.896 14.454 33.276 31.275 512.329 Note that at least one person made it on the Titanic for free. The highest fare paid was 512.3 pounds. So the range is easy to calculate 512.3 - 0 = 512.3. The difference between the highest and lowest paying passenger was about 512 pounds. This example also reveals the shortcoming of the range as a measure of spread. If there are any outliers in the data, they are going to show up in the range and so the range may give you a misleading idea of how spread out the values are. Note that the 75th percentile here is only 31.28 pounds, which would suggest that the 512.3 maximum is a pretty high outlier. We can check this by graphing a boxplot, as I have done in Figure 15. ggplot(titanic, aes(x=&quot;&quot;, y=fare))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;fare paid in British pounds&quot;)+ theme_bw() Figure 15: Boxplot of fare paid on the Titanic The maximum value is such an outlier that the rest of the boxplot has to be “scrunched” in order to fit it all into the graph. Clearly this is not a good indicator of spread. However, we have already seen a better measure of spread using a similar idea: the interquartile range or IQR. The IQR is just the range between the 25th and 75th percentile. We already have these numbers from the output above, so the IQR = 31.28-7.90=23.38. So, the difference in fare between the 25th and 75th percentile (the middle 50% of the data) was 23.4 pounds. That result suggests a much smaller spread of fares. You can also use the IQR command in R to directly calculate the IQR. IQR(titanic$fare) ## [1] 23.3792 Variance and standard deviation The most common measure of spread is the variance and its derivative measurement, the standard deviation. It is so common in fact, that most people simply refer to the concept of “spread” as “variance.” The variance can be defined as the “average squared distance to the mean.” Of course, “squared distance” is a bit hard to think about, so we more commonly take the square root of the variance to get the standard deviation which gives us the “average distance to the mean.” Imagine if you were to randomly pick one observation from your dataset and guess how far it would be from the mean. Your best guess would be the standard deviation. The calculation for the variance and standard deviation is a bit intimidating but we will break it down into steps to show it is not that hard. At the same time, I will show you to calculate the parts in R using the fare variable from the Titanic data. The overall formula for the variance (which is represented as \\(s^2\\)) is: \\[s^2=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\] That looks tough, but lets break it down. The first step is this: \\[(x_i-\\bar{x})\\] You take each value of your variable \\(x\\) and subtract the mean from it. This can be done in R easily: diffx &lt;- titanic$fare-mean(titanic$fare) This measure gives us a description of how far each observation is from the mean which is already kind of a measure of spread, but we can’t do much with it yet because some differences are positive (higher than the mean) and some are negative (lower than the mean). In fact, if we take the mean of these differences, it will be zero by definition because this is what it means for the mean to be the balancing point of the distribution. round(mean(diffx),5) ## [1] 0 The next step is: \\[(x_i-\\bar{x})^2\\] We just need to square the differences. This will get rid of our negative/positive problem, because the squared values will all be positive. diffx.squared &lt;- diffx^2 The next step is: \\[\\sum_{i=1}^n (x_i-\\bar{x})^2\\] We need to sum up all of our values. This value is sometimes called the sum of squared X or SSX for short. It is already pretty close to a measure of variance already. ssx &lt;- sum(diffx.squared) The more distance there is from the mean on average, the larger this value will be. However, it also gets larger when we have more values because we are just taking a sum. To get a number that is comparable across different number of observations, we need to do the final step: \\[s^2=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\\] We are going to divide our SSX value by the number of observations minus one. The “minus one” thing is a bit tricky and I don’t want to get into the details of why we do it here. When n is large, this will have little effect and basically you are taking an average of the squared distance from the mean. variance &lt;- ssx/(length(diffx)-1) variance ## [1] 2677.398 So the average squared distance from the mean fare is 2677.39 pounds squared. Of course, this isn’t a very interpretable number, so its probably better to square root it and get the standard deviation: sqrt(variance) ## [1] 51.74358 So, the average distance from the mean fare is 51.74 pounds. Note that I could have used the power of R to do this entire calculation in one line: sqrt(sum((titanic$fare-mean(titanic$fare))^2)/(length(titanic$fare)-1)) ## [1] 51.74358 Alternatively, I could have just used the sd command to have R do all the heavy lifting: sd(titanic$fare) ## [1] 51.74358 "],
["measuring-association-1.html", "Measuring Association", " Measuring Association Measuring association between variables, in some form or another, is really what social scientists use statistics for the most. Establishing whether two variables are related to one another can help to affirm or cast doubt on theories about the social world. Does substance use improve an adolescent’s popularity in school? Does increasing wealth in a country lead to more or less environmental degradation? Does income inequality inhibit voter participation? These are just a few of the questions one could ask that require a measurement of the association between two or more variables. In this module, we will learn about how to visualize and measure association between two variables. How we do this depends on the kinds of variables that we have. There are three possibilities: two categorical variables one categorical and one quantitative variable two quantitative variables Each of these cases requires that we learn and master different techniques. In the three sections that follow, we will learn how to measure association for all three cases. Slides for this module can be found here. "],
["the-two-way-table.html", "The Two-Way Table", " The Two-Way Table The two-way table (also known as a cross-tabulation or crosstab) gives the joint distribution of two categorical variables. Lets use our politics dataset to construct a two-way table of belief in anthropogenic climate change by political party: tab &lt;- table(politics$party, politics$globalwarm) tab ## ## No Yes ## Democrat 230 1235 ## Republican 577 664 ## Independent 326 1055 ## Other 47 104 The two-way table gives us the joint distribution of the two variables, which is the number of respondents who fell into both categories. For example, we can see that 234 democrats did not believe in anthropogenic climate change while 1230 did. From this table, we can also calculate the marginal distribution of each of the variables, which are just the distributions of each of the variables separately. We can do that by adding up across the rows and down the columns: Table 8: Two-way table of party affiliation by climate change belief, ANES data 2016 Deniers Believers Total Democrat 230 1235 230+1235=1465 Republican 577 664 577+664=1241 Independent 326 1055 326+1055=1381 Other 47 104 47+104=151 Total 230+577+326+47=1180 1235+664+1055+104=3058 4238 The marginal distribution of party affiliation is given by the Total column on the right and the marginal distribution of climate change belief is given by the Total row at the bottom. Looking at the column marginal, we can see that there were a total of 1465 Democrats, 1241 Republicans, and so on. Looking at the row marginal, we can see that there were 1180 anthropogenic climate change deniers and 3058 anthropogenic climate change believers. The final number (4238) in the far right corner is the total number of respondents altogether. You can get this number by summing up the column marginals (1180+3058) or row marginals (1465+1241+1381+151). The margin.table command in R will also calculate marginals for us. I can use the margin.table command on the table I created and saved above as tab to calculate the same marginals as above. Note that you need to indicate which marginal you want by a number, where 1=row and 2=column, as the second option to margin.table: margin.table(tab,1) ## ## Democrat Republican Independent Other ## 1465 1241 1381 151 margin.table(tab,2) ## ## No Yes ## 1180 3058 The two-way table provides us with evidence about the association between two categorical variables. To understand what the association looks like, we will learn how to calculate conditional distributions. Conditional distributions To this point, we have learned about the joint and marginal distributions in a two-way table. In order to look at the relationship between two categorical variables, we need to understand a third kind of distribution: the conditional distribution. The conditional distribution is the distribution of one variable conditional on being in a certain category of the other variable. In a two-way table, there are always two ways to calculate a conditional distribution. In our case, we could look at the distribution of climate change belief conditional on party affiliation, or we could look at the distribution of party affiliation conditional on climate change belief. Both of these distributions really give us the same information about the association, but sometimes one way is more intuitive to understand. In this case, I am going to start with the former case and calculate the distribution of climate change belief conditional on party affiliation. This conditional distribution is basically given by the rows of our two-way table, which give the number of individuals of a given party who fall into each belief category. For example, the distribution of denial/belief among Democrats is 429 and 1932, while among Republicans, this distribution is 708 and 681. However, these two rows are not directly comparable as they are because Republicans are a much smaller group than Democrats. Thus, even if the shares were very different between the two groups, the absolute numbers for Republicans would probably be smaller for both categories. In order to make these rows comparable, we need the proportion of each party that falls into each belief category. In order to do that, we need to divide our rows through by the marginal distribution of party affiliation, like so: Table 9: Calculation of distribution of belief conditional on party affiliation Deniers Believers Total Democrat 230/1465 1235/1465 1465 Republican 577/1241 664/1241 1241 Independent 326/1381 1055/1381 1381 Other 47/151 104/151 151 Note that each row gets divided by its row marginal. If we do the math here, we will come out with the following proportions: Table 10: Distribution of belief conditional on party affiliation Deniers Believers Total Democrat 0.157 0.843 1 Republican 0.465 0.535 1 Independent 0.236 0.764 1 Other 0.311 0.689 1 Note that the proportions should add up to 1 within each row because we are basically calculating the share of each row that belongs to each column category. To understand these conditional distributions, you need to look at the numbers within each row. For example, the first row tells us that 15.7% of Democrats are deniers and 84.3% of Democrats are believers. The second row tells us that 46.5% of Republicans are deniers and 53.5% of Republicans are believers. We can tell if there is an association between the row and column variable if these conditional distributions are different across rows. In this case, they are clearly very different. About 84.3% of Democrats are believers while only about half (53.5%) of Republicans are believers. About 76.4% of Independents are believers, while about 68.9% of members of other parties are believers. We can use the prop.table command we saw in the last module to estimate these conditional distributions. In order to do that we feed in the crosstab we calculated with tab and one additional argument that indicates which dimension (row or column ) we want to condition across. Like margin.table a value of 1 will condition on rows (rows sum to 1) and a value of 2 will condition on columns (columns sum to 1). If we condition on rows here, we will get the same table as above. prop.table(tab,1) ## ## No Yes ## Democrat 0.1569966 0.8430034 ## Republican 0.4649476 0.5350524 ## Independent 0.2360608 0.7639392 ## Other 0.3112583 0.6887417 Its important to remember which way you did the conditional distribution and get the interpretation correct. If you are not sure, just note which way the proportions add up to one - this is the direction you should be looking (i.e. within row or column). In this case, I am looking at the distribution of variables within rows, so the proportions refer to the proportion of respondents from a given political party who hold a given belief. But, I could have done my conditional distribution the other way: prop.table(tab,2) ## ## No Yes ## Democrat 0.19491525 0.40385873 ## Republican 0.48898305 0.21713538 ## Independent 0.27627119 0.34499673 ## Other 0.03983051 0.03400916 Note that this table looks deceptively similar to the table above. But look again. The numbers now don’t add up to one within each row. They do however add up to one within each column. In order to read this table properly, we have to understand that it is giving us the distribution within each column: the proportion of respondents who have a given belief who belong to a given political party. So we can see in the first number that 19.5% of deniers are Democrats, 48.2% are Republicans, 27.9% are Independents, and 4.1% belong to other parties. This distribution is very different from the party affiliation distribution of believers in the second column which tells us that there is an association. However, the large party cleavages on the issue are not as immediately obvious here as they were with the previous conditional distribution. Always think carefully about which conditional distribution is more sensible to interpret and always make sure that you are interpreting them in the correct way. Try it out In the interactive R session below, try to calculate the distribution of movie maturity ratings (e.g. PG, PG-13) by movie genre. Keep the following in mind: For easier display, its usually better to put the variable with more categories on the rows. In this case, Genre has more categories than Rating. For a more compact display, you should wrap your prop.table command in a round command with three decimal places. See the solution if you have trouble. Remember to check that your conditional distributions add up to one in the direction that you expect. Once you have the correct output, you can explore some interesting questions. Which movie genre is the most likely to be R-rated? Which movie genre is the most likely to be G-rated? Which genre is the least likely to receive a PG-13 rating? It is also possible to graph the conditional distribution as a set of barplots. To do that, we will learn a new feature of ggplot called faceting. Faceting allows us to make the same plot based on subsets of the data in a series of panels. In this case, we can use the code for a univariate barplot but faceted by First, lets save the output of our prop.table into a new object. ggplot(politics, aes(x=globalwarm, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~party)+ scale_y_continuous(labels = scales::percent)+ labs(x=&quot;belief in anthropogenic climate change&quot;, y=NULL)+ theme_bw() Figure 16: Distribution of belief in anthropogenic climate change by party affiliation, ANES 2016 Figure 16 is a comparative barplot. This is our first example of a graph that looks at a relationship. Each panel shows the distribution of climate change beliefs for respondents with that particular party affiliation. What we are interested in is whether the distribution looks different across each panel. In this case, because there were only two categories of the response variable, we only really need to look at the heights of the bar for one category to see the variation across party affiliation, which is substantial. Lets try an example with more than two categories. For this example, I want to know whether there was a difference in passenger class by gender on the Titanic. I start with a comparative barplot: ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~sex)+ scale_y_continuous(labels = scales::percent)+ labs(x=&quot;passenger class&quot;, y=NULL)+ theme_bw() Figure 17: Distribution of passenger class by gender I can also calculate the conditional distributions by hand using prop.table: round(prop.table(table(titanic$sex, titanic$pclass),1),3)*100 ## ## First Second Third ## Female 30.9 22.7 46.4 ## Male 21.2 20.3 58.5 As these numbers and Figure 17 both show, the distribution of men and women by passenger class is very different. Women were less likely to be in third class and more likely to be in first class than men, while about the same percent of men and women were in second class. Try it out In the interactive R session below, Create a barplot showing the conditional distribution of smoking behavior by race in the popularity dataset. Keep the following in mind: You must combine together different elements of ggplot, in this case ggplot itself and geom_histogram with a plus sign to display correctly. Use the facet_wrap command to make comparisons by race. Be sure to add nice labels for your x axis by adding a labs command. How do the results for the mean and median compare? What does this suggest about the underlying shape of the distribution? Odds ratio (advanced) We can also use the odds ratio to measure the association between two categorical variables. The odds ratio is not a term that is common in everyday speech but it is a critical concept in all kinds of scientific research. Lets take the different distributions of climate change belief for Democrats and Republicans. About 84% of Democrats were believers, but only 54% of Republicans were believers. How can we talk about how different these two numbers are from one another? We could subtract one from the other or we could take the ratio by dividing one by the other. However, both of these approaches have a major problem. Because the percents (and proportions) have minimum and maximum values of 0 and 100, as you approach those boundaries the differences necessarily have to shrink because one group is hitting its upper or lower limit. This makes it difficult to compare percentage or proportional differences across different groups because the overall average proportion across groups will affect the differences in proportion. Odds ratios are a way to get around this problem. To understand odds ratios, you first have to understand what odds are. All probabilities (or proportions) can be converted to a corresponding odds. If you have a \\(p\\) probability of success on a task, then your odds \\(O\\) of success are given by: \\[O=\\frac{p}{1-p}\\] The odds are basically the ratio of the probability of success to the probability of failure. This tells you how many successes you expect to get for every one failure. Lets say a baseball player gets a hit 25% of the time that the player comes up to bat (an average of 250). The odds are then given by: \\[O=\\frac{0.25}{1-0.25}=\\frac{0.25}{0.75}=0.33333\\] The hitter will get on average 0.33 hits for every one out. Alternatively, you could say that the hitter will get one hit for every three outs. Re-calculating probabilities in this way is useful because unlike the probability, the odds has no upper limit. As the probability of success approaches one, the odds will just get larger and larger. We can use this same logic to construct the odds that a Democratic and Republican respondent, respectively, will be climate change believers. For the Democrat, the probability is 0.843, so the odds are: \\[O=\\frac{0.843}{1-0.843}=\\frac{0.843}{0.157}=5.369\\] Among Democrats, there are 5.369 believers for every one denier. Among Republicans, the probability is 0.541, so the odds are: \\[O=\\frac{0.535}{1-0.535}=\\frac{0.535}{0.465}=1.151\\] Among Republicans, there are 1.151 believers for every one denier. This number is close to “even” odds of 1, which happen when the probability is 50%. The final step here is to compare those two odds. We do this by taking their ratio, which means we divide one number by the other: \\[\\frac{5.369}{1.151}=4.67\\] This is our odds ratio. How do we interpret it? This odds ratio tells us how much more or less likely climate change belief is among Democrats relative to Republicans. In this case, I would say that “the odds of belief in anthropogenic climate change are 4.665 times higher among Democrats than Republicans.” Note the “times” here. This 4.665 is a multiplicative factor because we are taking a ratio of the two numbers. You can calculate odds ratios from conditional distributions just as I have done above, but there is also a short cut technique called the cross-product. Lets look at the two-way table of party affiliation but this time just for Democrats and Republicans. For reasons I will explain below, I am going to reverse the ordering of the columns so that believers come first. Believer Denier Democrat 1235 230 Republican 664 577 The two numbers in blue are called the diagonal and the two numbers in red are the reverse diagonal. The cross-product is calculated by multiplying the two numbers in the diagonal by each other and multiplying the two numbers in the reverse diagonal together and then dividing the former product by the latter: \\[\\frac{1235*577}{664*230}=4.67\\] I get the exact same odds ratio as above without having to calculate the proportions and odds themselves. This is a useful shortcut for calculating odds ratios. The odds ratio that you calculate is always the odds of the first row being in the first column relative to those odds for the second row. Its easy to show how this would be different if I had kept the original ordering of believers and deniers: Denier Believer Democrat 230 1235 Republican 577 664 \\[\\frac{230*664}{577*1235}=0.21\\] I get a very different odds ratio, but that is because I am calculating something different. I am now calculating the odds ratio of being a denier rather than a believer. So I would say that the “the odds of denial of anthropogenic climate change among Democrats are only 22% of the odds for Republicans.” In other words, the odds of being a denier are much lower among Democrats. However, the information here is the same because the 0.21 here is exactly equal to 1/4.67. In other words, the odds ratio of denial is just the inverted mirror image of the odds ratio of belief. Its just important that you remember that when you calculate the cross-product, you are always calculating the odds ratio of being in the category of the first column, whatever category that may be. "],
["mean-differences.html", "Mean Differences", " Mean Differences Measuring association between a quantitative and categorical variable is fairly straightforward. We want to look for differences in the distribution of the quantitative variable at different categories of the categorical variables. For example, if we were interested in the gender wage gap, we would want to compare the distribution of wages for women to the distribution of wages for men. There are two ways we can do this. First, we can graphically examine the distributions using the techniques we have already developed, particularly the boxplot. Second, we can compare summary measures like the mean across categories. Graphically examining differences in distributions We could compare entire histograms of the quantitative variable across different categories of the categorical variable, but this is often too much information. A cleaner method is to use comparative boxplots. Comparative boxplots construct boxplots of the quantitative variable across all categories of the categorical variable and plot them next to each other for easier comparison. We can easily construct a comparative boxplot in ggplot by adding an x aesthetic to our existing boxplot code. Lets try an example looking at differences in the distribution of movie runtime across different movie genres. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ theme_bw() Figure 18: Boxplots of movie runtime by genre This plot is a good start, but I am running into a problem where the genre labels are running into each other on the x-axis because they are too long. We can solve this problem very easily by using the coord_flip command to flip the axis: ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ coord_flip()+ theme_bw() Figure 19: Boxplots of movie runtime by genre, with coordinates flipped Now my genre labels are much easier to read. However, there is one more addition I can make to this graph in order to improve its readability. I want to order my genre categories so that they are ordered from largest to smallest median runtime on the graph. I can do this by applying the reorder command directly within ggplot. The reorder command takes three arguments. The first argument is the categorical variable to be reordered (genre in my case). The second variable is the variable that reordering should be based upon (runtime in my case). The third argument is the mathematical function that will be used for sorting (median in my case). The full command looks like: ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_boxplot(fill=&quot;seagreen&quot;, outlier.color = &quot;red&quot;)+ labs(x=NULL, y=&quot;movie runtime in minutes&quot;)+ coord_flip()+ theme_bw() Figure 20: Boxplots of movie runtime by genre, with coordinates flipped Figure 20 now contains a lot of information. At a glance, I can see which genres had the longest median runtime and which had the shortest. But I can also see how much variation in runtime there is within movies by comparing the width of the boxes. For example, I can see that there is relatively little variation in runtime for animation movies, and the most variation in runtime among sci-fi/fantasy and action movies. I can also see some of the extreme outliers by genre. Most of these outliers are for extremely long movies, relative to the genre norm, but in a couple of cases, I can see movies that were remarkably short for their genre. Try it out In the interactive R session below, Create a comparative boxplot showing the relationship between movie maturity rating and tomato meter score from the movies dataset. Keep the following in mind: You must combine together different elements of ggplot, in this case ggplot itself and geom_boxplot with a plus sign to display correctly. Be sure to add nice labels for your x axis by adding a labs command. Because there are only four categories of maturity rating, you probably do not need to use a coord_flip command but you are welcome to try it out. Because maturity ratings are ordinal, its probably best to not reorder the categories from smallest to largest as above. Which maturity ratings get the best tomato meter scores? Which get the worst? Do the results surprise you? Do you have any ideas as to why you observe the pattern that you do? Comparing differences in the mean We can also establish a relationship by looking at differences in summary measures. Implicitly, we are already doing this in the comparative boxplot when we look at the median bars across categories. However, in practice it is more common to compare the mean of the quantitative variable across different categories of the categorical variable. In R, you can get the mean of one variable at different levels of a categorical variable using the tapply command like so: tapply(movies$Runtime, movies$Genre, mean) ## Action Animation Comedy Drama Family ## 111.91304 90.06475 100.45711 112.65060 102.60000 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 97.39545 108.46602 114.25641 109.89855 112.56202 ## Thriller ## 110.13260 The tapply command takes three arguments. The first argument is the quantitative variable for which we want means. The second argument is the categorical variable. The third argument is the method we want to run on the quantitative variable, in this case the mean. The output is the mean movie runtime by genre. If we want a quantitative measure of how genre and runtime are related we can calculate a mean difference. This number is simply the difference in means between any two categories. For example, action movies are 111.9 minutes long, on average, while horror movies are 97.4 minutes long, on average. The difference is \\(111.9-97.4=14.5\\). Thus, we can say that, on average, action movies are 14.5 minutes longer than horror movies. We could have reversed that subtraction to get \\(97.4-111.9=-14.5\\). In this case, we would say that horror movies are 14.5 minutes shorter than action movies. Either way, we get the same information. However, its important to keep track of which number applies to which category when you take the difference, so that you get the interpretation correct. Try it out In the interactive R session below, try to calculate the mean difference in hourly wages between men and women, based on the earnings data. Keep the following in mind: You need to use the tapply command to calculate the mean wages for each group. You can use the output of the tapply command to calculate the mean difference itself. We can also display these results graphically using a barplot, although this will take a little more processing for ggplot because ggplot expects data to be in a “data.frame” object and the output of tapply is a single vector of numbers. To make this work, we have to use the as.data.frame.table command to convert our object and then rename the variables. Also, to make this prettier, I am first going to sort the output from largest to smallest mean runtime: mruntime &lt;- tapply(movies$Runtime, movies$Genre, mean) #sort highest to lowest mruntime &lt;- sort(mruntime, decreasing=FALSE) #convert to data.frame mruntime &lt;- as.data.frame.table(mruntime) #rename variables colnames(mruntime) &lt;- c(&quot;genre&quot;,&quot;runtime&quot;) ggplot(mruntime, aes(x=genre, y=runtime))+ geom_col()+ coord_flip()+ labs(x=NULL, y=&quot;runtime in minutes&quot;)+ theme_bw() Figure 21: Mean runtime by movie genre This is is one of the few examples this term where we have to process a dataset into something else prior to feeding it into ggplot. The result is shown in Figure 21. While this information is useful, it doesn’t really tell us anything that we haven’t already seen in the comparative boxplot from Figure 20. The only real difference is that the boxplots showed us medians while this figure shows us means. However, the comparative boxplots also showed us additional information about spread and outliers and so are generally preferable. Figure 21 also breaks a common stylistic rule in data visualization. The big thick bars take up a lot of ink but carry relatively little information. This is called the “ink to information ratio” made famous by Edward Tufte. An alternative way to display this information would be to use “lollipops” rather than bars: ggplot(mruntime, aes(x=genre, y=runtime))+ geom_lollipop()+ coord_flip()+ labs(x=NULL, y=&quot;runtime in minutes&quot;)+ theme_bw() Figure 22: Using a lollipop graph to display ean runtime by movie genre is a lot easier on the eye and the ink cartridge "],
["scatterplot-and-correlation-coefficient.html", "Scatterplot and Correlation Coefficient", " Scatterplot and Correlation Coefficient The techniques for looking at the association between two quantitative variables are more developed than the other two cases, so we will spend more time on this topic. Additionally, the major approach here of ordinary least squares regression turns out to be a very flexible, extendable method that we will build on later in the term. When examining the association between two quantitative variables, we usually distinguish the two variables by referring to one variable as the dependent variable and the other variable as the independent variable. The dependent variable is the variable whose outcome we are interested in predicting. The independent variable is the variable that we treat as the predictor of the dependent variable. For example, lets say we were interested in the relationship between income inequality and life expectancy. We are interested in predicting life expectancy by income inequality, so the dependent variable is life expectancy and the independent variable is income inequality. The language of dependent vs. independent variable is causal, but its important to remember that we are only measuring the association. That association is the same regardless of which variable we set as the dependent and which we set as the independent. Thus, the selection of the dependent and independent variable is more about which way it more intuitively makes sense to interpret our results. The scatterplot We can examine the relationship between two quantitative variables by constructing a scatterplot. A scatterplot is a two-dimensional graph. We put the independent variable on the x-axis and the dependent variable on the y-axis. For this reason, we often refer generically to the independent variable as x and the dependent variable generically as y. To construct the scatterplot, we plot each observation as a point, based on the value of its independent and dependent variable. For example, lets say we are interested in the relationship between the median age of the state population and violent crime in our crime data. Our first observation, Alabama, has a median age of 37.8 and a violent crime rate of 378 crimes per 100,000. this is plotted in 23 below. Figure 23: Starting a scatterplot by plotting the firest observation If I repeat that process for all of my observations, I will get a scatterplot that looks like: Figure 24: Scatterplot of median age by the violent crime rate for all US states What are we looking for when we look at a scatterplot? There are four important questions we can ask of the scatterplot. First, what is the direction of the relationship. We refer to a relationship as positive if both variables move in the same direction. if y tends to be higher when x is higher and y tends to be lower when x is lower, then we have a positive relationship. On the other hand, if the variables move in opposite directions, then we have a negative relationship. If y tends to be lower when x is higher and y tends to be higher when x is lower, then we have a negative relationship. In the case above, it seems like we have a generally negative relationship. States with higher median age tend to have lower violent crime rates. Second, is the relationship linear? I don’t mean here that the points fall exactly on a straight line (which is part of the next question) but rather does the general shape of the points appear to have any “curve” to it. If it has a curve to it, then the relationship would be non-linear. This issue will become important later, because our two primary measures of association are based on the assumption of a linear relationship. In this case, there is no evidence that the relationship is non-linear. Third, what is the strength of the relationship. If all the points fall exactly on a straight line, then we have a very strong relationship. On the other hand, if the points form a broad elliptical cloud, then we have a weak relationship. In practice, in the social sciences, we never expect our data to conform very closely to a straight line. Judging the strength of a relationship often takes practice. I would say the relationship above is of moderate strength. Fourth, are there outliers? We are particularly concerned about outliers that go against the general trend of the data, because these may exert a strong influence on our later measurements of association. In this case, there are two clear outliers, Washington DC and Utah. Washington DC is an outlier because it has an extremely high level of violent crime relative to the rest of the data. Its median age tends to be on the younger side, so its placement is not inconsistent with the general trend. Utah is an outlier that goes directly against the general trend because it has one of the lowest violent crime rates and the youngest populations. This is, of course, driven by Utah’s heavily Mormon population, who both have high rates of fertility (leading to a young population) and whose church communities are able to exert a remarkable degree of social control over these young populations. Constructing scatterplots in R You can construct scatterplots in ggplot by using the geom_point geometry. You just need to define the aesthetics for x (on the x-axis) and y (on the y-axis). ggplot(crimes, aes(x=Poverty, y=Property))+ geom_point()+ labs(x=&quot;poverty rate&quot;, y=&quot;property crimes per 100,000 population&quot;)+ theme_bw() Figure 25: Scatterplot of a state’s poverty rate by property crime rate, for all US states Sometimes with large datasets, scatterplots can be difficult to read because of the problem of overplotting. This happens when many data points overlap, so that its difficult to see how many points are showing. For example: ggplot(movies, aes(x=Runtime, y=TomatoMeter))+ geom_point()+ labs(x=&quot;movie runtime in minutes&quot;, y=&quot;Percent of reviews that are positive&quot;)+ theme_bw() Figure 26: An example of the problem of overplotting where points are being plotted on top of each other Because so many movies are in the 90-120 minute range, it is difficult to see the density of points in this range and thus difficult to understand the relationship. One way to address this is to allow the points to be semi-transparent, so that as more and more points are plotted in the same place, the shading will become darker. We can do this in geom_point by setting the alpha argument to something less than one but greater than zero: ggplot(movies, aes(x=Runtime, y=TomatoMeter))+ geom_point(alpha=0.3)+ labs(x=&quot;movie runtime in minutes&quot;, y=&quot;Percent of reviews that are positive&quot;)+ theme_bw() Figure 27: Using a semi-transparent point will help us see areas where there is a dense tangle of points Overplotting can also be a problem with discrete variables because these variables can only take on certain values which will then exactly overlap with one another. This can be seen in Figure 28 which shows the relationship between age and wages in the earnings data. ggplot(earnings, aes(x=age, y=wages))+ geom_point()+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 28: Age is discrete so the scatterplot looks like a bunch of vertical lines of dots and is very hard to understand Because age is discrete all points are stacked up in vertical bars making it difficult to understand what is going on. One solution for this problem is to “jitter” each point a little bit by adding a small amount of randomness to the x and y values. The randomness added won’t affect our sense of the relationship but will reduce the issue of overplotting. We can do this simply in ggplot by replacing the geom_point command with geom_jitter. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter()+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 29: Jittering helps with overplotting but its still difficult to see how dense points are for most of the plot Jittering helped get rid of those vertical lines, but there are so many observations that we still have problems of understanding the density of points for most of the plot. If we add an alpha parameter to the geom_jitter command, we should be better able to understand the scatterplot. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01)+ labs(x=&quot;age&quot;, y=&quot;hourly wage&quot;)+ theme_bw() Figure 30: Jittering and transparency help us to make sense of the relationship between age and wages Try it out In the interactive R session below, Create a scatterplot showing the relationship between a student’s parental income and the number of friend nominations they receive using the popularity dataset. Keep the following in mind: You must combine together different elements of ggplot, in this case ggplot itself and geom_boxplot with a plus sign to display correctly. Be sure to add nice labels for your x axis by adding a labs command. You will observe significant issues with overplotting, so you should use geom_jitter and the alpha argument to help see patterns in the data. Does it seem like there is a relationship? How would you characterize its direction, linearity, and strength? Are there any outliers? The correlation coefficient We can measure the association between two quantitative variables with the correlation coefficient, r. The formula for the correlation coefficient is: \\[r=\\frac{1}{n-1}\\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{s_x}*\\frac{y_i-\\bar{y}}{s_y})\\] That looks complicated, but lets break it down step by step. We will use the association between median age and violent crimes as our example. The first step is to subtract the means from each of our x and y variables. This will give us the distance above or below the mean for each variable. diffx &lt;- crimes$MedianAge-mean(crimes$MedianAge) diffy &lt;- crimes$Violent-mean(crimes$Violent) The second step is to divide these differences from the mean of x and y by the standard deviation of x and y, respectively. diffx.sd &lt;- diffx/sd(crimes$MedianAge) diffy.sd &lt;- diffy/sd(crimes$Violent) Now each of your x and y values have been converted from their original form into the number of standard deviations above or below the mean. This is often called standarization. By doing this, we have put both variables on the same scale and have removed whatever original units they were measured in (in our case, years of age and crimes per 100,000). The third step is to to multiply each converted value of x by each converted value of y. product &lt;- diffx.sd*diffy.sd Why do we do this? First consider this scatterplot of our standardized x and y: Figure 31: Where a point falls in the four quadrants of this scatterplot indicate whether it provides evidence for a positive or negative relationship Points shown in blue have either both positive or both negative x and y values. When you take the product of these two numbers, you will get a positive product. This is evidence of a positive relationship. Points shown in red have one positive and one negative x and y value. When you take the product of these two numbers, you will get a negative product. This is evidence of a negative relationship. The final step is to add up all this evidence of a positive and negative relationship and divide by the number of observations (minus one). sum(product)/(length(product)-1) ## [1] -0.3015232 This final value is our correlation coefficient. We could have also calculated it by using the cor command: cor(crimes$MedianAge, crimes$Violent) ## [1] -0.3015232 How do we interpret this correlation coefficient? It turns out the correlation coefficient r has some really nice properties. First, the sign of r indicates the direction of the relationship. If r is positive, the association is positive. If r is negative, the association is negative. if r is zero, there is no association. Second, r has a maximum value of 1 and a minimum value of -1. These cases will only happen if the points line up exactly on a straight line, which never happens with social science data. However, it gives us some benchmark to measure the strength of our relationship. Figure 32 shows simulated scatterplots with different r in order to help you get a sense of the strength of association for different values of r. Figure 32: Strength of association for various values of the correlation coefficient, based on simulated data Third, r is a unitless measure of association. It can be compared across different variables and different datasets in order to make a comparison of the strength of association. For example, the correlation coefficient between unemployment and violent crimes is 0.45. Thus, violent crimes are more strongly correlated with unemployment than with median age (0.44&gt;0.30). The association between median age and property crimes is -0.36, so median age is more strongly related to property crimes than violent crimes (0.36&gt;0.30). There are some important cautions when using the correlation coefficient. First, the correlation coefficient will only give a proper measure of association when the underlying relationship is linear. if there is a non-linear (curved) relationship, then r will not correctly estimate the association. Second, the correlation coefficient can be affected by outliers. We will explore this issue of outliers and influential points more in later sections. "],
["statistical-inference.html", "Statistical Inference", " Statistical Inference Now that we have the basics of examining data down, we turn to another issue that we can address with statistical analysis. Howe confident are we that the the results from the data represent the larger population from which the data are drawn? This issue only applies to cases where the data we use constitutes a sample from a larger population. However, many of the datasets that we work with in the social sciences are of this type, so this is typically an important issue. We don’t want to to reach an incorrect conclusion that \\(x\\) is associated with \\(y\\) in cases when that association in our sample is basically a result of random chance. In many introductory statistics courses, statistical inference would take up the majority of the course and you would learn a variety of cookbook formulas for conducting “tests.” We won’t do much of that here. Instead I will focus on the logic of the two most common procedures in statistical inference: the confidence interval and the hypothesis test. Once you understand the logic behind these procedures, it turns out that all of the various “tests” are just iterations on the same basic theme. Nonetheless, we will have to use some formulas in this module with associated number crunching. This is the most math heavy module of the course, so be prepared. Slides for this module can be found here. "],
["the-problem-of-statistical-inference.html", "The Problem of Statistical Inference", " The Problem of Statistical Inference So far, we have only been looking at measurements from our actual datasets. We examined both univariate statistics like the mean, median, and standard deviation, as well as measures of association like the mean difference, correlation coefficient and OLS regression line slope. We can use this measures to draw conclusions about our data. In many cases, the dataset that we are working is only a sample from some larger population. Importantly, we don’t just want to know something about the sample, but rather we want to know something about the population from which the sample was drawn. For example, when polling organizations like Gallup conduct political polls of 500 people, they are not drawing conclusions about just those 500 people, but rather about the whole population from which those 500 people were sampled. To take another example from our General Social Survey (GSS) data on sexual frequency. We can calculate the mean sexual frequency by marital status: tapply(sex$sexf, sex$marital, mean) ## Married Widowed Divorced Separated Never married ## 56.094106 9.222628 41.388720 55.652778 53.617704 Married respondents had sex 2.5 (56.1-53.6) times more per year than never married individuals. We don’t want to draw this conclusion just for our sample. Rather, we want to know what the relationship is between marital status and sexual frequency in the US population as a whole. In other words, we want to infer from our sample to the population. Figure 33 shows this process graphically. Figure 33: The process of making statistical inferences The large blue rectangle is the population that we want to know about. Within this population, there is some value that we want to know. In this case, that value is the mean difference in sexual frequency between married and never married individuals. We refer to this unknown value in the population as a parameter. You will also notice that there are some funny-looking Greek letters in that box. We always use Greek symbols to represent values in the population. In this case, \\(\\mu_1\\) is the population mean of sexual frequency for married individuals and \\(\\mu_2\\) is the population mean of sexual frequency for never married individuals. Thus, \\(\\mu_1-\\mu_2\\) is the population mean difference in sexual frequency between married and never married individuals. We typically don’t have data on the entire population, which is why we need to draw a sample in the first place. Therefore, these population parameters are unknown. To estimate what they are, we draw a sample as shown by the smaller yellow square. In this sample, we can calculate the sample mean difference in sexual frequency between married and never married individuals, \\(\\bar{x}_1-\\bar{x}_2\\). We refer to a measurement in the sample as a statistic. We represent these statistics with roman letters to distinguish them from the corresponding value in the population. The statistic is always an estimate of the parameter. In this case, \\(\\bar{x}_1-\\bar{x}_2\\) is an estimate of \\(\\mu_1-\\mu_2\\). We can infer from the sample to the population and conclude that our best guess as to the true mean difference in the population is the value we got in the sample. The sample mean difference may be our best guess as to the true value in the population, but how confident are we in that guess? Intuitively, if I only had a sample of 10 people I would be much less confident than if I had a sample of 10,000 people. Statistical inference is the technique of quantifying our uncertainty in the estimate. If you have ever read the results of a political poll, you will be familiar with the term “margin of error.” This is a measure of statistical inference. Why might our sample produce inaccurate results? There are two sources of bias that could result in our sample statistic being different from the true population parameter. The first form of bias is systematic bias. Systematic bias occurs when something about the procedure for generating the sample produces a systematic difference between the sample and the population. Sometimes, systematic bias results from the way the sample is drawn. For example, if I sample my friends and colleagues on their voting behavior, I will likely introduce very large systematic bias in my estimate of who will win an election because my friends and colleagues are more likely than the general population to hold similar views to my own. Systematic bias can also result from the way questions are worded, the characteristics of interviewers, the time of day interviews are conducted, etc. Systematic bias can often be minimized in well-designed and executed scientific surveys. Statistical inference cannot do anything to account for systematic bias. The second form of bias is random bias. Random bias occurs when the sample statistic is different from the population parameter, just by random chance due to the actual sample that was drawn. In other words, even if there is no systematic bias in my survey design, I can get a bad estimate simply due to the bad luck of drawing a really unusual sample. Imagine that I am interested in estimating mean wealth in the United States and I happen to draw Bill Gates in my sample. I will dramatically overestimate mean wealth in the US. Random bias affects every sample, regardless of how well-designed and executed. In practice, the sample statistic is extremely unlikely to be exactly equal to the population parameter, so some degree of random bias is always present in every sample. However, this random bias will become less important as the sample size increases. In the previous example, Bill Gates is going to bias my results much more if I draw a sample of 10 people, than if I draw a sample of 100,000 people. Our goal with statistical inference is to more precisely quantify how bad that random bias could be in our sample. Notice the word “could” in the previous sentence. The tricky part about statistical inference is that while we know that random bias could be causing our sample statistic to be very different from the population parameter, we never know for sure whether random bias had a big effect or a small effect in our particular sample, because we don’t have the population parameter with which we could compare it. Keep this issue in mind in the next sections, as it plays a key role in how we understand our procedures of statistical inference. It is also important to keep in mind that statistical inference only works when you are actually drawing a sample from a larger population that you want to draw conclusions about. In some cases, our data either constitute a unique event, as in the Titanic case, that cannot be properly considered a sample of something larger or the data actually constitute the entire population of interest, as is the case in our dataset on movies. Although you will occasionally still see people use inferential measures on such data, it is technically inappropriate because there is no larger population to make inferences about. "],
["the-concept-of-the-sampling-distribution.html", "The Concept of the Sampling Distribution", " The Concept of the Sampling Distribution Lets say that you want to know the mean years of education of US adults. You implement a well-designed representative survey that samples 100 respondents from the USA. You ask people the simple question “how many years of education do you have?” You then calculate the mean years of education in your sample. This simple example involves three different kinds of distributions. Understanding the difference between these three different distributions is the key to unlocking how statistical inference works. The Population Distribution. This is the distribution of years of education in the entire US population. The mean of this distribution is given by \\(\\mu\\) and is a population parameter. If we had data on the entire population we could show the distribution and calculate \\(\\mu\\). However, because we don’t have data on the full population, the population distribution and \\(\\mu\\) are unknown. This distribution is also static - it doesn’t fluctuate. The Sample Distribution. This is the distribution of years of education in the sample of 100 respondents that I have. The mean of this distribution is \\(\\bar{x}\\) and is a sample statistic. Since we collected this data, this distribution and \\(\\bar{x}\\) are known. We can calculate \\(\\bar{x}\\) and we can show the distribution of years of education in the sample (with a histogram or boxplot, for example). The sample distribution is an approximation of the population distribution, but because of random bias, it may be somewhat different. Also, because of this random bias, the distribution is not static - if we were to draw another sample the two sample distributions would almost certainly not be identical. The Sampling Distribution. Imagine all the possible samples of size 100 that I could have drawn from the US population. Its a tremendously large number. If I had all those samples, I could calculate the sample mean of years of education for each sample. Then, I would have the mean years of education in every possible sample of size 100 that I could have drawn from the population. The sampling distribution is the distribution of all of these possible sample means. More generally, the sampling distribution is the distribution of the desired sample statistic in all possible samples of size \\(n\\). The sampling distribution is much more abstract than the other two distributions, but is key to understanding statistical inference. When we draw a sample and calculate a sample statistic from this sample, we are in effect reaching into the sampling distribution and pulling out a value. Therefore, the sampling distribution give us information about how variable our sample statistic might be as a result of randomness in the sampling. Example: class height As an example, lets use some data on a recent class that took this course. I will treat this class of 42 students as a population that I would like to know something about. In this case, I would like to know the mean height of the class. In most cases, the population distribution is unknown but in this case, I know the height of all 42 students because of surveys they all took at the beginning of the term. Figure 34 shows the population distribution of height in the class: Figure 34: Population distribution of height in a class of students The population distribution of height is bimodal which is typical, because we are mixing the heights of men and women. The population mean of height (\\(\\mu\\)) is 66.517 inches. Lets say I lost the results of the student survey and I wanted to know the mean height of the class. I could take a random sample of two students in order to calculate the mean height. Lets say I drew a sample of two people who were 68 and 74 inches respectively in height. I would estimate a sample mean of 71 inches which in this case would be too high. Lets say I took another sample of two students and ended up with a mean height of 66 which would be too low. Lets say I repeat this procedure until I had sampled all possible combinations of two students out of the twenty in the class. How many samples would this be? On the first draw from the population of 42 students there are 42 possible results. On the second draw, there are 41 possible results, because I won’t re-sample the student I selected the first time. This gives me 42*41=1722 possible combinations of 42 students in two draws. However, half of these draws are just mirror images of the other draws where I swap the first and second draw. Since I don’t care about the order, I actually have 1722/2=861 possible samples. I have used a computer routine to actually calculate the sample means in all 861 of those possible samples. The distribution of these sample means then gives us the sampling distribution of mean height for samples of size 2. Figure 35 shows a histogram of that distribution. Figure 35: The sampling distribution of class height for samples of size 2 When I randomly draw one sample of size 2 and calculate its mean, I am effectively reaching into this distribution and pulling out one of these values. Note that many of the means are clustered around the true population mean of 66.5 inches, but in a few cases I can get extreme overestimates or extreme underestimates. What if I were to increase the sample size? I have used the same computer routine to calculate the sampling distribution for samples of size 3, 4, and 5. Figure 36 shows the results. Figure 36: The sampling distribution of class height for samples of various sizes Clearly, the shape of these distributions is changing. Another way to visualize this would be to draw density graphs which basically fit a curve to the histograms. We can then then overlap these density curves on the same graph. Figure 37 shows this graph. Figure 37: The sampling distribution of class height for samples of various sizes. The vertical red line shows the true population mean. There are three thing to note here. First, each sampling distribution seems to have most of the sample means clustered (where the peak is) around the true population mean of 66.5. In fact, the mean of each of these sampling distributions is exactly the true population parameter, as I will show below. Second, the spread of the distributions is shrinking as the sample size increases. You can see that the when the sample size increases, the tails of the distribution are “pulled in” and more of the sample means are closer to the center. This indicates that we are less likely to draw a sample mean that is extremely different from the population mean in larger samples. Third, the shape of the distribution at larger sample sizes is becoming more symmetric and “bell-shaped.” Lets take a look at the mean and standard deviation of these sampling distributions: Table 11: Mean and standard deviation of height for the population of students in a class as well as from several sampling distributions of that population Distribution Mean Standard Deviation Population Distribution 66.517 4.87 Sampling Distribution (n=2) 66.517 3.363 Sampling Distribution (n=3) 66.517 2.71 Sampling Distribution (n=4) 66.517 2.316 Sampling Distribution (n=5) 66.517 2.044 Note that the mean of each sampling distribution is identical to the true population mean. This is not a coincidence. The mean of the sampling distribution of sample means is always itself equal to the population mean. In statistical terminology, this is the definition of an unbiased statistic. Given that we are trying to estimate the true population mean, it is reassuring that the “average” sample mean we should get is the true population mean. Also note that the standard deviation of the sampling distributions gets smaller with increasing sample size. This is the mathematically way of seeing the shrinking of the spread that we observed graphically. In larger sample sizes, we are less likely to draw a sample mean far away from the true population mean. Central limit theorem and the normal distribution The patterns we are seeing here are well known to statisticians. In fact, they are patterns that are predicted by the most important theorem in statistics, the central limit theorem. We won’t delve into the technical details of this theorem. We can generally interpret the central limit theorem to say: As the sample size increases, the sampling distribution of a sample mean becomes a normal distribution. This normal distribution will be centered on the true population mean \\(\\mu\\) and with a standard deviation equal to \\(\\sigma/\\sqrt{n}\\), where \\(\\sigma\\) is the population standard deviation. What is this “normal” distribution? The name is somewhat misleading because there is nothing particularly normal about the normal distribution. Most real-world distributions don’t look normal, but the normal distribution is central to statistical inference because of the central limit theorem. The normal distribution is a bell-shaped, unimodal, symmetric distribution. It has two characteristics that define its exact shape. The mean of the normal distribution define where its center is and the standard deviation of the normal distribution defines its spread. Lets look at the normal sampling distribution of the mean to become familiar with it. (#fig:normal_dist)The Normal Distribution The distribution is symmetric and bell shaped. The center of the distribution is shown by the red dotted line. This center will always be at the true population mean, \\(\\mu\\). The area of the normal distribution also has a regularity that is sometimes referred to as the “68%,95%,99.7%” rule. Regardless of the actual variable, 68% of all the sample means will be within 68% of the true population mean, 95% of all the sample means will be within 95% of the true population mean, and 99.7% of all the sample means will be within three standard deviations of the mean. This regularity will become very helpful later on for making statistical inferences. The standard error It is easy to get confused by the number of standard deviations being thrown around in this section. There are three standard deviations we need to keep track of to properly understand what is going on here. Each of these standard deviations is associated with one of the types of distributions I introduced at the beginning of this section. \\(\\sigma\\): the population standard deviation. In our example, this would be the standard deviation of height for all 42 students which is 4.8702139. Typically, like other values in the population, this number is unknown. \\(s\\): the sample standard deviation. In our example, this would be the standard deviation of height from a particular sample of a given size from the class. This number can be calculated for the sample that you have, using the techniques we learned earlier in the class. \\(\\sigma/\\sqrt{n}\\): The standard deviation of the sampling distribution of the sample mean. We divide the population standard deviation \\(\\sigma\\) by the square root of the sample size. In general, we refer to the standard deviation of the sampling distribution as the standard error, for short. So remember that when I refer to the “standard error” I am using shorthand for the “standard deviation of the sampling distribution.” Other sample statistics In the example here, I have focused on the sample mean as the sample statistic of interest. However, the logic of the central limit theorem applies to several other important sample statistics of interest to us. In particular, the sampling distributions of: means proportions mean differences proportion differences correlation coefficients all become normal as the sample size increases. Thus, this normal distribution becomes critically important in making statistical inferences. Note that the standard error formula \\(\\sigma/\\sqrt{n}\\) only applies to the sampling distribution of sample means. Other sample statistics have different formulas for their standard errors, which I will introduce in the next section. What can we do with the sampling distribution? Now that we know the sampling distribution of the sample mean should be normally distributed, what can we do with that information? The sampling distribution gives us information about how we would expect the sample means to be distributed. This seems like it should be helpful in figuring out whether we got a value close to the center or not. However, there is a catch. We don’t know either \\(\\mu\\), the center of this distribution or \\(\\sigma\\) which we need to calculate its standard deviation. Thus, we know theoretically what it should look like but we have no concrete numbers to determine its actual shape. We can fix the problem with not knowing \\(\\sigma\\) fairly easily. We don’t know \\(\\sigma\\) but we do have an estimate of it in \\(s\\), the sample standard deviation. In practice, we us this value to calculate an estimated standard error of \\(s/\\sqrt{n}\\). However, this substitution has consequences. Because we are using a sample statistic subject to random bias to estimate the standard error, this creates greater uncertainty in our estimation. I will come back to this issue in the next section. We still have the more fundamental problem that we don’t know where the center of the sampling distribution should be. In order to make statistical inferences, we are going to employ two different methods that make use of what we do know about the sampling distribution: Confidence intervals: Provide a range of values within which you feel confident that the true population mean resides. Hypothesis tests: Play a game of make believe. If the true population mean was a given value, what is the probability that I would get the sample mean value that I actually did? I will discuss these two different methods in the next two sections. "],
["confidence-intervals.html", "Confidence Intervals", " Confidence Intervals As Figure 38 shows, 95% of all the possible sample means will be within 1.96 standard errors of the true population mean \\(\\mu\\). Figure 38: 95% of all sample means will be within 1.96 standard errors of the true population mean. Lets say I were to construct the following interval for every possible sample: \\[\\bar{x}\\pm1.96(\\sigma/\\sqrt{n})\\] It follows from the statement above that for 95% of all samples, this interval would contain the true population mean, \\(\\mu\\). To see how this works graphically, imagine constructing this interval for twenty different samples from the same population. Figure 39 shows the mean and interval for each of these twenty samples, relative to the true population mean. Figure 39: Intervals of sample mean plus and minus 1.96 standard errors for 20 different samples, with the true population mean shown by the blue line. Sample means shown by dots. You can see that these sample means fluctuate around the true population mean due to random sampling error. The lines give the interval outlined above. In 19 out of the 20 cases, this interval contains the true population mean (as you can see by the fact that the interval crosses the blue line). The one sample where this is not true is shown in red. On average, 95% of samples will contain the true population mean in the interval, so 5% or 1 in 20 will not. We refer to this interval as the 95% confidence interval. Of course, in practice, we only construct one interval on the sample that we have. We use this interval to give a range of values that we feel “confident” will contain the true population mean. What do we mean by “confident?” The term “confident” is a little ambiguous. Given my statements above, it might be tempting to interpret the 95% confidence interval to mean that there is a 95% probability that the true population mean is within the interval. This interpretation seems intuitive and straightforward, but that interpretation is incorrect according to the classic approach to inference. The problem here is subtle, but from the classical viewpoint, probability is an objective phenomenon that relates to the outcomes of future processes over the long run. From this viewpoint, we cannot express our subjective uncertainties about numbers in terms of probabilities.The population mean is a single static number. This leads us to a sort of yoda-like statement: The population mean is either in your interval or it is not. There is no probability. This is why we use a more neutral term like “confidence.” If we want to be long-winded about it, we might say that we are 95% confident because “in 95% of all the possible samples I could have drawn, the true population mean would be in the interval. I don’t know if I have one of those 95% or the unlucky 5%, but nonetheless, there it is.” If this all seems a bit confusing, you are perfectly normal. As I said, this is the classic view of probability. Intuitively, people often think of uncertainty in probabilistic terms (e.g. what are the odds your team will win the game?). Many contemporary statisticians would in fact agree that it is perfectly okay to express subjective uncertainty as a probability. But, I still need to let you know that from the classic approach, interpreting your confidence interval as a probability statement is a no-no. Calculating the confidence interval for the sample mean Okay, lets try calculating a confidence interval. Lets try this out for age in the politics dataset. The formula is: \\[\\bar{x}\\pm1.96(\\sigma/\\sqrt{n})\\] Oh wait, we can’t do it! We don’t know the value of the population standard deviation \\(\\sigma\\). As I explained in the last section, we are going to have to do a little “fudging” here. Instead of \\(\\sigma\\), we can use our sample standard deviation \\(s\\). However, doing so will have consequences. Here is our new formula: \\[\\bar{x} \\pm t*(s\\sqrt{n})\\] As you can see, I have replaced the 1.96 with some number \\(t\\), referred to as the t-statistic. Basically to adjust for the greater uncertainty in using a sample statistic in my calculation of the standard error, I need to increase the number here slightly from 1.96. How much I increase it will depend on the degrees of freedom which are given by the sample size minus one (\\(n-1\\)). To figure out the correct t-statistic, I can use the qt command in R. qt(0.975, nrow(politics)-1) ## [1] 1.960524 The first command to qt is the confidence you want. This is a little bit tricky because for a 95% confidence interval, we actually want to input 0.975. This is because we are basically asking for only the upper tail of that normal distribution shown at the beginning of this section. This area contains only 2.5% of the area outside, with the other 2.5% being in the lower tail. The second number is the degrees of freedom which equals \\(n-1\\). In this case, we have such a large sample, that the t-statistic we need is very close to 1.96. In smaller samples, using the t-statistic rather than 1.96 can make a bigger difference. Its not a proper sample, but lets take the case of the crime data. Here there are only 51 observations, so the t-statistic is: qt(0.975, 51-1) ## [1] 2.008559 The difference from 1.96 is a little more noticeable. Lets return to the politics data. We now have all the information we need to calculate the 95% confidence interval: xbar &lt;- mean(politics$age) sdx &lt;- sd(politics$age) n &lt;- nrow(politics) se &lt;- sdx/sqrt(n) t &lt;- qt(0.975,n-1) xbar+t*se ## [1] 50.03165 xbar-t*se ## [1] 48.97307 We are 95% confident that the mean age among all US adults is between 48.97 and 50.03 years of age. As you can see, the large sample of nearly 6,000 respondents produces a very tight confidence interval. Calculating the confidence interval for other sample statistics As noted in the previous section, the sampling distribution of other sample statistics such as proportions, mean differences, and regression slopes is also normally distributed in large enough samples. This means that we can use the same approach to construct confidence intervals for other sample statistics. The general form of the confidence interval is: \\[\\texttt{(sample statistic)} \\pm t*\\texttt{(standard error)}\\] In order to do this for any of the above sample statistics, we only need to know how to calculate that sample statistic’s standard error and the degrees of freedom used to look up the t-statistic for that sample statistic. Table 12 provides a useful cheat sheet of those formulas: Table 12: Cheat sheet of equations for calculating standard errors and degrees of freedom Type SE df for \\(t\\) Mean \\(s/\\sqrt{n}\\) \\(n-1\\) Proportion \\(\\sqrt\\frac{\\hat{p}*(1-\\hat{p})}{n}\\) \\(n-1\\) Mean Difference \\(\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\) min(\\(n_1-1\\),\\(n_2-1\\)) Proportion Difference \\(\\sqrt{\\frac{\\hat{p}_1*(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2*(1-\\hat{p}_2)}{n_2}}\\) min(\\(n_1-1\\),\\(n_2-1\\)) Correlation Coefficient \\(\\sqrt{\\frac{1-r^2}{n-2}}\\) \\(n-2\\) I know some of that math might look intimidating but I will go through an example of each case below to show you how it works for each case. Example with proportions As an example, lets use the proportion of respondents who do not believe in anthropogenic climate change. In our politics sample, we get: table(politics$globalwarm) ## ## No Yes ## 1180 3058 n &lt;- 1180+3058 p_hat &lt;- 1180/n n ## [1] 4238 p_hat ## [1] 0.2784332 About 27.8% of the respondents in our sample are climate change deniers. What can we conclude about the proportion in the US population? First, lets figure out the t-statistic. We use the same \\(n-1\\) for degrees of freedom: t_stat &lt;- qt(0.975, n-1) t_stat ## [1] 1.960524 Our sample is large enough that we are basically using 1.96. Now we need to calculate the standard error. The formula from above is: \\[\\sqrt{\\frac{\\hat{p}*(1-\\hat{p})}{n}}\\] The term \\(hat{p}\\) is the standard way to represent the sample proportion, which in this case is 0.279. So, our formula is: \\[\\sqrt{\\frac{0.278*(1-0.278)}{4238}}\\] We can calculate this in R: se &lt;- sqrt(p_hat*(1-p_hat)/n) se ## [1] 0.006885228 We now have all the pieces to construct the confidence interval: p_hat+t_stat*se ## [1] 0.2919319 p_hat-t_stat*se ## [1] 0.2649346 We are 95% confident that the true percentage of climate change deniers in the the US population is between 26.5% and 29.2%. Example with mean differences using our Add health data, what is the mean difference in popularity (number of friend nominations) between frequent smokers and those who do not smoke frequently? tab &lt;- tapply(popularity$indegree, popularity$smoker, mean) tab ## Non-smoker Smoker ## 4.506699 4.796992 mean_diff &lt;- 4.796992 - 4.506699 mean_diff ## [1] 0.290293 In our sample data, frequent smokers had 0.290 more friends on average than those who did not smoke frequently. What is the confidence interval for that value in the population? We start by calculating the t-statistic for this confidence interval. We use the size of the smaller group minus one for the degrees of freedom. table(popularity$smoker) ## ## Non-smoker Smoker ## 3732 665 n1 &lt;- 3732 n2 &lt;- 665 t_stat &lt;- qt(.975, n2-1) t_stat ## [1] 1.963543 The value is pretty close to 1.96 but a little bigger. Now we need to calculate the standard error. The formula is: \\[\\sqrt{\\frac{s_1^2}{n_1}+\\frac{s_2^2}{n_2}}\\] We already have \\(n_1\\) and \\(n_2\\), so we just need to get the standard deviation of friend nominations for the two groups to get \\(s_1\\) and \\(s_2\\). We can do this with another tapply command, but changing from mean to sd in the third argument. tapply(popularity$indegree, popularity$smoker, sd) ## Non-smoker Smoker ## 3.652224 3.901188 s1 &lt;- 3.652224 s2 &lt;- 3.901188 se &lt;- sqrt(s1^2/n1+s2^2/n2) se ## [1] 0.1626661 Now we have all the pieces to put together the confidence interval: mean_diff - t_stat*se ## [1] -0.02910896 mean_diff + t_stat*se ## [1] 0.609695 We are 95% confident that in the population of US adolescents, those who smoke frequently have between 0.03 fewer to 0.61 more friend nominations, on average, than those who do not smoke frequently. Note that because our confidence interval contains both negative and positive value, we cannot be confident about whether smoking is truly associated with having more or less friends. The direction of the relationship between the two variables is uncertain. Example with proportion differences Lets continue to use the Add Health data. Do we observe a gender difference in smoking behavior in our sample? prop.table(table(popularity$smoker, popularity$sex), 2) ## ## Female Male ## Non-smoker 0.8534894 0.8435407 ## Smoker 0.1465106 0.1564593 p_hat_f &lt;- 0.1465106 p_hat_m &lt;- 0.1564593 p_hat_diff &lt;- p_hat_m-p_hat_f p_hat_diff ## [1] 0.0099487 In our sample, about 14.7% of girls were frequent smokers and about 15.6% of boys were frequent smokers. The percentage of boys who smoke is about 1% higher than the percentage of girls. Do we think this moderate difference in the sample is true in the population? Lets start again by calculating the appropriate t-statistic for our confidence interval. We use the same procedure as for mean differences above, choosing the size of the smaller group for the degrees of freedom. However, its important to note that our groups are now boys and girls, not smokers and non-smokers. table(popularity$sex) ## ## Female Male ## 2307 2090 n_f &lt;- 2307 n_m &lt;- 2090 t_stat &lt;- qt(.975, n_m-1) t_stat ## [1] 1.9611 Now we can calculate the standard error. The formula is: \\[\\sqrt{\\frac{\\hat{p}_1*(1-\\hat{p}_1)}{n_1}+\\frac{\\hat{p}_2*(1-\\hat{p}_2)}{n_2}}\\] That looks like a lot, but we just have to focus on plugging in the right values in R: se &lt;- sqrt((p_hat_f*(1-p_hat_f)/n_f)+(p_hat_m*(1-p_hat_m)/n_m)) se ## [1] 0.01083286 Now we have all the parts to calculate the confidence interval: p_hat_diff - t_stat*se ## [1] -0.01129562 p_hat_diff + t_stat*se ## [1] 0.03119302 We are 95% confident that in the population of US adolescents, between 1.1% fewer to 3.1% more boys smoke frequently than girls. As above, because our confidence interval includes both negative and positive values, we are not very confident at all about whether boys or girls smoke more frequently. Example with correlation coefficient Lets stick with the Add Health data. What is the correlation between GPA and the number of friend nominations that a student receives? r &lt;- cor(popularity$pseudoGPA, popularity$indegree) r ## [1] 0.1680881 In our sample, there is a moderately positive correlation between a student’s GPA and the number of friend nominations that a student receives. What is our confidence interval for the population? For the t-statistic, we use \\(n-2\\) for the degrees of freedom: n &lt;- nrow(popularity) t_stat &lt;- qt(.975, n-2) For the standard error, the formula is: \\[\\sqrt{\\frac{1-r^2}{n-2}}\\] This is straightforward to calculate in R: se &lt;- sqrt((1-r^2)/(n-2)) se ## [1] 0.01486952 Now we have all the parts we need to calculate the confidence interval: r - t_stat*se ## [1] 0.1389363 r + t_stat*se ## [1] 0.1972398 We are 95% confident that the true correlation coefficient between GPA and friend nominations in the population of US adolescents is between 0.139 and 0.197. While there is some difference in the strength of that relationship, we are pretty confident that the correlation is moderately positive. "],
["hypothesis-tests.html", "Hypothesis Tests", " Hypothesis Tests In social scientific practice, hypothesis testing is far more common than confidence intervals as a technique of statistical inference. Both techniques are fundamentally derived from the sampling distribution and produce similar results, but the methodology and interpretation of results is very different. In hypothesis testing, we play a game of make believe. Remember that the fundamental issue we are trying to work around is that we don’t know the value of the true population parameter and thus we don’t know where the center is for the sampling distribution of the sample statistic. In hypothesis testing, we work around this issue by boldly asserting what we think the true population parameter. We then test whether the data that we got are reasonably consistent with that assertion. Example: Coke winners Lets take a fairly straightforward example. Coca-Cola used to run promotions where a certain percentage of bottle caps were claimed to win you another free coke. In one such promotion, when I was in graduate school, Coca-Cola ran a promotion where they claimed that 1 in 12 bottles were winners. If this is true, then 8.3% (1/12=0.083) of all the coke bottles in every grocery store and mini mart should be winners. Being a grad student who needed to stay up late writing a dissertation fueled by caffeine and “sugar,” I use to drink quite a few Cokes. After only receiving a few winners after numerous attempts, I began to get suspicious of the claim. I started collecting bottle caps to see if I could statistically find evidence of fraudulent behavior. For the sake of this exercise, lets say I collected 100 coke bottle caps (I never got this high in practice, but its a nice round number) and that I only got five winners. My winning percentage is 5% which is lower than Coke’s claim of 8.3%. The critical question is whether it is likely or unlikely that I would get a winning percentage this different from the claim in a sample of 100 bottle caps. That is what a hypothesis test is all about. We are asking whether the data that we got are likely under some assumption about the true parameter. If they are unlikely, then we reject that assumption. if they are not unlikely, then we do not reject the assumption. We call that assumption the null hypothesis, \\(H_0\\). The null hypothesis is a statement about what we think the true value of the parameter is. The null hypothesis is our “working assumption” until we can be proven to be wrong. In this case, the parameter of interest is the true proportion of winners among the population of all Coke bottles in the US. Coke claims that this proportion is 0.083, so this is my null hypothesis. In mathematical terms, we write: \\[H_0: \\rho=0.083\\] I use the Greek letter \\(\\rho\\) as a symbol for the population proportion. I will use \\(\\hat{p}\\) to represent the sample proportion in my sample, which is 0.05. Some standard statistical textbooks will also claim that there is an “alternative hypothesis.” That alternative hypothesis is specified as “anything but the null hypothesis.” In my opinion, this is incorrect because vague statements about “anything else” do not constitute an actual hypothesis about the data. We are testing only whether the data are consistent with the null hypothesis. No other hypothesis is relevant. We got a sample proportion of 0.05 on a sample of 100. Assuming the null hypothesis is true, what would the sampling distribution look like from which I pulled my 0.05? Note the part in bold above. We are now playing our game of make believe. We know that on a sample of 100, the sample proportion should be normally distributed. It should also be centered on the true population proportion. Because we are assuming the null hypothesis is true, it should be centered on the value of 0.083. The standard error of this sampling distribution is given by: \\[\\sqrt{\\frac{0.083*(1-0.083)}{100}}=0.028\\] Therefore, we should have a sampling distribution that looks like: Figure 40: A game of make believe, or the sampling distribution for sample proportion of winning Coca-Cola bottle caps assuming the null hypothesis is true The blue line shows the true population proportion assumed by the null hypothesis. The red line shows my actual sample proportion. The key question of hypothesis testing is whether the observed data (or more extreme data) are reasonably likely under the assumption of the null hypothesis. Practically speaking, I want to know how far my sample proportion is from the true proportion and whether this distance is far enough to consider it unlikely. To calculate how far away I am on some standard scale, I divide the distance by the standard error of the sampling distribution to calculate how many standard errors my sample proportion is below the population parameter (assuming the null hypothesis is true). \\[\\frac{0.05-0.083}{0.028}=\\frac{-0.033}{0.028}=-1.18\\] My sample proportion is 1.18 standard errors below the center of the sampling distribution. Is this an unlikely distance? To figure this out, we need to calculate the area in the lower tail of the sampling distribution past my red line. This number will tell us the proportion of all sample proportions that would be 0.05 or lower, assuming the null hypothesis is true. This standardized measure of how far is sometimes called the test statistic for a given hypothesis test. Calculating this area is not a trivial exercise, but R provides a straightforward command called pt which is somewhat similar to the qt command above. We just need to feed in how many standard errors our estimate is away from the center (-1.18) and the degrees of freedom. These degrees of freedom are identical to the ones used in confidence intervals (in this case, \\(n-1\\), so 99). pt(-1.18, 99) ## [1] 0.1204139 There is one catch with this command. It always gives you the area in the lower tail, so if your sample statistic is above the center, you should still put in a negative value in the first command. We will see an example of this below. Our output indicates that 12% of all samples would produce a sample proportion of 0.05 or less when the true population proportion is 0.083. Graphically it looks like this: Figure 41: The proportion of all possible sample proportions that are lower than our sample proportion, assuming the null hypothesis is true The grey area is the area in the lower tail. It would seem that we are almost ready to conclude our hypothesis test. However, there is a catch and its a tricky one. Remember that I was interested in the probability of getting a sample proportion this far or farther from the true population proportion. This is not the same as getting a sample proportion this low or lower. I need to consider the possibility that I would have been equally suspicious if I had got a sample proportion much higher than 8.3%. In mathematically terms, that means I need to take the area in the upper tail as well, where I am .033 above the true population proportion. This is called a two-tailed test. Luckily, because the normal distribution is symmetric, this area will be identical to the area in the lower tail and so I can just double this percent. Figure 42: We have to also consider the possibility of getting a sample proportion as far from the population proportion but in the other direction Assuming the null hypothesis is true, there is a 24% chance of getting a sample proportion as far from the true population mean or farther, just by random chance. We call this probability the p-value. The p-value is the ultimate goal of the hypothesis test. All hypothesis tests produce a p-value and it is the p-value that we will use to make a decision about our test. What should that decision be? We have only two choices. If the p-value is low enough, then it is unlikely that we would have gotten this data or data more extreme, assuming the null hypothesis is true. Therefore, we reject the null hypothesis. If the p-value is not low enough, then it is reasonable that we would have gotten this data or data more extreme, assuming the null hypothesis is true. Therefore, we fail to reject the null hypothesis. Note that we NEVER accept or prove the null hypothesis. It is already our working assumption, so the best we can do for it is to fail to reject it and thus continue to use it as our working assumption. How low does the p-value need to be in order to reject it? There is no right answer here, because this is a subjective question. However, there is a generally agreed upon practice in the social sciences that we reject the null hypothesis when the p-value is at or below 0.05 (5%). Note that while there is general consensus around this number, it is an arbitrary cutpoint. The practical difference between a p-value of 0.049 and 0.051 is negligible, but under this arbitrary standard, we would make different decisions in each case. I would rather that you just learn to think about what the p-value represents and reach your own decision. No reasonable scientist, however, would reject the null hypothesis with a p-value of 24% as we have in our Coke case. Nearly 1 in 4 samples of size 100 would produce a sample proportion this different from the assumed true proportion of 8.3% just by random chance. I therefore do not have sufficient evidence to reject the null hypothesis that Coke is telling the truth. Note that I have not proved that Coke is telling the truth. I have only failed to produce evidence that they are lying. The general procedure of hypothesis testing The general procedure of hypothesis testing is as follows: State a null hypothesis. This null hypothesis is a claim about the true value of an unknown parameter. Calculate a test statistic that tells you how far your sample statistic is from the center of the sampling distribution, assuming the null hypothesis is true. For our purposes, this test statistic will always be the number of standard errors above or below the true population parameter, assuming the null hypothesis is true. Calculate the p-value for the test statistic. The p-value is the probability of getting a sample statistic this far or farther (in absolute value) from the true population parameter, assuming the null hypothesis is true. If the p-value is below some threshold (typically 0.05), reject the null hypothesis. Otherwise, fail to reject the null hypothesis. Interpreting p-values correctly P-values are widely misunderstood in practice. Studies have been done of practicing researchers across different disciplines where these researchers were asked to interpret a p-value from a multiple choice question and the majority get it wrong. Therefore, don’t feel bad if you are having trouble understanding a p-value. You are in good company! Nonetheless, proper interpretation of a p-value is critically important for our understanding of what a hypothesis test does. The reason many people get the interpretation of p-values wrong is that they want the p-value to express the probability of a hypothesis being correct or incorrect. People routinely misinterpret the p-value as a statement about the probability of the null hypothesis being correct. The p-value is NOT a statement about the probability of a hypothesis being correct or incorrect. For the same reason that we cannot call a confidence interval a probability statement, the classical approach dictates that we cannot characterize our subjective uncertainty about whether hypotheses are true or not by a probability statement. The hypothesis is either correct or it is not. There is no probability. Correctly interpreted, the p-value is a probability statement about the data, not about the hypothesis. Specifically, we are asking what the probability is of observing data this extreme or more extreme, assuming the null hypothesis is true. We are not making a probability statement about hypotheses. Rather we are assuming a hypothesis and then asking about the probability of the data. This difference may seem subtle, but it is in fact quite substantial in interpretation. The reason why everyone (including you and me) struggles with this is that our brains want it to be the other way around. Ultimately by rejecting or failing to reject we are making statements about whether we believe the hypothesis or not, but we are not doing that directly by a probability statement about the hypothesis but rather a probability statement about the likelihood of the data given the hypothesis. Hypothesis tests of relationships The hypothesis tests that we care the most about in the sciences are hypothesis tests about relationships between variables. We want to know whether the association we are observing in the sample is true in the population. In all of these cases, our null hypothesis is that there is no association, and we want to know whether the association we observe in the sample is strong enough to reject this null hypothesis of no association. We can do hypothesis tests of this nature for both mean differences and regression slopes. Example: mean differences Lets look at differences in mean income (measured in $1000) by religion in the politics dataset. tapply(politics$income, politics$relig, mean) ## Mainline Protestant Evangelical Protestant Catholic ## 81.83439 58.32606 77.53498 ## Jewish Non-religious Other ## 120.92958 88.62963 60.75311 I want to look at the difference between Evangelical Protestants and “Other Religions.” The mean difference here is: \\[58.32606-60.75311=-2.42705\\] Evangelical Protestants make $2,427 less than members of other religions, in my sample. Let me set up a hypothesis test where the null hypothesis is that Roman Catholics and members of other religions have the same income, or in other words, the mean difference in income is zero: \\[H_0: \\mu_c-\\mu_o=0\\] Where \\(\\mu_c\\) is the population mean income of evangelical Protestants and \\(\\mu_o\\) is the population mean income of members of other religions. In order to figure out how far my sample mean difference of -2.427 is from 0, I need to find the standard error of the mean difference. The formula for this number is: \\[\\sqrt{\\frac{s_c^2}{n_c}+\\frac{s_o^2}{n_o}}\\] I can calculate this in R: tapply(politics$income, politics$relig, sd) ## Mainline Protestant Evangelical Protestant Catholic ## 62.90763 51.23396 66.59281 ## Jewish Non-religious Other ## 89.84166 71.46392 56.37886 table(politics$relig) ## ## Mainline Protestant Evangelical Protestant Catholic ## 785 917 1015 ## Jewish Non-religious Other ## 71 567 883 se &lt;- sqrt(51.23396^2/917+56.37886^2/883) -2.42705/se ## [1] -0.9547436 The t-statistic of -0.95 here is not very large. I am only 0.44 standard errors below 0 on the sampling distribution, assuming the null hypothesis is true. Lets go ahead and calculate the p-value for this t-statistic. Remember that I need to put in the negative version of this number to the pt command. I also need to use the smaller of the two sample sizes for my degrees of freedom: 2*pt(-0.95, 883-1) ## [1] 0.3423725 In a sample of this size, there is an 34% chance of observing a mean income difference of $2,427 or more between evangelical Protestants and members of other religions, just by sampling error, assuming that there is no difference in income in the population. Therefore, I fail to reject the null hypothesis that evangelical Protestants and members of other religions make the same income. Example of proportion differences Lets look at the difference in smoking behavior between white and black students in the Add Health data. Our null hypothesis is: \\[H_0: \\rho_w-\\rho_c=0\\] In simple terms, our null hypothesis is that the same proportion of white and black adolescents smoke frequently. Lets look at the actual numbers from our sample: prop.table(table(popularity$smoker, popularity$race),2) ## ## White Black/African American Latino ## Non-smoker 0.79560106 0.94847162 0.89000000 ## Smoker 0.20439894 0.05152838 0.11000000 ## ## Asian/Pacific Islander Other ## Non-smoker 0.90123457 0.92592593 ## Smoker 0.09876543 0.07407407 ## ## American Indian/Native American ## Non-smoker 0.80769231 ## Smoker 0.19230769 p_w &lt;- 0.20439894 p_b &lt;- 0.05152838 p_diff &lt;- p_w-p_b p_diff ## [1] 0.1528706 About 20.4% of white students smoked frequently, compared to only 5.2% of black students. The difference in proportion is a large 15.3% in the sample. This would seem to contradict our null hypothesis. However, we need to confirm that a difference this large in a sample of our size is unlikely to happen by random chance. To do that we need to calculate the standard error, just as we learned to do it for proportion differences in the confidence interval section: table(popularity$race) ## ## White Black/African American ## 2637 1145 ## Latino Asian/Pacific Islander ## 400 162 ## Other American Indian/Native American ## 27 26 n_w &lt;- 2637 n_b &lt;- 1145 se &lt;- sqrt((p_w*(1-p_w)/n_w)+(p_b*(1-p_b)/n_b)) se ## [1] 0.01021531 Now many standard errors is our observed difference in proportion from zero? t_stat &lt;- p_diff/se t_stat ## [1] 14.96485 Wow, thats a lot. We can be pretty confident already without the final step of the p-value, but lets calculate it anyway. Remember ot always take the negative version of the t-statistic you calculated: 2*pt(-14.96485, n_b-1) ## [1] 2.247969e-46 The p-value is astronomically small. In a sample of this size, the probability of observing a difference in the proportion frequent smokers between whites and blacks of 15.3% or larger if there is no difference in the population is less than 0.0000001%. Therefore, I reject the null hypothesis and conclude that white students are more likely to be frequent smokers than are black students. Example of correlation coefficient Lets look at the correlation between the parental income of a student and the number of friend nominations they receive. Our null hypothesis will be that there is no relationship between parental income and student popularity in the population of US adolescents. Lets look at the data in our sample: r &lt;- cor(popularity$parentinc, popularity$indegree) r ## [1] 0.1247392 In the sample we observe a moderately positive correlation between a student’s parental income and the number of friend nominations they receive. How confident are we that we wouldn’t observe such a large correlation coefficient in our sample by random chance if the null hypothesis is true? First, we need to calculate the standard error: n &lt;- nrow(popularity) se &lt;- sqrt((1-r^2)/(n-2)) se ## [1] 0.01496633 How many standard errors are we away from the assumption of zero correlation? r/se ## [1] 8.334658 What is the probability of being that far away from zero for a sample of this size? 2*pt(-8.334658, n-2) ## [1] 1.027779e-16 The probability is very small. In a sample of this size, the probability is less than 0.00000001% of observing a correlation coefficient between parental income and friend nominations received of an absolute magnitude of 0.125 or higher when the true correlation is zero in the population. Therefore, we reject the null hypothesis and conclude that there is a positive correlation between parental income and popularity among US adolescents. Statistical Significance When a researcher is able to reject the null hypothesis of “no association,” the result is said to be statistically significant. This is a somewhat unfortunate phrase that is sometimes loosely interpreted to indicate that the result is “important” in some vague scientific sense. In practice, it is important to distinguish between substantive and statistical significance. In very large datasets, standard errors will be very small, and thus it is possible to observe associations that are very small in substantive size that are nonetheless statistically significant. On the flip side, in small datasets, standard errors will often be large, and thus it is possible to observe associations that are very large in substantive size but not statistically significant. It is important to remember that “statistical significance” is a reference to statistical inference and not a direct measure of the actual magnitude of an association. I prefer the term “statistically distinguishable” to “statistically significant” because it more clearly indicates what is going on. In the previous example, we found that the income differences in our sample between Catholics and members of other religions are not statistically distinguishable from zero. We also found that the negative association in our sample between age and sexual frequency was statistically distinguishable from zero. Establishing whether an association is worthwhile in its substantive effect is a totally different exercise from establishing whether it is statistically distinguishable from zero. It is also important to remember that a statistically insignificant finding is not evidence of no relationship because we never accept the null hypothesis. We have just failed to find sufficient evidence of a relationship. No evidence of an association is not evidence of no association. "],
["building-models-1.html", "Building Models", " Building Models Up to this point, we have learned the elementary components of a good statistical analysis. However, the typical social scientist doesn’t spend that much time with these elementary components. Instead, most social scientific analysis depends on building statistical model. A statistical model is a formal mathematical representation of how we think variables might be related to one another. By building models, we can better understand the relationships between variables and how these relationships are affected by other variables. We will focus on model building in some form or another for all the remaining modules of this course. We will begin with the simplest kind of model: we just try to fit a straight line through a set of points on a scatterplot. Although this approach may not seem very sophisticated, it forms the basis for more advanced modeling techniques we will learn later. After understanding this basic model, often called the *OLS regression model** we will move on to a variety of techniques we can use to build more complicated models that are both more realistic and more informative. Throughout this module, we will focus primarily on issues of interpretation. We are now starting to learn the techniques that you will see presented in real world social science research. Being able to interpret and understand this work is a key objective of this course. Slides for this module can be found here. "],
["the-ols-regression-line.html", "The OLS Regression Line", " The OLS Regression Line Figure 43 shows a scatterplot of the relationship between median age and violent crime rates: Figure 43: Scatterplot of median age and violent crime rates across US states, with a best-fitting straight line drawn through points I have also plotted a line through those points. When you were trying to determine the direction of the relationship many of you were probably imagining a line going through the points already. Of course, if we just tried to “eyeball” the best line, we would get many different results. The line I have graphed above, however, is the best fitting line, according to standard statistical criteria. It is the best-fitting line because it minimizes the total distance from all of the points collectively to the line. This line is called the ordinary least squares regression line ( or OLS regression line, for short). This fairly simply concept of fitting the best line to a set of points on a scatterplot is the workhorse of social science statistics and is the basis for most of the models that we will explore in this module. The Formula for a Line Remember the basic formula for a line in two-dimensional space? In algebra, you probably learned something like this: \\[y=a+bx\\] The two numbers that relate \\(x\\) to \\(y\\) are \\(a\\) and \\(b\\). The number \\(a\\) gives the y-intercept. This is the value of \\(y\\) when \\(x\\) is zero. The number \\(b\\) gives the slope of the line, sometimes referred to as the “rise over the run.” The slope indicates the change in \\(y\\) for a one-unit increase in \\(x\\). The OLS regression line above also has a slope and a y-intercept. But we use a slightly different syntax to describe this line than the equation above. The equation for an OLS regression line is: \\[\\hat{y}_i=b_0+b_1x_i\\] On the right-hand side, we have a linear equation (or function) into which we feed a particular value of \\(x\\) (\\(x_i\\)). On the left-hand side, we get not the actual value of \\(y\\) for the \\(i\\)th observation, but rather a predicted value of \\(y\\). The little symbol above the \\(y\\) is called a “hat” and it indicates the “predicted value of \\(y\\).” We use this terminology to distinguish the actual value of \\(y\\) (\\(y_i\\)) from the value predicted by the OLS regression line (\\(\\hat{y}_i\\)). The y-intercept is given by the symbol \\(b_0\\). The y-intercept tells us the predicted value of \\(y\\) when \\(x\\) is zero. The slope is given by the symbol \\(b_1\\). The slope tells us the predicted change in \\(y\\) for a one-unit increase in \\(x\\). In practice, the slope is the more important number because it tells us about the association between \\(x\\) and \\(y\\). Unlike the correlation coefficient, this measure of association is not unitless. We get an estimate of how much we expect \\(y\\) to change in terms of its units for a one-unit increase in \\(x\\). For the scatterplot in Figure 43 above, the slope is -25.6 and the y-intercept is 1343.9. We could therefore write the equation like so: \\[\\hat{\\texttt{crime rate}_i}=1343.9-25.6(\\texttt{median age}_i)\\] We would interpret our numbers as follows: The model predicts that a one-year increase in age within a state is associated with 25.6 fewer violent crimes per 100,000 population, on average. (the slope) The model predicts that in a state where the median age is zero, the violent crime rate will be 1343.9 crimes per 100,000 population, on average. (the intercept) There is a lot to digest in these interpretations and I want to return to them in detail, but first I want to address a more basic question. How did I know that these are the right numbers for the best-fitting line? Calculating the Best-Fitting Line The slope and intercept of the OLS regression line are determined based on addressing one simple criteria: minimize the distance between the actual points and the line. More formally, we choose the slope and intercept that produce the minimum sum of squared residuals (SSR). A residual is the vertical distance between an actual value of \\(y\\) for an observation and its predicted value: \\[residual_i=y_i-\\hat{y}_i\\] These residuals are also sometimes called error terms, because the larger they are in absolute value, the worse is our prediction. Take a look at the Figure 44 below which shows the residuals graphically as vertical distances between the actual point and the line. Figure 44: Scatterplot with best-fitting line shown in blue and residuals shown in red Unless the points all fall along an exact straight line, there is no way for me to eliminate these residuals altogether, but some lines will produce higher residuals than others. What I am aiming to do is minimize the sum of squared residuals which is given by: \\[SSR = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2\\] I square each residual and then sum them up. By squaring, I eliminate the problem of some residuals being negative and some positive. To see how this all works, you can play around with the interactive example below which allows you to guess slope and intercept for a scatterplot and then see how well you did in minimizing the sum of squared residuals. Fortunately, we don’t have to figure out the best slope and intercept by trial and error, as in the exercise above. There are relatively straightforward formulas for calculating the slope and intercept. They are: \\[b_1=r\\frac{s_y}{s_x}\\] \\[b_0=\\bar{y}-b_1*\\bar{x}\\] The r here is the correlation coefficient. The slope is really just a re-scaled version of the correlation coefficient. We can calculate this with the example above like so: slope &lt;- cor(crimes$MedianAge, crimes$Violent)*sd(crimes$Violent)/sd(crimes$MedianAge) slope ## [1] -25.5795 I can then use that slope value to get the y-intercept: mean(crimes$Violent)-slope*mean(crimes$MedianAge) ## [1] 1343.936 Using the lm command to calculate OLS regression lines in R We could just use the given formulas to calculate the slope and intercept in R, as I showed above. However, the lm command will become particularly useful later in the term when we extend this basic OLS regression line to more advanced techniques. In order to run the lm command, you need to input a formula. The structure of this formula looks like “dependent~independent” where “dependent” and “independent” should be replaced by your specific variables. The tilde (~) sign indicates the relationship. So, if we wanted to use lm to calculate the OLS regression line we just looked at above, I would do the following: model1 &lt;- lm(crimes$Violent~crimes$MedianAge) Please keep in mind that the dependent variable always goes on the left-hand side of this equation. You will get very different answers if you reverse the ordering. In this case, I have entered in the variable names using the data$variable syntax, but lm also offers you a more streamlined way of specifying variables, by including a data option separately so that you only have to put the variable names in the formula, like so: model1 &lt;- lm(Violent~MedianAge, data=crimes) Because I have specified data=crimes, R knows that the variables “Violent” and “MedianAge” refer to variables within this dataset. The result will be the same as the previous command, but this approach makes it easier to read the formula itself. I have saved the output of the lm command into a new object that I have called “model1”. You can call this object whatever you like. This is out first real example of the “object-oriented” nature of R. I can apply a variety of functions to this object in order to extract information about the relationship. If I want to get the most information, I can run a summary on this model. summary(model1) ## ## Call: ## lm(formula = Violent ~ MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -381.76 -118.02 -36.25 99.28 853.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1343.94 434.21 3.095 0.00325 ** ## MedianAge -25.58 11.56 -2.214 0.03154 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188.1 on 49 degrees of freedom ## Multiple R-squared: 0.09092, Adjusted R-squared: 0.07236 ## F-statistic: 4.9 on 1 and 49 DF, p-value: 0.03154 There is a lot information here and we actually don’t know what most of it means yet. All we want is the intercept and slope. These numbers are given by the two numbers in the “Estimate” column of the “Coefficients” section. The intercept is 1343.94 and the slope is -25.58. We could also run the coef command which will give us just the slope and intercept of the model. coef(model1) ## (Intercept) MedianAge ## 1343.9360 -25.5795 This result is much more compact and will do for our purposes at the moment. Adding an OLS regression line to a plot You can easily add an OLS regression line to a scatterplot in ggplot. We can do this using the geom_smooth function. However we also need to specify that our method of smoothing is “lm” (for linear model) with the method=\"lm\" argument. Here is the code for the example earlier: ggplot(crimes, aes(x=MedianAge, y=Violent))+ geom_smooth(method=&quot;lm&quot;, se=TRUE)+ geom_point()+ labs(x=&quot;median age in state&quot;, y=&quot;violent crimes per 100,000 population&quot;)+ theme_bw() Figure 45: Use geom_smooth to plot an OLS regression line with or without a confidence interval band You will notice that Figure 45 also adds a band of grey. This is the confidence interval band for my line and is drawn by default. We will discuss issues of inference for the OLS regression line below. If you want to remove this you can change the se argument in geom_smooth to FALSE. Try it out In the interactive R session below, Create a scatterplot showing the relationship between a student’s parental income and the number of friend nominations they receive using the popularity dataset and then add the best fitting line to your plot. Keep the following in mind: You must combine together different elements of ggplot, in this case ggplot itself and geom_boxplot with a plus sign to display correctly. Be sure to add nice labels for your x axis by adding a labs command. You will observe significant issues with overplotting, so you should use geom_jitter and the alpha argument to help see patterns in the data. Use geom_smooth to add the line. Don’t forget to specify the method. Does the best-fitting line surprise you? What did you expect it to look like? The OLS regression line as a model You will note that I saved the output of my lm command above as model. The lm command itself stands for “linear model.” What do I mean by this term “model?” When we talk about “models” in statistics, we are talking about modeling the relationship between two or more variables in a formal mathematical way. In the case of the OLS regression line, we are predicting the dependent variable as a linear function of the independent variable. Just as the general term model is used to describe something that is not realistic but rather an idealized representation, the same is true of our statistical models. We certainly don’t believe that our linear function provides a correct interpretation of the exact relationship between our two variables. Instead we are trying to abstract from the details and fuzziness of the relationship to get a “big picture” of what the relationship looks like. However, we always have to consider that our model is not a very good representation of the relationship. The most obvious potential problem is if the relationship is non-linear and yet we fit the relationship by a linear model, but there can be other problems as well. I will discuss these more below and the next few sections of this module will give us techniques for building better models. However, we first need to focus on how to interpret the results we just got. Interpeting Slopes and Intercepts Learning to properly interpret slopes and intercepts (especially slopes) is the number one most important thing you will learn all term, because of how common the use of OLS regression is in social science statistics. You simply cannot pass the class unless you can interpret these numbers. So take the time to be careful in interpretation here. Interpreting Slopes In abstract terms, the slope is always the predicted change in \\(y\\) for a one unit increase in \\(x\\). However, this abstract definition will simply not do when you are dealing with specific cases. You need to think about the units of \\(x\\) and \\(y\\) and interpret the slope in concrete terms. There are also a few other caveats to consider. Take the interpretation I used above for the -25.6 slope of median age as a predictor of violent crime rates. My interpretation was: The model predicts that a one year increase in age within a state is associated with 25.6 fewer violent crimes per 100,000 population, on average. There are multiple things going on in this sentence that need to be addressed. First, lets address the phrase “model predicts.” The idea of a model is something we will explore more later, but for now I will say that when we fit a line to a set of points to predict \\(x\\) by \\(y\\), we are applying a model to the data. In this case, we are applying a model that relates \\(y\\) to \\(x\\) by a simple linear function. All of our conclusions are dependent on this being a good model. Prefacing your interpretation with “the model predicts…” highlights this point. Second, a “one year increase in age” indicates the meaning of a one unit increase in \\(x\\). Never literally say a “one unit increase in \\(x\\).” Think about the units of \\(x\\) and describe the change in \\(x\\) in these terms. Third, I use “is associated with” to indicate the relationship. This phrase is intentionally passive. We want to avoid causal language when we describe the relationship. Saying something like “when \\(x\\) increases by one \\(y\\) goes up by \\(b_1\\)” may sound more intuitive, but it also implies causation. The use of “is associated with” here indicates that the two variables are related without implicitly implying that one causes the other. Using causal language is the most common mistake in describing the slope. Fourth, “25.6 fewer violent crimes per 100,000 population” is the expected change in \\(y\\). Again, you always have to consider the unit scale of your variables. In this case, \\(y\\) is measured as the number of crimes per 100,000 population, so a decrease of 25.6 means 25.6 fewer violent crimes per 100,000 population. Fifth, I append the term “on average” to the end of my interpretation. This is because we know that our points don’t fall on a straight line and so we don’t expect a deterministic relationship between median age and violent crime. Rather, we think that if we were to take a group of states that had one year higher median age than another group of states, the average difference between the groups would be -25.6. Lets try a couple of other examples to see how this works. I will use the lm command in R to calculate the slopes and intercepts, which I explain in the section below. First, lets look at the association between age and sexual frequency (I will explain the code I use here later in this section). coef(lm(sexf~educ, data=sex)) ## (Intercept) educ ## 49.7295901 0.0266939 The slope here is 0.03. Education is measured in years and sexual frequency is measured as the number of sexual encounters per year. So, the interpretation of the slope should be: The model predicts that a one year increase in education is associated with 0.03 more sexual encounters per year, on average. There is a tiny positive effect here, but in real terms the relationship is basically zero. It would take you about 100 years more education to get laid 3 more times. Just think of the student loan debt. Now, lets take the relationship between movie runtimes and tomato meter ratings: coef(lm(TomatoMeter~Runtime, data=movies)) ## (Intercept) Runtime ## 5.1074601 0.4054953 The slope is 0.41. Runtime is measured in minutes. The tomato meter is the percent of reviews that were judged to be positive. The model predicts that a one minute increase in movie runtime length is associated with a 0.38 percentage point increase in the movie’s Tomato Meter rating, on average. Longer movies tend to have higher ratings. We may rightfully question the assumption of linearity for this relationship however. It seems likely that if a movie can become too long, so its possible the relationship here may be non-linear. We will explore ways of modeling that potential non-linearity later in the term. Interpreting Intercepts Intercepts give the predicted value of \\(y\\) when \\(x\\) is zero. Again you should never interpret an intercept in these abstract terms but rather in concrete terms based on the unit scale of the variables involved. What does it mean to be zero on the \\(x\\) variable? In our example of the relationship of median age to violent crime rates, the intercept was 1343.9. Our independent variable is median age and the dependent variable is violent crime rates, so: The model predicts that in a state where the median age is zero, the violent crime rate would be 1343.9 crimes per 100,000 population, on average. Note that I use the same “model predicts” and “on average” prefix and suffix for the intercept as I used for the slope. Beyond that I am just stating the predicted value of \\(y\\) (crime rates) when \\(x\\) is zero in the concrete terms of those variables. Is it realistic to have a state with a median age of zero? No, its not. You will never observe a US state with a median age of zero. This is a common situation that often confuses students. In cases when zero falls outside the range of the independent variable, the intercept is not a particular useful number because it does not tell us about a realistic situation. The intercept’s only “job” is to give a number that allows the line to go through the points on the scatterplot at the right level. You can see this in the interactive exercise above if you select the right slope of 148 and then vary the intercept. In general making predictions for values of \\(x\\) that fall outside the range of \\(x\\) in the observed data is problematic. This is often leads to intercepts which don’t make a lot of sense. This problem with zero being outside the range of data is also evident in the other two examples of slopes from the previous section. When looking at the relationship between education and sexual frequency, no respondents are actually at zero years of education and no movies are at zero minutes of runtime. In truth, to fit the line correctly, we only need the slope and one point along the line. It is convenient to choose the point where \\(x=0\\) but there is no reason why we could not choose a different point. It is actually quite easy to calculate a different predicted value along the line by re-centering the independent variable. To re-center the independent variable \\(x\\), we just need to to subtract some constant value \\(a\\) from all the values of \\(x\\), like so: \\[x^*=x-a\\] The zero value on our new variable \\(x^*\\) will indicates that we are at the value of \\(a\\) on the original variable \\(x\\). If we then use \\(x^*\\) in the OLS regression line rather than \\(x\\), the intercept will give us the predicted value of \\(y\\) when \\(x\\) is equal to \\(a\\). Lets try this out on the model predicting violent crimes by median age. We will create a new variable where we subtract 35 from the median age variable and use that in the regression model. crimes$MedianAge.ctr &lt;- crimes$MedianAge-35 coef(lm(Violent~MedianAge.ctr, data=crimes)) ## (Intercept) MedianAge.ctr ## 448.6533 -25.5795 The intercept now gives me the predicted violent crime rate in a state with a median age of 35. In effect, I have moved my y-intercept from zero to thirty-five as is shown in Figure 46 below. Figure 46: Re-centering the independent variable moves the intercept but does not change the slope Its also possible to re-center an independent variable in the lm command without creating a whole new variable. If you surround the re-centering in the I() function within the formula, R will interpret the result of whatever is inside the I() function as a new variable. Here is an example based on the previous example: coef(lm(Violent~I(MedianAge-35), data=crimes)) ## (Intercept) I(MedianAge - 35) ## 448.6533 -25.5795 Try it out In the interactive R session below, create a linear model predicting hourly wages by age where you re-center age on 40. Keep the following in mind: You will need to fill in the formula part of the lm command below. Remember that your dependent variable (in this case wages) goes on the left side of the ~. Use the I(x-number) syntax as shown above to re-center age in the model formula. How would you interpret the intercept in the model output? How good is \\(x\\) as a predictor of \\(y\\)? If I selected a random observation from the dataset and asked you to predict the value of \\(y\\) for this observation, what value would you guess? Your best guess would be to guess the mean of y because this is the case where your average error would be smallest. This error is defined by the distance between the mean of y and the selected value, \\(y_i-\\bar{y}\\). Now, lets say instead of making you guess randomly I first told you the value of another variable \\(x\\) and gave you the slope and intercept predicting \\(y\\) from \\(x\\). What is your best guess now? You should guess the predicted value of \\(\\hat{y}_i\\) from the regression line because now you have some additional information. There is no way that having this information could make your guess worse than just guessing the mean. The question is how much better do you do than guessing the mean. Answering this question will give us some idea of how good \\(x\\) is as a predictor of \\(y\\). We can do this by separating, or partitioning the total possible error in our first case when we guessed the mean, into the part accounted for by \\(x\\) and the part that is unaccounted for by \\(x\\). I demonstrate this partitioning for one observation in our crime data (the state of Alaska) with the scatterplot in Figure 47 below. Figure 47: We can parition the total distance (in red) between an observation’s value of the dependent variable and the mean (the dotted horizontal line) into the part accounted for by the model (in gold) and the residual (in green) that is unaccounted for by the model The distance in red is the total distance between the observed violent crime rate in the state of Alaska and the mean violent crime rate across all states (given by the dotted line). If I were instead to use the OLS regression line predicting the violent crime rate by median age, I would predict a higher violent crime rate than average for Alaska because of its relatively low median age, but I would still predict a crime rate that is too low relative to the actual crime rate. The red line can be partitioned into th gold line which is the improvement in my estimate and the green line which is the error that remains in my prediction from the model. If I could then repeat this process for all of the states, I could calculate the percentage of the total red lines that the gold lines cover. This would give me an estimate of how much I reduce the error in my prediction by using the regression line rather than the mean to predict a state’s violent crime rate. In practice, we actually need to square those vertical distances because some are negative and some are positive and then we can sum them up over all the observations. So we get the following formulas: Total variation: \\(SSY=\\sum_{i=1}^n (y_i-\\bar{y})^2\\) Explained by model: \\(SSM=\\sum_{i=1}^n (\\hat{y}_i-\\bar{y})^2\\) Unexplained by model: \\(SSR=\\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\) The proportion of the variation in \\(y\\) that is explainable or accountable by variation in \\(x\\) is given by \\(SSM/SSY\\). This looks like a kind of nasty calculation, but it turns out there is a much simpler way to calculate this proportion. If we just take our correlation coefficient \\(r\\) and square it. We will get this proportion. This measure is often called “r squared” and can be interpreted as the proportion of the variation in \\(y\\) that is explainable or accountable by variation in \\(x\\). In the example above, we can calculate R squared: cor(crimes$MedianAge, crimes$Violent)^2 ## [1] 0.09091622 About 9% of the variation in violent crime rates across states can be accounted for by variation in the median age across states. Inference for OLS Regression models When working with sample data, our usual issues of statistical inference apply to regression models. In this case, our primary concern is the estimate of the regression slope because the slope measures the relationship between \\(x\\) and \\(y\\). We can think of an underlying OLS regression model in the population: \\[\\hat{y}_i=\\beta_0+\\beta_1x_i\\] We use Greek “beta” values because we are describing unobserved parameters in the population. The null hypothesis in this case would be that the slope is zero indicating no relationship between \\(x\\) and \\(y\\): \\[H_0:\\beta_1=0\\] In our sample, we have a sample slope \\(b_1\\) that is an estimate of \\(\\beta_1\\). We can apply the same logic of hypothesis testing and ask whether our \\(b_1\\) is different enough from zero to reject the null hypothesis. We just need to find the standard error for this sample slope and the degrees of freedom to use for the test and we can do this manually. However, I have good news for you. You don’t have to do any of this by hand because the lm function does it for you automatically. Lets look at the full output of the model predicting violent crime rates from median age again using the summary command: model &lt;- lm(Violent~MedianAge, data=crimes) summary(model) ## ## Call: ## lm(formula = Violent ~ MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -381.76 -118.02 -36.25 99.28 853.41 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1343.94 434.21 3.095 0.00325 ** ## MedianAge -25.58 11.56 -2.214 0.03154 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 188.1 on 49 degrees of freedom ## Multiple R-squared: 0.09092, Adjusted R-squared: 0.07236 ## F-statistic: 4.9 on 1 and 49 DF, p-value: 0.03154 The “Coefficients” table in the middle gives us all the information we need. The first column gives us the sample slope of -25.58. The second column gives us the standard error for this slope of 11.56. The third column gives us the t-statistic derived by dividing the first column by the second column. The final column gives us the p-value for the hypothesis test. In this case, there is about a 3.2% chance of getting a sample slope this large on a sample of 51 cases if the true value in the population is zero. Of course, in this case its nonsensical because we don’t have a sample, but the numbers here will be valuable in cases with real sample data. Regression Line Cautions OLS regression models can be very useful for understanding relationships, but they do have some important limitations that you should be aware of when you are doing statistical analysis. There are three major limitations/cautions to be aware of when using OLS regression: OLS regression only works for linear relationships. Outliers can sometimes exert heavy influence on estimates of the relationship Don’t extrapolate beyond the scope of the data. Linearity By definition, an OLS regression line is a straight line. If the underlying relationship between x and y is non-linear, then the OLS regression line will do a poor job of measuring that relationship. One common case of non-linearity is the case of diminishing returns in which the slope gets weaker at higher values of x. Figure 48 demonstrates a class case of non-linearity in the relationship between a country’s life expectancy and GDP per capita. Figure 48: Scatterplot of GDP per capita and life expectancy across countries, 2007 The relationship is clearly a strongly positive one, but also one of diminishing returns where the positive relationship seems to plateau at higher levels of GDP per capita. This makes sense because the same absolute increase in country wealth at low levels of life expectancy can be used to reduce the incidence of well-understood infectious and parasitic diseases, whereas the same absolute increase in country wealth at high levels of life expectancy must try to reduce the risk of less understood and treatable diseases like cancer. You get more bang for your buck when life expectancy is low. Figure 49 shows what happens if we try to fit a line to this data. ## Warning: Removed 3 rows containing missing values (geom_smooth). Figure 49: Fitting a line to a non-linear relationship will cause systematic errors in your prediction Clearly a straight line is a poor fit. We systematically overestimate life expectancy at low and high GDP per capita and underestimate life expectancy in the middle. Its possible, in some circumstances, to correct for this problem of non-linearity but we will not explore those options in this module. For now, its just important to be aware of the problem and if you see clear non-linearity then you should question the use of an OLS regression line. Outliers and Influential Points An outlier is an influential point if removing this observation from the dataset substantially changes the slope of the OLS regression line. You can try the interactive exercise below to see how removing points changes the slope of your line (click on a point a second time to add it back). Can you identify any influential points? For the case of median age, Utah and DC both have fairly strong influences on the shape of the line. Removing DC makes the relationship weaker, while removing Utah makes the relationship stronger. Outliers will tend to have the strongest influence when their placement is inconsistent with the general pattern. In this case, Utah is very inconsistent with the overall negative effect because it has both low median age and low crime rates. Lets say that you have identified an influential point. What then? In truth there is only so much you can do. You cannot remove a valid data point just because it is an influential point. There are two cases where it would be legitimate to exclude the point. First, if you have reason to believe that the observation is an outlier because of a data error, then it would be acceptable to remove it. Second, if you have a strong argument that the observation does not belong with the rest of the cases, because it is logically different, then it might be OK to remove it. In our case, there is no legitimate reason to remove Utah, but there probably is a legitimate reason to remove DC. Washington DC is really a city and the rest of our observations are states that contain a mix of urban and rural population. Because crime rates are higher in urban areas, DC’s crime rates look very exaggerated compared to states. Because of this “apples and oranges” problem, it is probably better to remove DC. If our unit of analysis was cities, on the other hand, then DC should remain. In large datasets (1000+ observations), its unusual that a single point or even a small cluster of points will exert much influence on the shape of the line. The concern about influential points is mostly a concern in small datasets like the crime dataset. Thou Doth Extrapolate Too Much Its dangerous enough to assume that a linear relationship holds for your data (see the first point in this module). Its doubly dangerous to assume that this linear relationship holds beyond the scope of your data. Lets take the relationship between sexual frequency and age. We saw in the previous module that the slope here is -1.3 and the intercept is 108. The intercept itself is outside the scope of the data because we only have data on the population 18 years and older. It would be problematic to make predictions about the sexual frequency of 12 year olds, let alone zero-year olds. Another trivial example would be to look at the growth rate of children 5-15 years of age by correlating age with height. It would be acceptable to use this model to predict the height of a 14 year old, but not a 40 year old. We expect that this growth will eventually end sometime outside the range of our data when individuals reach their final adult height. If we extrapolated the data, we would predict that 40 year olds would be very tall. "],
["the-power-of-controlling-for-other-variables.html", "The Power of Controlling for Other Variables", " The Power of Controlling for Other Variables In the previous module, I showed that the OLS regression line predicting sexual frequency by years of education was 0.03. So in my dataset, there is a very small positive association between sexual frequency and years of education. Its possible that this is a causal effect. We could even spin stories about why we think such a positive association (a very small one) might exist. Maybe more educated people appear sexier to the opposite sex. Maybe more educated people take better care of themselves and thus are healthier and more able to have sex. Maybe more educated people are just more sexually liberated. Before we get carried away however its important to consider whether our results might be spurious. Its possible that the positive association between years of education and sexual frequency is driven by a third variable that we haven’t accounted for. This is a common problem in research using observational data. Association does not necessarily mean causation because of the potential for other variables to account for our observed association (and because of the possibility of reverse causation). We refer to such variables as lurking or confounding variables. In this case, the potentially confounding variable that we need to consider is age. Lets look at the association between age and each of our other variables (sexual frequency and education). cor(sex$sexf,sex$age) ## [1] -0.3974668 cor(sex$educ,sex$age) ## [1] -0.06018569 Age is negatively correlated with sexual frequency. We have observed this relationship before and it is not terribly surprising. Older people have less sex, on average. The negative correlation between age and years of education is perhaps a little more surprising. Older people have less education than younger people, on average. This may seem surprising to you because as you get older you have more opportunity to complete more education. However, you have to remember that the data we have are a snapshot in time. We are not tracking individuals over time as they age, but rather looking at differences between older and younger people at a single point in time. This kind of data is often called a cross-sectional dataset. Because we are looking at a single point in time, the age differences really reflect differences in birth cohorts or what people often loosely call “generations.” Remember that this dataset is from 2004. The difference between a 20 year old and a 60 year old is that the 20 year old was born in 1984 and the 60 year old was born in 1944. Because we are comparing birth cohorts, the differences in educational attainment reflect history more than life cycle. Older cohorts were less educated than younger birth cohorts. On average, you will be more educated than your parents and your parents were more educated than your grandparents. Thus, the correlation between age and education is negative. These two negative correlations suggest a spurious reason why we might observe a positive association between sexual frequency and education. Younger people have more education and younger people have more sex. Thus, when we look at the relationship between sexual frequency and education, we see a positive association but that positive association is indirectly driven by youth and the association of youth with both education and sex. How can we examine whether this potential spurious explanation is accurate? It turns out that we can add more than one independent variable to an OLS regression model at the same time. The mathematical structure of such a model would be: \\[\\hat{frequency}_i=b_0+b_1(education_i)+b_2(age_i)\\] We now have two different “slopes”, \\(b_1\\) and \\(b_2\\). These two slopes give the association of education and age, respectively, on sexual frequency, while controlling for the other independent variable. We now have what is called a multivariate OLS regression model. This “controlling” concept is a key point that I will return to below, but first I want to try to think graphically about what this model is doing. In the case of bivariate regression, we thought of fitting a line to a scatterplot in two-dimensional space. We are doing something similar here, but since we now have three variables, our scatterplot is in three dimensions. Figure 50 shows an interactive three-dimensional plot of the three variables. Figure 50: Interactive 3d scatterplot of years of education, age, and sexual frequency The dependent variable is shown on the vertical (z) axis and the two independent variables are shown on the “width” and “depth” axes (x and y). The flat plane shown is defined by the OLS regression model equation above. So, rather than fitting a straight line through the data, I am fitting a plane. Note however that if you rotate the 3d scatterplot to hide the age “depth” dimension, it will then look like a two-dimensional scatterplot between years of education and sexual frequency. In this case, the the edge of the plane would indicate the slope between years of education and sexual frequency. Similarly, I could rotate it the other way to look at the relationship between age and sexual frequency. How do I know what are the best values for \\(b_0\\), \\(b_1\\), and \\(b_2\\) that define my plane? The logic is the same as for bivariate OLS regression: I choose values that minimize the sum of squared residuals (SSR): \\[SSR=\\sum_{i=1}^n (\\hat{y}_i-y_i)^2\\] SSR is a measure of how far the predicted values of the dependent variable are from the actual values, so we want the intercept and slopes that minimizes this error. Unlike the bivariate case, however, there is no simple formula that I can give you for the slope and intercept, without some knowledge of matrix algebra. However, R can calculate the correct numbers for you easily. I am not concerned with your technical ability to calculate these numbers by hand, but I do want you to understand why those are the “best” numbers. They are the best numbers because they minimize the sum of the squared residuals for the model. We can calculate this model in R just by adding another variable to our model in the lm command: model &lt;- lm(sexf~educ+I(age-18), data=sex) coef(model) ## (Intercept) educ I(age - 18) ## 91.062080 -0.427747 -1.303385 Note that as I did in the previous module, I am re-centering age on 18 years so that I have reasonable value for the interpretation of the intercept. In equation form, our model will look like: \\[\\hat{frequency}_i=91.06-0.43(education_i)-1.30(age_i-18)\\] Interpreting results in a multivariate OLS regression models How do we interpret the results? Intercept: The model predicts that 18-year old individuals at with no education will have 91.06 sexual encounters per year, on average. Education Slope: The model predicts that, holding age constant, an additional year of education is associated with 0.43 fewer sexual encounters per year, on average. Age Slope: The model predicts that, holding education constant, an additional year of age is associated with 1.3 fewer sexual encounters per year, on average. The intercept is now the predicted value when all independent variables are zero. My interpretation of the slopes is almost identical to the bivariate case, except for one very important addition. I am now estimating the effect of each independent variable on the dependent variable while holding constant all other independent variables. You could also say “controlling for all other independent variables.” What does it mean to “hold other variables constant?” It means that when we look at the effect of one independent variable, we are looking at how the predicted value of the dependent variable changes while keeping all the other variables the same. For instance, the education effect above is the effect of a one year increase in education among individuals of the same age. Because we are looking at the effect of education among individuals of the same age, age should no longer have a confounding effect on our estimate of the effect of education. Thus holding constant/controlling for other variables helps to remove the potential spurious effect of those variables as confounders. Note how the effect of education on sexual frequency changed once I included age as a control variable. Before controls, I estimated a slightly positive slope (0.03) but now I am estimating a substantial negative slope (-0.43). So my understanding of the relationship between education and sexual frequency is completely reversed. When you compare individuals of the same age, more educated individuals have less sex, on average, than less educated individuals. Crime example Lets build a regression model where we predict the property crime rate in a state by the percent of adults in the state without a high school diploma and the median age of the state’s residents. summary(lm(Property~PctLessHS+MedianAge, data=crimes)) ## ## Call: ## lm(formula = Property ~ PctLessHS + MedianAge, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1080.38 -376.78 12.19 346.82 1600.56 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5136.66 1411.21 3.640 0.000666 *** ## PctLessHS 69.47 24.79 2.803 0.007286 ** ## MedianAge -83.31 35.30 -2.360 0.022368 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 567.2 on 48 degrees of freedom ## Multiple R-squared: 0.2495, Adjusted R-squared: 0.2182 ## F-statistic: 7.977 on 2 and 48 DF, p-value: 0.001021 Note that I am giving you the full output of summary now, but we can find the slopes and intercept by looking at the Estimate column of the “Coefficients” table. “Coefficients” is another term for slopes and intercepts because that it the technical term for these values in the regression model equation. The model is: \\[\\hat{\\texttt{crime}}_i=5137+69(\\texttt{pct less hs}_i)-83(\\texttt{median age}_i)\\] The model predicts that, comparing two states with the same median age of residents, a one percent increase in the percent of the state with less than a high school diploma is associated with an increase of 69 property crimes per 100,000, on average. The model predicts that, comparing two states with the same percentage of adults without a high school diploma, a one year increase in the median age of a state’s residents is associated with a decrease of 83 property crimes per 100,000, on average. Note that we also get the \\(R^2\\) value from the summary command. In multivariate models, the \\(R^2\\) value always tells you what proportion of the variation in the dependent variable is accountable for by variation in all of the independent variables combined. In this case \\(R^2\\) is 0.2495. About 25% of the variation in property crime rates across states is accountable for by variation in the percent of adults without a high school diploma and the median age of residents across states. Including more than two independent variables If we can include two independent variables in a regression model, why stop there? Why not include three or four or more? The number of independent variables you can include is only limited by the sample size (you can never have more independent variables than the sample size minus one), although in practice we generally stop well short of this limit for pragmatic reasons. Lets take the model above predicting property crime rates by percent of adults with less than a high school diploma and the median age of residents. Lets add the poverty rate as another predictor: summary(lm(Property~PctLessHS+MedianAge+Poverty, data=crimes)) ## ## Call: ## lm(formula = Property ~ PctLessHS + MedianAge + Poverty, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1174.43 -236.69 -30.96 286.41 1218.77 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4240.38487 1383.49276 3.065 0.0036 ** ## PctLessHS 0.04504 36.07642 0.001 0.9990 ## MedianAge -73.27098 33.68882 -2.175 0.0347 * ## Poverty 97.67933 38.52145 2.536 0.0146 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 537.6 on 47 degrees of freedom ## Multiple R-squared: 0.3398, Adjusted R-squared: 0.2976 ## F-statistic: 8.063 on 3 and 47 DF, p-value: 0.0001947 The model predicts: A one percent increase in the percent of adults in a state without a high school diploma is associated with 0.05 more property crimes per 100,000, on average, holding constant the median age of residents and the poverty rate in a state. This result is about as close to zero as you will find. A one year increase in the median age of a state’s residents is associated with 73 fewer property crimes per 100,000, on average, holding constant the percent of adults without a high school diploma and the poverty rate in a state. A one percent increase in a state’s poverty rate is associated with 98 more crimes per 100,000, on average, holding constant the percent of adults without a high school diploma and the median age of residents in a state. 34% of the variation in property crime rates across states can be accounted for by variation in the percent of adults without a high school diploma, residents’ median age, and the poverty rates across states. When I interpret the models now, I am holding constant the other two variables when I estimate the effect of each. Note that controlling for the poverty rate has a huge effect on the education variable whose effect goes from a substantial positive effect to basically zero effect. What does this tell us? Poverty rates and high school dropout rates are positively correlated and so when you don’t control for poverty rates, it looks like the high school dropout rate predicts crime because states with high high school dropout rates have high poverty rates and high poverty rates predict property crime rates. Once you control for the poverty rate, you see that it is economic deprivation not educational deprivation that is driving the crime rate. In general, the form of the multivariate regression model is: \\[\\hat{y}_i=b_0+b_1x_{i1}+b_2x_{i2}+b_3x_{i3}+\\ldots+b_px_{ip}\\] The intercept is given by \\(b_0\\). This is the predicted value of \\(y\\) when all of the independent variables are zero. The remaining \\(b\\)’s give the slopes for all of the variables up through the \\(p\\)th variable. Each of these gives the predicted change in \\(y\\) for a unit increase in that independent variable, holding all other independent variables constant. How to read a table of regression results In academic journal articles and books, the results of OLS regression models are represented in a fairly standard way. In order to understand how to read these articles, you need to understand this presentation style. Its not immediately intuitive for everyone. Table 13 below shows the typical style. In this table, I am reporting three regression models with the property crime rates as the dependent variable and three different independent variables. Table 13: OLS regression models predicting violent crime rates for US states Model 1 Model 2 Model 3 Intercept 1892.85*** 5136.66*** 4240.38** (335.33) (1411.21) (1383.49) Percent Less than HS 78.83** 69.47** 0.05 (25.58) (24.79) (36.08) Median Age -83.31* -73.27* (35.30) (33.69) Poverty Rate 97.68* (38.52) R-squared 0.16 0.25 0.34 N 51 51 51 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05. Standard errors in parenthesis. When reading this table and others like it, keep the following issues in mind: The first question you should ask is “what is the dependent variable?” This is the outcome that we are trying to predict. Typically, the dependent variable will be listed in the title of the table. In this case, the title tells you that the dependent variable is property crime rates and the unit of analysis is US states. The independent variables are listed on the rows of the table. In this case, I have independent variables of percent less than HS, median age, and the poverty rate. As I will explain below, just because an independent variable is listed here does not mean that it is actually included in all models. Models are listed in each column of the table. If numbers are listed for the row of a particular independent variable then that variable is included in that particular model. In this case, I have three different models. The first model only has numbers listed for Percent less than HS, so that is the only independent variable in the first model. The second model has numbers listed for Percent less than HS and Median Age, so both of these variables are included in the model. The third model includes all three variables in the model. Remember that in each case the dependent variable is the property crime rate. Within each cell with numbers listed there is a lot going on. We are primarily interested in the main number listed at the top. This number is the slope (or intercept in the case of the “Constant” row). The number in parenthesis is the standard error for each slope in the model. You could use this standard error and the slope estimate above it to calculate t-statistics and p-values exactly. However, the asterisks give you an easy visual short cut to determine the rough size of the p-value. These asterisks indicate if the p-value is below a certain level, as shown in the notes at the bottom. The cut-offs of 0.05, 0.01, and 0.001 used here are pretty standard for the discipline. So an asterisks generally means that the result is “statistically significant.” However, its important to keep in mind as noted above that these cut-offs are ultimately arbitrary and should never be confused with the substantive size of the effect itself. At the bottom, you typically get a number of summary measures of the model. The only two we care about are the number of observations and the \\(R^2\\) of the model. The advantage of organizing the table in this fashion is that we can easily see how the relationship between a given independent variable and the dependent variable changes as we add in other control variables by just looking at the numbers across a row. For example, we can see from Model 1 that the percent of the population with less than a high school diploma is initially pretty strongly positively related to violent crime rates. A one percentage point increase in this variable is associated with 78.83 more violent crimes per 100,000 population, on average. Controlling for the median age of the population in Model 2reduces this effect slightly but we still see a strong relationship. However, once we control for the poverty rate in a state the percent less than high school diploma effect completely vanishes (it becomes 0.05 which is effectively zero in this case). The poverty rate, on the other hand, has a big positive association with violent crime rates. What is going on here? It seems that the percent of the population with less than a high school diploma is only indirectly related to violent crime rates by its positive association with the poverty rate. But high poverty rates are much more directly responsible for high violent crime rates. In other words, a low high school completion rate predicts higher poverty rates and higher poverty rates predict more violent crimes, but a low high school completion rate does not directly predict more violent crimes. "],
["including-categorical-variables-as-predictors.html", "Including Categorical Variables as Predictors", " Including Categorical Variables as Predictors To this point, we only know how to include quantitative variables into OLS regression models. However, it turns out you can use a fairly easy trick to include categorical variables as independent variables in OLS regression models. By including categorical variables as independent variables, we expand considerably the range of things that we can do with OLS regression models. The most difficult part of this trick is correctly interpreting your results. Indicator variables As an example, I am going to look at the relationship between religious affiliation and sexual frequency. To keep our example simple I am going to dichotomize the religious affiliation variable, which means I am going to collapse it into two categories, rather than the six categories in the dataset. I will use a simply dichotomy of “Not Religious/Religious.” In R, I can create this variable like so: sex$norelig &lt;- sex$relig==&quot;None&quot; This is technically a boolean variable, which means it takes a TRUE or FALSE value. For our purposes, TRUE is a non-religious person. We already know how to look at the relationship between sexual frequency and this dichotomized religious affiliation variable. We can look at the mean differences in sexual frequency across our two categories: tapply(sex$sexf, sex$norelig, mean) ## FALSE TRUE ## 48.33671 59.84862 59.84862-48.33671 ## [1] 11.51191 The non-religious have sex 11.5 more times per year than the religious, on average. Hallelujah? We can represent this same mean difference in a regression model framework by using an indicator variable. An indicator variable is a variable that only takes a value of zero or one. It takes a value of one when the observation is in the indicated category and a zero otherwise. Mathematically, we would say: \\[nonrelig_i=\\begin{cases} 1 &amp; \\text{if non-religious}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] The indicated category is the category which gets a one on the indicator variable. In this case the indicated category is non-religious. The reference category is the category that gets no indicator variable. In this case, that is just the religious group. Later on, we will see that this can become slightly more complicated. You can think of the indicator variable as an on/off switch where 1 indicates that it is “on” (i.e. the observation belongs to the indicated category) and 0 indicates that it is “off” (i.e. the observation does not belong to the indicated category). What would happen if we put this indicator variable into a regression model predicting sexual frequency like so: \\[\\hat{frequency}_i=b_0+b_1(nonrelig_i)\\] How would we interpret the slope and intercept for such a model? Figure 51 shows a scatterplot of this relationship. Figure 51: A scatterplot of the religious indicator variable by sexual frequency. Points are jittered to avoid overplotting. The mean for each group is plotted in red. Notice that all of the points align vertically either at the 0 or 1 on the x-axis. This is because the indicator variable can only take these two values. I have jittered points slightly to avoid overplotting. I have also plotted the means of the two groups in red dots and the OLS regression line for the scatterplot in blue. It turns out, that in order to be the best-fitting line, this OLS regression line must connect the two dots that represent the mean of each group. What will the slope of this line be? If we go up “one unit” on the non-religious indicator variable we have gone from a religious person to a non-religious person and the change in predicted sexual frequency is equal to the mean difference of 11.5 between the groups. The intercept is given by the value at zero which is just given by the mean sexual frequency among the religious of 48.3. So, the OLS regression line should look like: \\[\\hat{frequency}_i=48.3+11.5(nonrelig_i)\\] I can calculate these same numbers in R with the lm command: coef(lm(sexf~norelig, data=sex)) ## (Intercept) noreligTRUE ## 48.33671 11.51191 The numbers are the same. More important than the numbers, however, is the interpretation of the numbers. The intercept is the mean of the dependent variable for the reference category. The slope is the mean difference between the reference category and the indicated category. In this case, I would say: Religious individuals have sex 48.3 times per year, on average. Non-religious individuals have sex 11.5 times more per year than non-religious individuals, on average. Note that I can derive the sexual frequency of the non-religious from these two numbers by taking the value for the non-religious and adding the mean difference to find out that non-religious individuals have sex 59.8 times per year, on average. Reversing the indicator variable What if I switched my indicator variable so that the religious were indicated and the non-religious were the reference category? \\[relig_i=\\begin{cases} 1 &amp; \\text{if religious}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Lets try it out in R and see (the != below is computer lingo for “not equal to”): sex$religious &lt;- sex$relig!=&quot;None&quot; coef(lm(sexf~religious, data=sex)) ## (Intercept) religiousTRUE ## 59.84862 -11.51191 Lets compare the two models: \\[\\hat{frequency}_i=48.3+11.5(nonrelig_i)\\] \\[\\hat{frequency}_i=59.8-11.5(relig_i)\\] Both models give me the exact same information, but from the perspective of a different reference group. The first model tells me the mean sexual frequency of the religious (48.3) and how much more sex the non-religious have on average (11.5). The second model tells me the mean sexual frequency of the non-religious (59.8) and how much less sex the religious have (-11.5). I can easily derive one model from the other, without actually having to calculate it in R. Therefore, which category you set as the reference category is really a matter of taste, rather than one of consequence. The results are the same either way. Categorical variables with more than two categories What if I have a categorical variable that has more than two categories? Lets expand the religious variable that I dichotomized back to its original scale. There are six different categories: Fundamentalist Protestant, Mainline Protestant, Catholic, Jewish, Other, and None: summary(sex$relig) ## Fund Protestant Mainline Protestant Catholic Jewish ## 556 529 507 39 ## Other None ## 145 327 Lets look at the mean sexual frequency for each of these groups. round(tapply(sex$sexf, sex$relig, mean),1) ## Fund Protestant Mainline Protestant Catholic Jewish ## 49.6 44.2 49.2 39.8 ## Other None ## 57.9 59.8 Figure 52 plots these means on a number line to get a visual display of the differences: Figure 52: The mean sexual frequency of each group arrayed on a vertical number line. The red lines indicate the distance between each group and the reference category of fundamentalist Protestant. Nones and others clearly have much higher mean sexual frequency than the remaining religious groups and Jews have much lower mean sexual frequency. The three Christian groups cluster in the middle, although mainline protestants have a lower mean sexual frequency than the other two. This plot also shows the mean differences between the groups, with fundamentalist Protestants set as the reference category. The vertical distances from the dotted red line (the mean of fundamentalist Protestants) give the mean differences between each religious group and fundamentalist Protestants. So we can see that “Nones” have sex 10.2 more times per year than fundamentalist Protestants, on average, and mainline Protestants have sex 5.4 fewer times per year, on average, than fundamentalist Protestants. We can use the same logic of indicator variables we developed above to represent the mean differences between groups observed here in a regression model framework. However, because we now have six categories, we will need five indicator variables. You always need one less indicator variable than the number of categories. The category which doesn’t get an indicator variable is your reference category. As per the graph above, I will make Fundamentalist Protestants my reference category. Therefore, I need one indicator variable for each of the other five categories: \\[main_i=\\begin{cases} 1 &amp; \\text{if main}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[catholic_i=\\begin{cases} 1 &amp; \\text{if catholic}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[jewish_i=\\begin{cases} 1 &amp; \\text{if jewish}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[other_i=\\begin{cases} 1 &amp; \\text{if other religion}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] \\[none_i=\\begin{cases} 1 &amp; \\text{if no religion}\\\\ 0 &amp; \\text{otherwise} \\end{cases}\\] Now lets put these variables into an OLS regression model: \\[\\hat{frequency}_i=b_0+b_1(main_i)+b_2(catholic_i)+b_3(jewish_i)+b_4(other_i)+b_5(none_i)\\] We can figure out how all this works by getting the predicted value for the member of a specific group. That respondent should get a 1 for the variable where they are a member and a zero on all other variables. For example, a fundamentalist protestant should get a zero on all of these variables: \\[\\hat{frequency}_i=b_0+b_1(0)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0\\] So, the intercept is the predicted value for fundamentalist Protestants. Similarly we could calculate the predicted value for mainline Protestants: \\[\\hat{frequency}_i=b_0+b_1(1)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0+b_1\\] The difference between the two is \\(b_1\\), so this “slope” gives the mean difference between mainline and fundamentalist Protestants. We could do the same thing for Catholics: \\[\\hat{frequency}_i=b_0+b_1(0)+b_2(1)+b_3(0)+b_4(0)+b_5(0)=b_0+b_2\\] The mean difference between Catholics and fundamentalist Protestants is given by \\(b_2\\). In general, each of the “slopes” is the mean difference between the indicated category and the reference category. In this case, the reference category is fundamentalist Protestants so each of the slopes gives the mean difference between that religious category and fundamentalist Protestant, just like the graph above. R is fairly intelligent about handling all of these indicator variables and you don’t actually have to create these five different variables. If you put a categorical variable into your regression formula, R will know to treat it as a set of indicator categories. The only catch is that R will already have a default category set as the reference. It just so happens that in our GSS data, fundamentalist Protestants are already set as the reference. So I can run this model by: model &lt;- lm(sexf~relig, data=sex) round(coef(model),2) ## (Intercept) religMainline Protestant religCatholic ## 49.60 -5.44 -0.36 ## religJewish religOther religNone ## -9.84 8.30 10.25 You can tell which category is the reference by which category is left out here. Note how the coefficients (given by the estimates column) match the mean differences I calculated above in the graph. We are simply reproducing these mean differences in a regression model framework. Categorical and quantitative variables combined in a single model If all we are doing is reproducing mean differences between categories, what good is this method? After all, we already know how to do that. The major advantage of putting these mean differences into a regression model framework is that we can control for other potentially confounding variables. These sexual frequency differences by religious affiliation are a prime example. Lets take a look at the age differences between religious affiliations: round(tapply(sex$age, sex$relig, mean),1) ## Fund Protestant Mainline Protestant Catholic Jewish ## 45.4 47.3 44.9 53.7 ## Other None ## 38.6 39.5 Notice how closely these age differences mirror the differences in sexual frequency. Others and nones are the youngest, while Jews are the oldest. Among Christians, mainline Protestants are older than fundamentalist Protestants and Catholics. We also know from prior work that age has a negative effect on sexual frequency. This should make us suspicious that some (or all) of the observed differences in sexual frequency between religious groups simply reflect age differences between those groups. We can easily address this issue by simply including age as a control variable in our model: model &lt;- lm(sexf~relig+age, data=sex) round(coef(model),2) ## (Intercept) religMainline Protestant religCatholic ## 107.90 -3.04 -0.96 ## religJewish religOther religNone ## 0.88 -0.43 2.69 ## age ## -1.28 We now interpret those slopes as the mean difference in sexual frequency between fundamentalist Protestants and the indicated category, among individuals of the same age. So for example, we would interpret the 2.69 on “None” as: The model predicts that, among individuals of the same age, those with no religious preference have sex 2.69 more times per year than fundamentalist protestants, on average. We would also interpret the age effect controlling for religious affiliation like so: The model predicts, that holding religious affiliation constant, a one year increase in age is associated with 1.28 fewer instances of sex per year, on average. Table 14 below helps to highlight the change in the effects once age is controlled. Table 14: OLS regression models predicting sexual frequency Model 1 Model 2 Intercept 49.60*** 107.90*** (2.26) (3.68) Mainline Protestant -5.44 -3.04 (3.24) (2.99) Catholic -0.36 -0.96 (3.28) (3.02) Jewish -9.84 0.88 (8.84) (8.17) Other 8.30 -0.43 (4.98) (4.61) None 10.25** 2.69 (3.72) (3.45) Age -1.28*** (0.07) R-squared 0.01 0.16 N 2103 2103 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05. Standard errors in parenthesis. Reference category is fundamentalist Protestant All of the coefficients (except for Catholic, which was tiny anyway) have declined substantially in size. The mean differences from fundamentalist Protestants for both Jews and other religions have basically disappeared and the “None” effect has been severely reduced. In other words, almost all or all of the observed differences in sexual frequency by religious affiliation were indirectly a product of underlying age differences between religious affiliations. If you were just about ready to convert to a different religion to get laid more often, you may want to hold off for the moment. Try it out In the interactive R session below, lets try creating a full linear model. We are going to predict hourly wages by the following independent variables: Age, re-centered on 40 Number of children, not re-centered Gender Highest educational degree Keep the following in mind: You will need to fill in the formula part of the lm command below. Remember that your dependent variable (in this case wages) goes on the left side of the ~. Use the I(x-number) syntax as shown above to re-center age in the model formula. How would you interpret all of the slopes and intercepts in the model output? Are you surprised by any of the results. "],
["interaction-terms.html", "Interaction Terms", " Interaction Terms By definition, a linear model is an additive model. As you increase or decrease the value of one independent variable you increase or decrease the predicted value of the dependent variable by a set amount, regardless of the other values of the independent variable. This is an assumption built into the linear model by its additive form, and it may misrepresent some relationships where independent variables interact with one another to produce more complicated effects. In particular, in this section, we want to know whether the effect (i.e. the slope) of one independent variable varies by the value of another independent variable. The nature of additive models As an example for this section, I am going to look at the relationship between movie genre, runtime, and tomato meter ratings. To simplify things, I am going to only look at these relationships for two genres: action and comedy. I can limit my movies dataset to these two genres with the following command: movies.short &lt;- subset(movies, Genre==&quot;Comedy&quot; | movies$Genre==&quot;Action&quot;) Now lets look at a simple model where genre and runtime both predict Tomato Meter ratings. round(coef(lm(TomatoMeter~Genre+Runtime, data=movies.short)),2) ## (Intercept) GenreComedy Runtime ## -1.75 4.43 0.41 Genre is a categorical variable and action movies are set as the reference category. In equation form, the model looks like: \\[\\hat{meter}_i=-1.75+4.43(comedy_i)+0.31(runtime_i)\\] I can interpret my slopes as follows: The model predicts that when comparing movies of the same runtime, comedies have Tomato Meter ratings 4.43 percentage points higher than action movies, on average. The model predicts that, holding constant movie genre, a one minute increase in movie runtime is associated with a 0.31 percentage point increase in the Tomato Meter rating, on average. This is an additive model. If we move from an action movie to a comedy of the same runtime, our predicted Tomato Meter rating goes up by 4.43, regardless of the actual value of runtime. If we increase movie runtime by one minute while keeping genre the same, our predicted Tomato Meter rating goes up by 0.41, regardless of whether that genre is action or comedy. It may help to graphically visualize the nature of this additive relationship. We can do this by plotting lines showing the relationship between runtime and Tomato Meter ratings separately for our two different genres of action and comedy. The line for action movies is given by: \\[\\hat{meter}_i=-1.75+4.43(0)+0.41(runtime_i)=-1.75+0.41(runtime_i)\\] The line for comedy movies is given by: \\[\\hat{meter}_i=-1.75+4.43(1)+0.41(runtime_i)=2.68+0.41(runtime_i)\\] Each line has an intercept and a slope. Notice that the intercepts are different but the slopes are the same. That means we have two parallel lines at different levels. Figure 53 overlays these two parallel lines on top of a scatterplot of movie runtime by tomato meter for these two genres. Figure 53: Predicted Tomato Meter by runtime for two genres based on an additive OLS regression model. The lines must be parallel. The parallel lines are an assumption of the OLS regression model structure we have used. There are two consequences of this assumption. First, At every single level of runtime, the predicted Tomato Meter difference between comedy and action movies is exactly 4.62. This can be seen on the graph by the consistent gap between the blue and red line. Second, the effect of runtime on the Tomato Meter rating is assumed to be the same for action and comedy movies. This can be seen on the graph by the fact that both lines have the exact same slope. Although these may seem like two different issues, they are really the same issue from different perspectives. If we were to allow the slopes of the blue and red line to be different, then the gap between them would not be static. The questions is how can we allow the slopes of the two lines to be different. This is where the concept of the interaction term comes in. The interaction term An interaction term is a variable that is constructed from two other variables by multiplying those two variables together. In our case, we can easily construct an interaction term as follows: movies.short$comedy &lt;- movies.short$Genre==&quot;Comedy&quot; movies.short$interaction &lt;- movies.short$Runtime*movies.short$comedy In this case, I had to create a real indicator variable for comedy before I could multiply them, but then I just multiply this indicator variable by movie runtime. Now lets add this interaction term to the model: model &lt;- lm(TomatoMeter~Runtime+comedy+interaction, data=movies.short) round(coef(model), 2) ## (Intercept) Runtime comedyTRUE interaction ## -14.45 0.52 24.36 -0.19 We now have an additional “slope” for the interaction term. Lets write this model out in equation form to try to figure out what is going on here. \\[\\hat{meter}_i=-14.45+24.36(comedy_i)+0.52(runtime_i)-0.19(runtime_i*comedy_i)\\] Remember that the interaction term is just a literal multiplication of the two other variables. To figure out how this all works, lets once again separate this into two lines predicting Tomato Meter by runtime, for comedies and action movies separately. For action movies, the equation is: \\[\\hat{meter}_i=-14.45+24.36(0)+0.52(runtime_i)-0.19(runtime_i*0)=-14.45+0.52(runtime_i)\\] For comedy movies, the equation is: \\[\\hat{meter}_i=-14.45+24.36(1)+0.52(runtime_i)-0.19(runtime_i*1)=(-14.45+24.36)+(0.52-0.19)(runtime_i)=9.91+0.33(run_i)\\] We now have two lines with different intercept and different slopes. The interaction term has allowed the effect of runtime on the Tomato Meter to vary by type of genre. In this case, the interaction term tells us how much smaller the slope is for comedy movies than for action movies. We can also just plot the lines to see how it looks, as I have done in Figure 54. Figure 54: An interaction term allows for non-parallel lines, and thus different effects of runtime on tomato meter ratings by genre The pattern here is fairly clear. Short comedies get better ratings than short action movies, while long comedies get worse ratings than long action movies. Put another way, comedies get less “return” in terms of their ratings when increasing their length than do action movies. This can be seen by the much steeper slope for action movies. Interpreting interaction terms Interpreting interaction terms can be tricky, because the inclusion of an interaction term also changes the meaning of other slopes in the model. The slopes for the two variables that make up the interaction term are called the main effects. In our example, those two variables are runtime and the comedy indicator variable and the main effects of these variables are 0.52 and 24.36, respectively. The most important rule to remember is that when an interaction term is in a model, the main effects are only the expected effects when the other variable involved in the interaction is zero. This is because the interaction implies that the effects of the two variables are not constant but rather change depending on the value of the other variable in the interaction term. Therefore, we can only interpret effects at a particular value of the other variable. So I would interpret these main effects as follows: The model predicts that among action movies, a one minute increase in movie runtime is associated with a 0.52 point increase in the Tomato Meter rating, on average. The model predicts that among movies with zero minutes of runtime (outside the scope of data of course), comedies are predicted to have Tomato Meter ratings 24.36 points higher than action movies, on average. Notice that I did not have to say I was controlling for the other variable. I am doing more than controlling when I include an interaction term. I am conditioning the effect of one variable on the value of another. That is why I instead use the phrase “among observations that are zero on the other variable.” Note that you could also include other non-interacted variables in this model as well, like maturity rating, in which case you would also need to indicate that you controlled for those variables. Interpreting interaction terms themselves can also be tricky because they are the difference in the effect of on variable depending on the value of another. One approach is to interpret this difference in effect directly. In this case, we would say: The model predicts that the predicted increase in Tomato Meter ratings for a one minute increase in movie runtime is 0.19 points smaller for comedy movies than for action movies, on average. You have to be careful with this type of interpretation. In this case, both slopes were still positive so I can talk about how the effect was smaller. However, in some cases, the slopes may end up in different directions entirely which would require a somewhat different interpretation. Another approach is to actually calculate the slope for the indicated category (comedies) and interpret it directly: The model predicts that among comedy movies, a one minute increase in movie runtime is associated with a 0.33 increase in the Tomato Meter rating, on average (which is lower than for action movies). In short, you have to be careful and thoughtful when thinking about how to interpret interaction terms. Interaction terms in R In the example above, I created the interaction term manually, but I didn’t actually need to do this. R has a shortcut method for calculating interaction terms: model &lt;- lm(TomatoMeter~Runtime*Genre, data=movies.short) round(coef(model),2) ## (Intercept) Runtime GenreComedy Runtime:GenreComedy ## -14.45 0.52 24.36 -0.19 The results are exactly the same as before. To include an interaction term between two variables I just have to connect them with a * rather than a + in the lm formula. By default, R will include each variable separately as well as their interaction. Try it out In the interactive R session below, lets try a linear model that predicts hourly wages by number of children, gender, and the interaction between the two variables. You will need to fill in the formula part of the lm command below. Remember that your dependent variable (in this case wages) goes on the left side of the ~. Use the I(x-number) syntax as shown above to re-center age in the model formula. What does the interaction term tell you about different wage returns to children for men and women? Interaction terms with multiple categories In the above example, I only compared comedy and action movies in order to keep the comparison simple, but it is possible to run the same analysis on the full movie dataset to see how runtime varies across all genres. model &lt;- lm(TomatoMeter~Runtime*Genre, data=movies) round(coef(model),2) ## (Intercept) Runtime ## -14.45 0.52 ## GenreAnimation GenreComedy ## 11.96 24.36 ## GenreDrama GenreFamily ## 59.50 -29.06 ## GenreHorror GenreMusical/Music ## 2.51 21.07 ## GenreMystery GenreRomance ## 2.30 66.59 ## GenreSciFi/Fantasy GenreThriller ## 5.92 12.26 ## Runtime:GenreAnimation Runtime:GenreComedy ## 0.14 -0.19 ## Runtime:GenreDrama Runtime:GenreFamily ## -0.39 0.32 ## Runtime:GenreHorror Runtime:GenreMusical/Music ## -0.03 -0.12 ## Runtime:GenreMystery Runtime:GenreRomance ## 0.06 -0.52 ## Runtime:GenreSciFi/Fantasy Runtime:GenreThriller ## -0.03 -0.04 Thats a lot of numbers! There is a slope for each genre except action (10 in all) and an interaction between runtime and each genre except action (another 10 in all). What we are estimating here are 11 different lines (on for each genre) for the relationship between runtime and Tomato Meter rating. Because action movies are the reference, the main effect of runtime is the slope for action movies (0.52). The interaction terms show us how much larger or smaller the effect of runtime is for each given genre. So the effect is 0.39 smaller for dramas for a total effect of 0.13 (0.52-0.39). It is 0.32 larger for family movies for a total effect of 0.84 (0.52+0.32), and so forth. Similarly, the intercept is the intercept only for action movies. To get the intercept for other genres, we take the intercept value itself and add the main effect of genre. So for dramas the intercept is -14.45+59.5=45.05 and for family movies it is -14.45-29.06=-43.51. If we put all these slopes and intercepts together, we will get 11 lines as shown in Figure 55. Figure 55: Interaction terms allow each genre to get a different return from increasing runtime There is a lot going on here, but we can detect some interesting patterns. Almost all of the lines are positive indicating that longer movies tend to generally get better ratings. This is not true of Romances however, where there is a slight negative relationship between movie runtime and Tomato Meter ratings. Dramas also have a fairly flat slope and a high intercept, so they tend to outperform most other short movies but don’t fare as well compared to other genres when they are longer. The steepest slope is for family movies, which apparently are horrible when short (think “Beethoven 6: Beethoven saves Christmas, again” or something), but do much better when longer. Interaction terms with two categorical variables The examples so far have involved interacting a quantitative variable with a categorical variable which gives you a different line for each category of your categorical variable. However, we can also create an interaction term between two categorical variables. As an example, lets look at differences in wages in the earnings dataset by race and education. To simplify things, I am going to dichotimize race into white/non-white and education into less than Bachelor’s degree/Bachelor’s degree or more, as follows: earnings$nwhite &lt;- earnings$race!=&quot;White&quot; earnings$college &lt;- as.numeric(earnings$educ)&gt;3 Lets look at mean wages across these combination of categories: tapply(earnings$wages, earnings[,c(&quot;nwhite&quot;,&quot;college&quot;)], mean) ## college ## nwhite FALSE TRUE ## FALSE 19.99246 33.91417 ## TRUE 16.74181 31.82208 White college graduates make $33.91 per hour, on average, while non-white college graduates make $31.82 per hour, on average. Whites without a college degree make $19.99, on average, while non-whites without a college degree make $16.74, on average. If we put this in a table, I can show that there are four different ways to make comparisons between these numbers. Table 15: Mean wages in dollars per hour by race and education No degree Bachelor’s degree Difference White 19.99 33.91 13.92 Non-white 16.74 31.82 15.08 Difference -3.25 -2.09 1.16 If we look at the two differences along the far-right column, we are seeing the “returns” in terms of wages for a college degree separately for whites and non-whites. The return for whites is $13.92 per hour and the return for non-whites is higher at $15.08. If we look at the differences along the bottom row, we are seeing the racial inequality in wages separately for those with no degree and those with a college degree. Among those with no college degree, non-whites make $3.25 less per hour than whites. Among those with a college degree, non-whites make $2.09 less per hour than whites. The racial gap in wages gets smaller among those who have completed a college degree. Now lets look at the difference in the differences. For the racial gap in wages this is given by -3.25-(-2.09)=1.16. For the returns to a college degree this is given by 15.08-13.92=1.16. The difference in the differences is the same! This is because we are looking at the same relationship in two different ways. If non-whites get a better return to college than whites, then the racial gap in wages must get smaller among the college-educated. Similarly, if the racial gap in wages gets smaller at the college level, it tells us that non-whites must get a better return on their college education. This 1.16 number is basically an interaction term. We can interpret the number as the difference in returns to wages from a college degree between whites and non-whites. Alternatively, we can interpret the number as the difference in the racial wage gap between those with no degree and those with a college degree. Either way, we have the same information, with the same finding: greater educational attainment reduces racial inequality because minorities get a greater return on their college degrees. Lets try modeling this relationship with an OLS regression model. First lets try a model without interaction terms: model &lt;- lm(wages~nwhite+college, data=earnings) coef(model) ## (Intercept) nwhiteTRUE collegeTRUE ## 19.850655 -2.866436 14.263698 Lets put this into an equation framework: \\[\\hat{wages}_i=19.85-2.87(nwhite_i)+14.26(college_i)\\] We can use this equation to fill in the predicted valued of the same table we calculated by hand above: Table 16: Predicted wages in dollars per hour by race and education from an additive model No degree Bachelor’s degree Difference White 19.85 19.85+14.26=34.11 14.26 Non-white 19.85-2.87=16.98 19.85-2.87+14.26=31.24 14.26 Difference -2.87 -2.87 0 The predicted values do not match the exact values above. More importantly, if you look at the differences, you can see that the returns to education are assumed to be identical for whites and non-whites ($14.26) and the racial gap is assumed to be the same for those with no degree and those with a college degree (-$2.87). This is the limitation of the additive model. We assume that the effects of race and college completion are not affected by each other. If we want to determine whether returns to college are different by race, we need to model the interaction term, as follows: model &lt;- lm(wages~nwhite*college, data=earnings) coef(model) ## (Intercept) nwhiteTRUE collegeTRUE ## 19.992457 -3.250648 13.921709 ## nwhiteTRUE:collegeTRUE ## 1.158565 In equation form: \\[\\hat{income}_i=19.99-3.25(nwhite_i)+13.92(college_i)+1.16(nwhite_i*college_i)\\] Lets use this model to get predicted values in our table: Table 17: Predicted wages in dollars per hour by race and education from an interactive model No degree Bachelor’s degree Difference White 19.99 19.99+13.92=33.91 13.92 Non-white 19.99-3.25=16.74 19.99-3.25+13.92+1.16=31.82 15.08 Difference -3.25 -2.09 1.16 Our model now fits the data exactly and the differences are allowed to vary by the other category, so that we can see the differences in returns to college by race and the differences in the racial gap by education level. The interaction term itself of 1.16 is the same to what we calculated by hand. If we were to interpret the intercept and slopes from the model above, we would say: Whites with no college degree had mean wages of $19.99 per hour. Among those with no college degree, non-whites earn $3.25 less per hour than whites, on average. Among whites, those with a college degree have wages $13.92 per hour higher on average than those without a college degree. The returns to wages from a college degree are $1.16 larger for non-whites than they are for whites, on average. "],
["model-complications.html", "Model Complications", " Model Complications In this chapter, we will expand our understanding of the linear model to address many issues that the practical researcher must face. We begin with a review and reformulation of the linear model. We then move on to discuss how to address violations of assumptions such as non-linearity and heteroskedasticity (yes, this is a real word), sample design and weighting, missing values, multicollinearity, and model selection. By the end of this chapter, you will be well-supplied with the tools for conducting a real-world analysis using the linear model framework. Slides for this module can be found here. "],
["the-linear-model-revisited.html", "The Linear Model, Revisited", " The Linear Model, Revisited Reformulating the linear model Up until now, we have used the following equation to describe the linear model mathematically: \\[\\hat{y}_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] In this formulation, \\(\\hat{y}_i\\) is the predicted value of the dependent variable for the \\(i\\)th observation and \\(x_{i1}\\), \\(x_{i2}\\). through \\(x_{ip}\\) are the values of \\(p\\) independent variables that predict \\(\\hat{y}_i\\) by a linear function. \\(\\beta_0\\) is the y-intercept which is the predicted value of dependent variable when all the independent variables are zero. \\(\\beta_1\\) through \\(\\beta_p\\) are the slopes giving the predicted change in the dependent variable for a one unit increase in a given independent variable holding all of the other variables constant. This formulation is useful, but we can now expand and re-formulate it in a way that will help us understand some of the more advanced topics we will discuss in this and later chapters. The formula is above is only the “structural” part of the full linear model. The full linear model is given by: \\[y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}+\\epsilon_i\\] I have changed two things in this new formula. On the left-hand side, we have the actual value of the dependent variable for the \\(i\\)th observation. In order to make things balance on the right-hand side of the equation, I have added \\(\\epsilon_i\\) which is simply the residual or error term for the \\(i\\)th observation. We now have a full model that predicts the actual values of \\(y\\) from the actual values of \\(x\\). If we compare this to the first formula, it should become clear that every term except the residual term can be substituted for by \\(\\hat{y}_i\\). So, we can restate our linear model using two separate formulas as follows: \\[\\hat{y}_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] \\[y_i=\\hat{y}_i+\\epsilon_i\\] By dividing up the formula into two separate components, we can begin to think of our linear model as containing a structural component and a random (or stochastic) component. The structural component is the linear equation predicting \\(\\hat{y}_i\\). This is the formal relationship we are trying to fit between the dependent and independent variables. The stochastic component on the other hand is given by the \\(\\epsilon_i\\) term in the second formula. In order to get back to the actual values of the dependent variable, we have to add in the residual component that is not accounted for by the linear model. From this perspective, we can rethink our entire linear model as a partition of the total variation in the dependent variable into the structural component that can be accounted for by our linear model and the residual component that is unaccounted for by the model. This is exactly as we envisioned things when we learned to calculate \\(R^2\\) in previous chapters. Marginal effects The marginal effect of \\(x\\) on \\(y\\) gives the expected change in \\(y\\) for a one unit increase in \\(x\\) at a given value of \\(x\\). If that sounds familiar, its because this is very similar to the interpretation we give of the slope in a linear model. In a basic linear model with no interaction terms, the marginal effect of a given independent variable is in fact given by the slope. The difference in interpretation is the little part about “at a given value of x.” In a basic linear model this addendum is irrelevant because the expected increase in \\(y\\) for a one unit increase in \\(x\\) is the same regardless of the current value of \\(x\\) – thats what it means for an effect to be linear. However, when we start delving into more complex model structures, such as interaction terms and some of the models we will discuss in this module, things get more complicated. The marginal effect can help us make sense of complicated models. The marginal effect is calculated using calculus. I won’t delve too deeply into the details here, but the marginal effect is equal to the partial derivative of y with respect to x. This partial derivative gives us what is called the tangent line of a curve at any point of \\(x\\) which measures the instantenous rate of change in some mathematical function. Lets take the case of a simple linear model with two predictor variables: \\[y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\epsilon_i\\] We can calculate the marginal effect of \\(x_1\\) and \\(x_2\\) by taking their partial derivatives. If you don’t know how to do that, its fine. But if you do know a little calculus, this is a simple calculation: \\[\\frac{\\partial y}{\\partial x_1}=\\beta_1\\] \\[\\frac{\\partial y}{\\partial x_2}=\\beta_2\\] As stated above, the marginal effects are just given by the slope for each variable, respectively. Marginal effects really become useful when we have more complex models. Lets now try a model similar to the one above but with an interaction term: \\[y_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\beta_3x_{i1}x_{i2}+\\epsilon_i\\] The partial derivatives of this model turn out to be: \\[\\frac{\\partial y}{\\partial x_1}=\\beta_1+\\beta_3x_{i2}\\] \\[\\frac{\\partial y}{\\partial x_2}=\\beta_2+\\beta_3x_{i1}\\] The marginal effect of each independent variable on \\(y\\) now depends on the value of the other independent variable. Thats how interaction terms work, but now we can see that effect much more clearly in a mathematical sense. The slope of each independent variable is itself determined by a linear function rather than being a single constant value. Marginal effect interaction example Lets try this with a concrete example. For this case, I will look at the effect on wages of the interaction between number of children in the household and gender: model &lt;- lm(wages~nchild*gender, data=earnings) round(coef(model), 3) ## (Intercept) nchild genderFemale nchild:genderFemale ## 24.720 1.779 -2.839 -1.335 So, my full model is given by: \\[\\hat{\\text{wages}}_i=24.720+1.779(\\text{nchildren}_i)-2.839(\\text{female}_i)-1.335(\\text{nchildren}_i)(\\text{female}_i)\\] Following the formulas above, the marginal effect of number of children is given by: \\[1.779-1.335(\\text{female}_i)\\] Note that this only takes two values because female is an indicator variable. When we have a man, the \\(\\text{female}\\) variable will be zero and therefore the effect of an additional child will be 1.779. When we have a woman, \\(\\text{female}\\) variable will be one, and the effect of an additional child will be \\(1.779-1.335=0.444\\). The marginal effect for the gender gap will be given by: \\[-2.839-1.335(\\text{nchildren}_i)\\] So the gender gap starts at 2.839 with no children in the household and grows by 1.335 for each additional child. We can also plot up a marginal effects graph that shows us how the gender gap changes by number of children in the household. nchild &lt;- 0:6 gender_gap &lt;- -2.839-1.335*nchild df &lt;- data.frame(nchild, gender_gap) ggplot(df, aes(x=nchild, y=gender_gap))+ geom_line(linetype=2, color=&quot;grey&quot;)+ geom_point()+ labs(x=&quot;number of children in household&quot;, y=&quot;predicted gender gap in hourly wages&quot;)+ scale_y_continuous(labels = scales::dollar)+ theme_bw() Figure 56: Marginal effect of number of children on the gender gap in wages Linear model assumptions Two important assumptions underlie the linear model. The first assumption is the assumption of linearity. In short, we assume that the the relationship between the dependent and independent variables is best described by a linear function. In prior chapters, we have seen examples where this assumption of linearity was clearly violated. When we choose the wrong model for the given relationship, we have made a specification error. Fitting a linear model to a non-linear relationship is one of the most common forms of specification error. However, this assumption is not as deadly as it sounds. Provided that we correctly diagnose the problem, there are a variety of tools that we can use to fit a non-linear relationship within the linear model framework. We will cover this techniques in the next section. The second assumption of linear models is that the residual or error terms \\(\\epsilon_i\\) are independent of one another and identically distributed. This is often called the i.i.d. assumption, for short. This assumption is a little more subtle. In order to understand, we need to return to this equation from earlier: \\[y_i=\\hat{y}_i+\\epsilon_i\\] One way to think about what is going on here is that in order to get an actual value of \\(y_i\\) you feed in all of the independent variables into your linear model equation to get \\(\\hat{y}_i\\) and then you reach into some distribution of numbers (or a bag of numbers if you need something more concrete to visualize) to pull out a random value of \\(\\epsilon_i\\) which you add to the end of your predicted value to get the actual value. When we think about the equation this way, we are thinking about it as a data generating process in which we get \\(y_i\\) values from completing the equation on the right. The i.i.d. assumption comes into play when we reach into that distribution (or bag) to draw out our random values. Independence assumes that what we drew previously won’t affect what we draw in the future. The most common violation of this assumption is in time series data in which peaks and valleys in the time series tend to come clustered in time, so that when you have a high (or low) error in one year, you are likely to have a similar error in the next year. Identically distributed means that you are drawn from the same distribution (or bag) each time you make a draw. One of the most common violation of this assumption is when error terms tend to get larger in absolute size as the predicted value grows in size. In later sections of this chapter, we will cover the i.i.d. assumption in more detail including the consequences of violating it, diagnostics for detecting it, and some corrections for when it does occur. Estimating a linear model Up until now, we have not discussed how R actually calculates all of the slopes and intercept for a linear model with multiple independent variables. We only know the equations for a linear model with one independent variable. Even though we don’t know the math yet behind how linear model parameters are estimated, we do know the rationale for why they are selected. We choose the parameters that minimized the sum of squared residuals given by: \\[\\sum_{i=1}^n (y_i-\\hat{y}_i)^2\\] In this section, we will learn the formal math that underlies how this estimation occurs, but first a warning: there is a lot of math ahead. In normal everyday practice, you don’t have to do any of this math, because R will do it for you. However, it is useful to know how linear model estimating works “under the hood” and it will help with some of the techniques we will learn later in the book. Matrix algebra crash course In order to learn how to estimate linear model parameters, we will need to learn a little bit of matrix algebra. In matrix algebra we can collect numbers into vectors which are single dimension arrays of numbers and matrices which are two-dimensional arrays of numbers. We can use matrix algebra to represent our linear regression model equation using one-dimensional vectors and two-dimensional matrices. We can imagine \\(y\\) below as a vector of dimension 3x1 and \\(\\mathbf{X}\\) as a matrix of dimension 3x3. \\[ \\mathbf{y}=\\begin{pmatrix} 4\\\\ 5\\\\ 3\\\\ \\end{pmatrix} \\mathbf{X}= \\begin{pmatrix} 1 &amp; 7 &amp; 4\\\\ 1 &amp; 3 &amp; 2\\\\ 1 &amp; 1 &amp; 6\\\\ \\end{pmatrix} \\] We can multiply vectors and matrices together by taking each element in the row of a matrix by the corresponding element in the vector and summing them up: \\[\\begin{pmatrix} 1 &amp; 7 &amp; 4\\\\ 1 &amp; 3 &amp; 2\\\\ 1 &amp; 1 &amp; 6\\\\ \\end{pmatrix} \\begin{pmatrix} 4\\\\ 5\\\\ 3\\\\ \\end{pmatrix}= \\begin{pmatrix} 1*4+7*5+3*4\\\\ 1*4+3*5+2*3\\\\ 1*4+1*5+6*3\\\\ \\end{pmatrix}= \\begin{pmatrix} 51\\\\ 25\\\\ 27\\\\ \\end{pmatrix}\\] You can also transpose a vector or matrix by flipping its rows and columns. My transposed version of \\(\\mathbf{X}\\) is \\(\\mathbf{X}&#39;\\) which is: \\[\\mathbf{X}= \\begin{pmatrix} 1 &amp; 1 &amp; 1\\\\ 7 &amp; 3 &amp; 1\\\\ 4 &amp; 2 &amp; 6\\\\ \\end{pmatrix}\\] You can also multiple matrices by each other using the same pattern as for multiplying vectors and matrices but now you start a new column each time you move down a row of the first matrix. So to “square” my matrix: \\[ \\begin{eqnarray*} \\mathbf{X}&#39;\\mathbf{X}&amp;=&amp; \\begin{pmatrix} 1 &amp; 1 &amp; 1\\\\ 7 &amp; 3 &amp; 1\\\\ 4 &amp; 2 &amp; 6\\\\ \\end{pmatrix} \\begin{pmatrix} 1 &amp; 7 &amp; 4\\\\ 1 &amp; 3 &amp; 2\\\\ 1 &amp; 1 &amp; 6\\\\ \\end{pmatrix}&amp;\\\\ &amp; =&amp; \\begin{pmatrix} 1*1+1*1+1*1 &amp; 1*7+1*3+1*1 &amp; 1*4+1*2+1*6\\\\ 7*1+3*1+1*1 &amp; 7*7+3*3+1*1 &amp; 7*4+3*2+1*6\\\\ 4*1+2*1+6*1 &amp; 4*7+2*3+6*1 &amp; 4*4+2*2+6*6\\\\ \\end{pmatrix}\\\\ &amp; =&amp; \\begin{pmatrix} 3 &amp; 11 &amp; 12\\\\ 11 &amp; 59 &amp; 40\\\\ 12 &amp; 40 &amp; 56\\\\ \\end{pmatrix} \\end{eqnarray*} \\] R can help us with these calculations. the t command will transpose a matrix or vector and the %*% operator will to matrix algebra multiplication. y &lt;- c(4,5,3) X &lt;- rbind(c(1,7,4),c(1,3,2),c(1,1,6)) X ## [,1] [,2] [,3] ## [1,] 1 7 4 ## [2,] 1 3 2 ## [3,] 1 1 6 t(X) ## [,1] [,2] [,3] ## [1,] 1 1 1 ## [2,] 7 3 1 ## [3,] 4 2 6 X%*%y ## [,1] ## [1,] 51 ## [2,] 25 ## [3,] 27 t(X)%*%X ## [,1] [,2] [,3] ## [1,] 3 11 12 ## [2,] 11 59 40 ## [3,] 12 40 56 #a shortcut for squaring a matrix crossprod(X) ## [,1] [,2] [,3] ## [1,] 3 11 12 ## [2,] 11 59 40 ## [3,] 12 40 56 We can also calculate the inverse of a matrix. The inverse of a matrix (\\(\\mathbf{X}^{-1}\\)) is the matrix that when multiplied by the original matrix produces the identity matrix which is just a matrix of ones along the diagonal cells and zeroes elsewhere. Anything multiplied by the identity matrix is just itself, so the identity matrix is like 1 at the matrix algebra level. Calculating an inverse is a difficult calculation that I won’t go through here, but R can do it for us easily with the solve command: inverse.X &lt;- solve(X) inverse.X ## [,1] [,2] [,3] ## [1,] -0.8 1.9 -0.1 ## [2,] 0.2 -0.1 -0.1 ## [3,] 0.1 -0.3 0.2 round(inverse.X %*% X,0) ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 Linear model in matrix form We now have enough matrix algebra under our belt that we can re-specify the linear model equation in matrix algebra format: \\[\\mathbf{y}=\\mathbf{X\\beta+\\epsilon}\\] \\(\\begin{gather*} \\mathbf{y}=\\begin{pmatrix} y_{1}\\\\ y_{2}\\\\ \\vdots\\\\ y_{n}\\\\ \\end{pmatrix} \\end{gather*}\\), \\(\\begin{gather*} \\mathbf{X}= \\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; \\ldots &amp; x_{1p}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; \\ldots &amp; x_{2p}\\\\ \\vdots &amp; \\vdots &amp; \\ldots &amp; \\vdots\\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; \\ldots &amp; x_{np}\\\\ \\end{pmatrix} \\end{gather*}\\), \\(\\begin{gather*} \\mathbf{\\epsilon}=\\begin{pmatrix} \\epsilon_{1}\\\\ \\epsilon_{2}\\\\ \\vdots\\\\ \\epsilon_{n}\\\\ \\end{pmatrix} \\end{gather*}\\), \\(\\begin{gather*} \\mathbf{\\beta}=\\begin{pmatrix} \\beta_{1}\\\\ \\beta_{2}\\\\ \\vdots\\\\ \\beta_{p}\\\\ \\end{pmatrix} \\end{gather*}\\) Where: \\(\\mathbf{y}\\) is a vector of known values of the independent variable of length \\(n\\). \\(\\mathbf{X}\\) is a matrix of known values of the independent variables of dimensions \\(n\\) by \\(p+1\\). This matrix is sometimes referred to as the design matrix. \\(\\mathbf{\\beta}\\) is a vector of to-be-estimated values of intercepts and slopes of length \\(p+1\\). \\(\\mathbf{\\epsilon}\\) is a vector of residuals of length \\(n\\) that will be equal to \\(\\mathbf{y-X\\beta}\\). Note that we only know \\(\\mathbf{y}\\) and \\(\\mathbf{X}\\). We need to estimate a \\(\\mathbf{\\beta}\\) vector from these known quantities. Once we know the \\(\\mathbf{\\beta}\\) vector, we can calculate \\(\\mathbf{\\epsilon}\\) by just taking \\(\\mathbf{y-X\\beta}\\). We want to estimate \\(\\mathbf{\\beta}\\) in order to minimize the sum of squared residuals. We can represent this sum of squared residuals in matrix algebra format as a function of \\(\\mathbf{\\beta}\\): \\[\\begin{eqnarray*} SSR(\\beta)&amp;=&amp;(\\mathbf{y}-\\mathbf{X\\beta})&#39;(\\mathbf{y}-\\mathbf{X\\beta})\\\\ &amp;=&amp;\\mathbf{y}&#39;\\mathbf{y}-2\\mathbf{y}&#39;\\mathbf{X\\beta}+\\mathbf{\\beta}&#39;\\mathbf{X&#39;X\\beta} \\end{eqnarray*}\\] If you remember the old FOIL technique from high school algebra (first, outside, inside, last) that is exactly what we are doing here, in matrix algebra form. We now have a function that defines our sum of squared residuals. We want to choose the values of \\(\\mathbf{\\beta}\\) that minimize the value of this function. In order to do that, I need to introduce a teensy bit of calculus. To find the minimum (or maximum) value of a function, you calculate the derivative of the function with respect to the variable you care about and then solve for zero. Technically, you also need to calculate second derivatives in order to determine if its a minimum or maximum, but since this function has no maximum value, we know that the result has to be a minimum. I don’t expect you to learn calculus for this course, so I will just mathemagically tell you that the derivative of \\(SSR(\\beta)\\) with respect to \\(\\mathbf{\\beta}\\) is given by: \\[-2\\mathbf{X&#39;y}+2\\mathbf{X&#39;X\\beta}\\] We can now set this to zero and solve for \\(\\mathbf{\\beta}\\): \\[\\begin{eqnarray*} 0&amp;=&amp;-2\\mathbf{X&#39;y}+2\\mathbf{X&#39;X\\beta}\\\\ -2\\mathbf{X&#39;X\\beta}&amp;=&amp;-2\\mathbf{X&#39;y}\\\\ (\\mathbf{X&#39;X})^{-1}\\mathbf{X&#39;X\\beta}&amp;=&amp;(\\mathbf{X&#39;X})^{-1}\\mathbf{X&#39;y}\\\\ \\mathbf{\\beta}&amp;=&amp;(\\mathbf{X&#39;X})^{-1}\\mathbf{X&#39;y}\\\\ \\end{eqnarray*}\\] Ta-dah! We have arrived at the matrix algebra solution for the best fitting parameters for a linear model with any number of independent variables. Lets try this formula out in R: X &lt;- as.matrix(cbind(rep(1, nrow(movies)), movies[,c(&quot;Runtime&quot;,&quot;BoxOffice&quot;)])) head(X) ## rep(1, nrow(movies)) Runtime BoxOffice ## 164 1 118 47.1 ## 165 1 104 3.9 ## 166 1 122 30.7 ## 167 1 111 11.4 ## 168 1 123 39.9 ## 169 1 116 32.4 y &lt;- movies$TomatoMeter beta &lt;- solve(crossprod(X))%*%crossprod(X,y) beta ## [,1] ## rep(1, nrow(movies)) 11.09207358 ## Runtime 0.32320152 ## BoxOffice 0.05923506 model &lt;- lm(TomatoMeter~Runtime+BoxOffice, data=movies) coef(model) ## (Intercept) Runtime BoxOffice ## 11.09207358 0.32320152 0.05923506 It works! We can also estimate standard errors using this matrix algebra format. To get the standard errors, we first need to calculate the covariance matrix. \\[\\sigma^{2}(\\mathbf{X&#39;X})^{-1}\\] The \\(\\sigma^2\\) here is the variance of our residuals. Technically, this is the variance of the residuals in the population but since we usually only have a sample we actually calculate: \\[s^2=\\frac{\\sum(y_i-\\hat{y}_i)^2}{n-p-1}\\] based on the fitted values of \\(\\hat{y}_i\\) from our linear model. \\(p\\) is the number of independent variables in our model. The covariance matrix actually provides us with some interesting information about the correlation of our independent variables. For our purposes we just want the square root of the diagonal elements, which gives us the standard error for our model. Continuing with the example estimating tomato meter ratings by run time and box office returns in our movies dataset, lets calculate all the numbers we need for an inference test: #get the predicted values of y y.hat &lt;- X%*%beta df &lt;- length(y)-ncol(X) #calculate our sample variance of the residual terms s.sq &lt;- sum((y-y.hat)^2)/df #calculate the covariance matrix covar.matrix &lt;- s.sq*solve(crossprod(X)) #extract SEs from the square root of the diagonal se &lt;- sqrt(diag(covar.matrix)) ## calculate t-stats from our betas and SEs t.stat &lt;- beta/se #calculate p-values from the t-stat p.value &lt;- 2*pt(-1*abs(t.stat), df) data.frame(beta,se,t.stat,p.value) ## beta se t.stat p.value ## rep(1, nrow(movies)) 11.09207358 3.269128357 3.392976 7.019316e-04 ## Runtime 0.32320152 0.031734307 10.184609 6.617535e-24 ## BoxOffice 0.05923506 0.007978845 7.424014 1.540215e-13 #compare to the lm command summary(model)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 11.09207358 3.269128357 3.392976 7.019316e-04 ## Runtime 0.32320152 0.031734307 10.184609 6.617535e-24 ## BoxOffice 0.05923506 0.007978845 7.424014 1.540215e-13 And it all works. As I said at the beginning, this is a lot of math and not something you need to think about every day. I am not expecting all of this will stick the first time around, but I want it to be here for you as a handy reference for the future. "],
["modeling-non-linearity.html", "Modeling Non-Linearity", " Modeling Non-Linearity By definition, a linear model is only appropriate if the underlying relationship being modeled can accurately be described as linear. To begin, lets revisit a very clear example of non-linearity introduced in an earlier chapter. The example is the relationship between GDP per capita in a country and that country’s life expectancy. We use data from Gapminder to show this relationship with 2007 data. library(gapminder) library(ggrepel) ggplot(subset(gapminder, year==2007), aes(x=gdpPercap, y=lifeExp))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text_repel(data=subset(gapminder, year==2007 &amp; gdpPercap&gt;5000 &amp; lifeExp&lt;60), aes(label=country), size=2)+ labs(x=&quot;GDP per capita&quot;, y=&quot;Life expectancy at birth&quot;, subtitle = &quot;2007 data from Gapminder&quot;)+ scale_x_continuous(labels=scales::dollar)+ theme_bw() Figure 57: The scatterplot of life expectancy and GDP per capita from 2007 shows a clear non-linear diminishing returns relationship Figure 57 shows a scatterplot of this relationship. The relationship is so strong that its non-linearity is readily apparent. Although GDP per capita is positively related to life expectancy, the strength of this relationship diminishes substantial at higher levels of GDP. This is a particular form of non-linearity that is often called a “diminishing returns” relationship. The greater the level of GDP already, the less return you get on life expectancy for an additional dollar. Figure 57 also shows the best fitting line for this scatterplot in blue. This line is added with the geom_smooth(method=\"lm\") argument in ggplot. We can see that this line does not represent the relationship very well. How could it really, given that the underlying relationship clearly curves in a way that a straight line cannot? Even more problematic, the linear fit will produce systematic patterns in the error terms for the linear model. for countries in the middle range of GDP per capita, the line will consistently underestimate life expectancy. At the low and high ends of GDP per capita, the line will consistently overestimate life expectancy. This identifiable pattern in the residuals is an important consequence of fitting a non-linear relationship with a linear model and is in fact one of the ways we will learn to diagnose non-linearity below. Non-linearity is not always this obvious. One of the reasons that it is so clear in Figure 57 is that the relationship between GDP per capita and life expectancy is so strong. When the relationship is weaker, then points tend to be more dispersed and non-linearity might not be so easily diagnosed from the scatterplot. Figure 58 and Figure 59 show two examples that are a bit more difficult to diagnose. ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_jitter(alpha=0.2)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 58: Is this scatterplot of tomato rating by box office returns non-linear? Figure 58 shows a scatterplot of the relationship between tomato rating and box office returns in the movies dataset. I have already made a few additions to this plot to help with the visual display. The tomato rating only comes rounded to the first decimal place, so I use geom_jitter rather than geom_point to perturb the points slightly and reduce overplotting. I also use alpha=0.2 to set the planets as semi-transparent. This is also helps with identifying areas of the scatterplot that are dense with points because they turn darker. The linearity of this plot is hard to determine. This is partially because of the heavy right-skew of the box office returns. Many of the points are densely packed along the x-axis because the scale of the y-axis is so large to deal with outliers. Nonetheless, visually it appears there might be some evidence of an increasing slope as tomato rating gets higher - this is an exponential relationship which is the inverse of the diminishing returns relationship we saw earlier. Still, its hard to know if we are really seeing it or not with the data in this form. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=1)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ theme_bw() Figure 59: Is the relationship between age and hourly wage non-linear? Figure 59 shows the relationship between a respondent’s age and their hourly wages from the earnings data. There are so many observations here that I reduce alpha all the way to 0.01 to address overplotting. We also have a problem of a right-skewed dependent variable here. Nonetheless, you can make out a fairly clear dark band across the plot that seems to show a positive relationship. It also appears that this relationship may be of a diminishing returns type with a lower return to age after age 30 or so. However, with the wide dispersion of the data, it is difficult to feel very confident about the results. The kind of uncertainty inherent in Figures 58 and 59 is far more common in practice than the clarity in Figure 57. Scatterplots can be a first step to detecting non-linearity, but usually we need some additional diagnostics. In the next two sections, I will cover two diagnostic approaches to identifying non-linearity: smoothing and residual plots. Smoothing Smoothing is a technique for estimating the relationship in a scatterplot without the assumption that this relationship be linear. In order to understand what smoothing does, it will be first helpful to draw a non-smoothed line between all the points in a scatterplot, starting with the smallest value of the independent variable to the largest value. I do that in Figure 60. As you can see this produces a very jagged line that does not give us much sense of the relationship because it bounces around so much. movies &lt;- movies[order(movies$TomatoRating),] ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_point(alpha=0.05)+ geom_line()+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 60: Connecting the dots in our scatterplot leads to a very jagged line The basic idea of smoothing is to smooth out that jagged line. This is done by replacing each actual value of \\(y\\) with a predicted value \\(\\hat{y}\\) that is determined by that point and its nearby neighbors. The simplest kind of smoother is often called a “running average.” In this type of smoother, you simply take \\(k\\) observations above and below the given point and calculate the mean or median. For example, lets say we wanted to smooth the box office return value for the movie Rush Hour 3. We want to do this by reaching out to the two neighbors above and below this movie in terms of the tomato rating. After sorting movies by tomato rating, here are the values for Rush Hour 3 and the two movies closest to it: Title TomatoRating BoxOffice Awake 4.2 14.3 Ghost Rider 4.2 115.8 Rush Hour 3 4.2 140.1 Balls of Fury 4.2 32.8 Nobel Son 4.2 0.3 To smooth the value for Rush Hour 3, we can take either the median or the mean of the box office returns for these five values: box_office &lt;- c(14.3, 115.8, 140.1, 32.8, 0.3) mean(box_office) ## [1] 60.66 median(box_office) ## [1] 32.8 The mean smoother would give us a value of $60.66 million and the median smoother a value of $32.8 million. In either case, the value for Rush Hour 3 is pulled far away from its outlier position at $140.1 million, thus smoothing the display. To get a smoothed line, we need to repeat this process for every point in the dataset. The runmed command will do this for us once we have the data sorted correctly by the independent variable. Figure 61 shows two different smoothed lines applied to the movie scatterplot, with different windows. movies$BoxOffice.smooth1 &lt;- runmed(movies$BoxOffice, 5) movies$BoxOffice.smooth2 &lt;- runmed(movies$BoxOffice, 501) ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_point(alpha=0.2)+ geom_line(col=&quot;grey&quot;, size=1, alpha=0.7)+ geom_line(aes(y=BoxOffice.smooth1), col=&quot;blue&quot;, size=1, alpha=0.7)+ geom_line(aes(y=BoxOffice.smooth2), col=&quot;red&quot;, size=1, alpha=0.7)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 61: Two different median smoothers are shown. The one in blue uses a window of two neighbors to each side while the one one in red uses a window of 250 neighbors on each side. It is clear here that using two neighbors on each side is not sufficient in a dataset of this size. With 250 neighbors on each side, we get something much smoother, but it also reveals a sharp uptick at the high end of the dataset, which suggests some possible non-linearity. A more sophisticated smoothing procedure is available via the LOESS (locally estimated scatterplot smoothing) smoother. The LOESS smoother uses a regression model with polynomial terms (discussed below) on a local subset of the data to estimate a predicted value for each observation. The regression model also typically weights values so that observations closer to the index observation count more in the estimation. This technique tends to produce better results than median or mean smoothing. You can easily fit a LOESS smoother to a scatterplot in ggplot by specifying method=\"loess\" in the geom_smooth function. In fact, this is the default method for geom_smooth for datasets smaller than a thousand observations. Figure 62 plots this LOESS smoother as well as a linear fit for comparison. ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_jitter(alpha=0.2)+ geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;, se=FALSE)+ geom_smooth(se=FALSE, method=&quot;loess&quot;)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 62: A LOESS smoother to the data is shown in blue while a linear fit is shown in red. Again we see the exponential relationship in the LOESS smoother. The LOESS smoother is also much smoother than the median smoothers we calculated before. By default the LOESS smoother in geom_smooth will use 75% of the full data in its subset for calculating the smoothed value of each point. This may seem like a lot, but remember that it weights values closer to the index point. You can adjust this percentage using the span argument in geom_smooth. SHINY APP The main disadvantage of the LOESS smoother is that it becomes computationally inefficient as the number of observations increases. If I were to apply the LOESS smoother to the 145,647 observations in the earnings data, it would kill R before producing a result. For larger dataset, another option for smoothing is the general additive model (GAM). This model is complex and I won’t go into the details here, but it provides the same kinds of smoothing as LOESS but is much less computationally expensive. GAM smoothing is the default in geom_smooth for datasets larger than a thousand observations, so you do not need to include a method. Figure 63 shows A GAM smoother applied to the relationship between wages and age. The non-linear diminishing returns relationship is clearly visible in the data. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=1)+ geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;, se=FALSE)+ geom_smooth(se=FALSE)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ theme_bw() Figure 63: A GAM smoother (shown in blue) applied to the relationship between age and wages in the earnings data. A linear fit is shown in red. Residual Plots Another technique for detecting non-linearity is the to plot the fitted values of a model by the residuals. To demonstrate how this works, lets first calculate a model predicting life expectancy by GDP per capita from the gapminder data earlier. model &lt;- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007)) If you type plot(model), you will get a series of model diagnostic plots in base R. The first of these plots is the residual vs. fitted values plot. We are going to make the same plot, but in ggplot. In order to do that, I need to introduce a new package called broom that has some nice features for summarizing results from your model. It is part of the Tidyverse set of packages. To install it type install.packages(\"broom\"). You can then load it with library(broom). The broom package has only three commands: tidy which shows the key results from the model including coefficients, standard errors and the like. augment which adds a variety of diagnostic information to the original data used to calculate the model. This includes residuals, cooks distance, fitted values, etc. glance which gives a summary of model level goodness of fit statistics. At the moment, we want to use augment. Here is what the it looks like: library(broom) augment(model) ## # A tibble: 142 x 9 ## lifeExp gdpPercap .fitted .se.fit .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 43.8 975. 60.2 0.973 -16.4 0.0120 8.82 0.0207 -1.85 ## 2 76.4 5937. 63.3 0.818 13.1 0.00846 8.86 0.00928 1.48 ## 3 72.3 6223. 63.5 0.812 8.77 0.00832 8.90 0.00411 0.990 ## 4 42.7 4797. 62.6 0.848 -19.9 0.00907 8.77 0.0231 -2.25 ## 5 75.3 12779. 67.7 0.750 7.61 0.00709 8.91 0.00263 0.858 ## 6 81.2 34435. 81.5 1.52 -0.271 0.0292 8.93 0.0000144 -0.0309 ## 7 79.8 36126. 82.6 1.61 -2.75 0.0327 8.93 0.00167 -0.315 ## 8 75.6 29796. 78.5 1.29 -2.91 0.0211 8.93 0.00118 -0.331 ## 9 64.1 1391. 60.5 0.958 3.61 0.0116 8.93 0.000975 0.408 ## 10 79.4 33693. 81.0 1.48 -1.59 0.0278 8.93 0.000471 -0.181 ## # … with 132 more rows We can use the dataset returned here to build a plot of the fitted values (.fitted) by the residuals (.resid) like this: ggplot(augment(model), aes(x=.fitted, y=.resid))+ geom_point()+ geom_hline(yintercept = 0, linetype=2)+ geom_smooth(se=FALSE)+ labs(x=&quot;fitted values of life expectancy&quot;, y=&quot;model residuals&quot;)+ theme_bw() Figure 64: A residual vs. fitted values plot for a model predicting life expectancy by GDP per capita. Their is a clear trend here whigh signifies non-linearity. If a linear model fitted the data well, then we would expect to see no evidence of a pattern in the scatterplot of 64. We should just see a cloud of points centered around the value of zero. The smoothing line helps us see patterns, but in this case the non-linearity is so strong that we would be able to detect it even without the smoothing. This clear pattern indicates that we non-linearity in our model and we should probably reconsider our approach. Figures 65 and 66 show residual vs fitted value plots for the two other cases of movie box office returns and wages that we have been examining. In these cases, the patterns are somewhat less apparent without the smoothing. The movie case does not appear particularly problematic although there is some argument for exponential increase at very high values of tomato rating. The results for wages suggest a similar diminishing returns type problem. Figure 65: A residual vs. fitted values plot for a model predicting movie box office returns by tomato rating. Figure 66: A residual vs. fitted values plot for a model predicting movie box office returns by tomato rating. Both of these figures also reveal an additional problem unrelated to non-linearity. Both figures show something of a cone shape to the scatterplot in which the variance of the residuals gets larger at higher fitted values. This is the problem of heteroskedasticity that we will return to in the next section. Now that we have diagnosed the problem of non-linearity, what can we do about it? There are several ways that we can make adjustments to our models to allow for non-linearity. The most common technique is to use transformations to change the relationship between independent and dependent variables. Another approach is to include polynomial terms into the model that can simulate a parabola. A third, less common, option is to create a spline term that allows for non-linearity at specific points. I cover each of these techniques below. Transformations You transform your data when you apply a mathematical function to a variable to transform its values into different values. There are a variety of different transformations that are commonly used in statistics, but we will focus on the one transformation that is most common in the social sciences: the log transformation. Transformations can directly address issues of non-linearity. By definition if you transform the independent variable, the dependent variable, or both variables in your model, then the linear relationship assumed by the model between the transformed variables. On the original scale of the variables, the relationship is non-linear. Transformations can also have additional side benefits besides fitting non-linear relationships. The log transformation, for example, will pull in extreme values and skewness in a distribution, reducing the potential for outliers to exert a strong influence on results. Take Figure 67 for example. This figure shows the distribution of hourly wages in the earnings data. This distribution is heavily right-skewed, despite the fact that wages have been top-coded at $100. Any model of wages is likely to be influenced considerably by the relatively small but still numerically substantial number of respondents with high wages. Figure 67: Hourly wages are heavily right skewed even after top-coding the data at a wage of $100. Lets look at that distribution again, but this time on the log-scale. We will cover in more detail below what the mathematical log means, but for now you can just think of the log-scale as converting from an additive scale to a multiplicative scale. On the absolute scale, we want to have equally spaced intervals on the axes of our graph to represent a constant absolute amount difference. You can see in Figure 67 that the tick-marks for wages are at evenly spaced intervals of $25. On the log-scale we want evenly-spaced intervals on the axes of our graph to correspond to multiplicative changes. So on a log base 10 scale, we would want the numbers 1, 10, and 100, to be evenly spaced because each one represents a 10 fold increase multiplicative increase. In ggplot, you can change the axis to a log-scale with the argument scale_x_log10. Figure 68 shows the same distribution of hourly wages, but this time using the log-scale. Figure 68: On the log-scale, hourly wages is less skewed, although now we have a slight problem of a left-skew You can clearly see in Figure 68 that we have pulled in the extreme right tail of the distribution and have a somewhat more symmetric distribution. The transformation here hasn’t perfectly solved things, because now we have a slight left-skew for very low hourly wages. Nonetheless, even these values are closer in absolute value after the transformation and so will be less likely to substantially influence our model. This transformation can also help us with the cone-shaped residual issue we saw earlier (i.e. heteroskedasticity), but we will discuss that more in the next section. Because transformations can often solve multiple problems at once, they are very popular. In fact, for some common analysis like estimating wages or CO2 emissions, log transformations are virtually universal. The difficult part about transformations is understanding exactly what kind of non-linear relationship you are estimating by transforming and how to interpret the results from models with transformed variables. We will develop this understanding using the natural log transformation on the examples we have been looking at in this section. The log transformation Before getting into the mathematical details of the log transformation, lets first demonstrate its power. Figure 69 shows the relationship between Rotten Tomatoes rating and box office returns in our movies data. However, this time I have applied a log-scale to box office returns. Although we still see some evidence of non-linearity in the LOESS estimator, the relationship looks much more clearly linear than before. But the big question is how do we interpret the slope and intercept for the linear model fit to this log-transformed data? ggplot(movies, aes(x=TomatoRating, y=BoxOffice))+ geom_jitter(alpha=0.2)+ geom_smooth(method=&quot;lm&quot;, color=&quot;red&quot;, se=FALSE)+ geom_smooth(se=FALSE, method=&quot;loess&quot;)+ scale_y_log10(labels = scales::dollar)+ labs(x=&quot;Rotten Tomatoes Rating&quot;, y=&quot;Box Office Returns (millions)&quot;)+ theme_bw() Figure 69: Scatterplot of the relationship between rotten tomatoes rating and box office returns, with a log-scale applied to box office returns. In order to understand the model implied by this scatterplot, we need to delve a little more deeply into the math of the log transformation. In general, the log equation means converting some number into the exponential value of some base multiplier. So, if we wanted to know what the log base 10 value of 27 is, we need to know what exponential value applied to 10 would produce 27: \\[27=10^x\\] We can calculate this number easily in R: x &lt;- log(27, base=10) x ## [1] 1.431364 10^x ## [1] 27 Log base 10 and log base 2 are both common values used in teaching log transformations, but the standard base we use in statistics is the mathematical constant \\(e\\) (2.718282). This number is of similar importance in mathematics as \\(\\pi\\) and is based on the idea of what happens to compound interest as the period of interest accumulation approaches zero. This has a variety of applications from finance to population growth, but none of this concerns us directly. We are interested in \\(e\\) as a base for our log function because of certain other useful properties discussed further below that ease interpretation. When we use \\(e\\) as the base for our log function, then we are using what is often called the “natural log.” We can ask the same question about 27 as before but now with the natural log: x &lt;- log(27) x ## [1] 3.295837 exp(x) ## [1] 27 I didn’t have to specify a base here because R defaults to the natural log. The exp function calculates \\(e^x\\). For our purposes, the most important characteristic of the log transformation is that it converts multiplicative relationships into additive relationships.This is because of a basic mathematical relationship where: \\[e^a*a^b=e^{a+b}\\] \\[log(x*y) = log(x)+log(y)\\] You can try this out in R to see that it works: exp(2)*exp(3) ## [1] 148.4132 exp(2+3) ## [1] 148.4132 log(5*4) ## [1] 2.995732 log(5)+log(4) ## [1] 2.995732 We can use these mathematical relationships to help understand a model with log-transformed variables. First lets calculate the model implied by the scatterplot above. We can just use the log command to directly transform box office returns in the lm formula: model &lt;- lm(log(BoxOffice)~TomatoRating, data=movies) coef(model) ## (Intercept) TomatoRating ## 0.9727622 0.2406026 OK, so what does that mean? It might be tempting to interpret the results here as you normally would. We can see that tomato rating has a positive effect on box office returns. So, a one point increase in tomato rating is associated with a 0.241 increase in … what? Remember that our dependent variable here is now the natural log of box office returns, not box office returns itself. We could literally say that it is a 0.241 increase in log box office returns, but that is not a very helpful or intuitive way to think about the result. Similarly the intercept gives us the predicted log box office returns when tomato rating is zero. That is not helpful for two reasons: its outside the scope of the data, and we don’t really know how to think about a log box office returns of 0.973. In order to translate this into something meaningful, lets try looking at this in equation format. Here is what we have: \\[\\hat{\\log(y_i)}=0.973+0.241*x_i\\] What we really want is to be able to understand this equation back on the original scale of the dependent variable, which in this case is box office returns in millions of dollars. Keep in mind that taking \\(e^{\\log(y)}\\) just gives us back \\(y\\). We can use that logic here. If we “exponentiate” (take \\(e\\) to the power of the values) the left-hand side of the equation, then we can get back to \\(\\hat{y}_i\\). However, remember from algebra, that what we do to one side of the equation, we have to do to both sides. That means: \\[e^{\\hat{\\log(y_i)}}=e^{0.973+0.241*x_i}\\] \\[\\hat{y}_i=(e^{0.971})*(e^{0.241})^{x_i}\\] We now have quite a different looking equation for predicting box office returns. The good news is that we now just have our predicted box office returns back on the left-hand side of the equation. The bad news is that the right hand side looks a bit complex. Since \\(e\\) is just a constant value, we can go ahead and calculate the values in those parentheses (called “exponentiating”): exp(0.971) ## [1] 2.640584 exp(0.241) ## [1] 1.272521 That means: \\[\\hat{y}_i=(2.64)*(1.27)^{x_i}\\] We now have a multiplicative relationship rather than an additive relationship. How does this changes our understanding of the results? To see, lets plug in some values for \\(x_i\\) and see how it changes our predicted income value. A tomato rating of zero is outside the scope of our data, but lets plug it in for instructional purposes anyway: \\[\\hat{y}_i=(2.64)*(1,27)^{0}=(2.64)(1)=2.64\\] So, the predicted box office returns when \\(x\\) is zero is just given by exponentiating the intercept. Lets try increasing tomato rating by one point: \\[\\hat{y}_i=(2.64)*(1.27)^{1}=(2.64)(1.27)\\] I could go ahead and finish that multiplication, but I want to leave it here to better show how to think about the change. A one point increase in tomato rating is associated with an increase in box office returns by a multiplicative factor of 1.27. In other words, a one point increase in tomato rating is associated with a 27% increase in income, on average. What happens if I add another point? \\[\\hat{y}_i=(2.64)*(1.27)^{2}=(2.64)(1.27)(1.27)\\] Each additional point leads to a 27% increase in predicted box office returns. This is what I mean by a multiplicative increase. We are no longer talking about the predicted change in box office returns in terms of absolute numbers of dollars, but rather in relative terms of percentage increase. In general, in order to properly interpret your results when you log the dependent variable, you must exponentiate all of your slopes and the intercept. You can then interpret them as: The model predicts that a one-unit increase in \\(x_j\\) is associated with a \\(e^{b_j}\\) multiplicative increase in \\(y\\), on average while holding all other independent variables constant. The model predicts that \\(y\\) will be \\(e^{b_0}\\) on average when all independent variables are zero. Of course, just like all of our prior examples, you are responsible for converting this into sensible English. Lets try a more complex model in which we predict log box office returns by tomato rating, runtime and movie maturity rating: model &lt;- lm(log(BoxOffice)~TomatoRating+Runtime+Rating, data=movies) coef(model) ## (Intercept) TomatoRating Runtime RatingPG RatingPG-13 RatingR ## -0.81318674 0.17783703 0.03863665 -0.75833147 -1.29892956 -2.96580424 We have a couple of added complexities here. We now have results for categorical variables as well as negative numbers to interpret. In all cases, we want to exponentiate to interpret correctly, so lets go ahead and do that: exp(coef(model)) ## (Intercept) TomatoRating Runtime RatingPG RatingPG-13 RatingR ## 0.44344268 1.19463061 1.03939275 0.46844739 0.27282368 0.05151902 The tomato rating effect can be interpreted as before but now with control variables: The model predicts that a one point increase in the tomato rating is associated with a 19% increase in box office returns, on average, holding constant movie runtime and maturity rating. The runtime effect can be interpreted in a similar way. The model predicts that a one minute increase in movie runtime is associated with a 3.9% increase in box office returns, on average, holding constant movie tomato rating and maturity rating. In both of these cases, it makes sense to take the multiplicative effect and convert that into a percentage increase. If I multiply the predicted box office returns by 1.039, then I am increasing it by 3.9%. When effects get large, however, it sometimes makes more sense to just interpret it as a multiplicative factor. For example, if the exponentiated coefficient was 3, I could interpret this as a 200% increase. However, it probably makes more sense in this case to just say something along the lines of the “predicted value of \\(y\\) triples in size for a one point increase in \\(x\\).” The categorical variables can be interpreted as we normally do in terms of the average difference between the indicated category and the reference category. However, now it is the multiplicative difference. So, taking the PG effect of 0.468, we could just say: The model predicts that, controlling for tomato rating and runtime, PG-rated movies make 46.8% as much as G-rated movies at the box office, on average. However, its also possible to talk about how much less a PG-rated movie makes than a G-rated movie. If a PG movie makes 46.8% as much, then it equivalently makes 52.2% less. We get this number by just subtracting the original percentage from 100. So I could have said: The model predicts that, controlling for tomato rating and runtime, PG-rated movies make 52.2% less than G-rated movies, on average. In general to convert any coefficient \\(\\beta_j\\) from a model with a logged dependent variable to a percentage change scale we can follow this formula: \\[(e^{\\beta_j}-1)*100\\] This will give us the percentage change and the correct direction of the relationship. You may have noted that the effect of runtime in the model above before exponentiating was 0.0386 and after we exponentiated the result, we concluded that the effect of one minute increase in runtime was 3.9%. If you move over the decimal place two, those numbers are very similar. This is not a coincidence. It follows from what is known as the Taylors series expansion for the exponential: \\[e^x=1+x-\\frac{x^2}{2!}+\\frac{x^3}{3!}-\\frac{x^4}{4!}+\\ldots\\] Any exponential value \\(e^x\\) can be calculated from this Taylor series expansion which continues indefinitely. However, when \\(x\\) is a small value less than one, note that you are squaring, cubing, and so forth, which results in an even smaller number that is divided by an increasingly large number. So for, small values of \\(x\\) the following approximation works reasonably well: \\[e^x\\approx1+x\\] In the case I just noted where \\(x=0.0386\\), the actual value of \\(e^{0.0386}\\) is 1.0394 while the approximation gives us 1.0386. That is pretty close. This approximation starts to break down around \\(x=0.2\\). The actual value of \\(e^{0.2}\\) is 1.221 or about a 22.1% increase, whereas the approximation gives us 1.2 or a straight 20% increase. You can use this approximation to get a ballpark estimate of the effect size in percentage change terms for coefficients that are not too large without having to exponentiate. So I can see that the coefficient of 0.177 for tomato rating is going to be roughly a 18-19% increase and the coefficient of 0.0386 is going to be a little under a 4% increase, all without having to exponentiate. Notice that this does not work at all well for the very large coefficients for movie maturity rating. Logging the independent variable The previous example showed how to interpret results when you log the dependent variables. Logging the dependent variable will help address exponential type relationships. However in cases of a diminishing returns type relationship, we instead need to log the independent variable. Figure 70 shows the GDP per capita by life expectancy scatterplot from before but now with GDP per capita on a log scale. Figure 70: Scatterplot of life expectancy by GDP per capita, with a log-scale applied to GDP per capita. Look how much more linear the relationship looks. It is really quite impressive how this simple transformation straightened out that very clear diminishing returns type relationship we observed before. This is because on the log-scale a one unit increase means a multiplicative change so, you have to make a much larger absolute change when you are already high in GDP per capita to get the same return in life expectancy. Models with logged independent variables are a little trickier to interpret. To understand how it works, lets go ahead and calculate the model: model &lt;- lm(lifeExp~log(gdpPercap), data=subset(gapminder, year==2007)) round(coef(model), 5) ## (Intercept) log(gdpPercap) ## 4.94961 7.20280 So our model is: \\[\\hat{y}_i=4.9+7.2\\log{x_i}\\] Because the log transformation is on the right hand side we can no longer use the trick of exponentiating its value because that would just give us \\(e^{\\hat{y}_i}\\) on the left hand side. It would also be impractical if we added other independent variables to the model. Instead, we can interpret our slope by thinking about how a 1% increase in \\(x\\) would change the predicted value of \\(y\\). For ease of interpretation, lets start with \\(x=1\\). The log of 1 is always zero, so: \\[\\hat{y}_i=4.9+7.2\\log{1}=4.9+7.2*0=\\beta_0\\] So, the intercept here is actually the predicted value of life expectancy when GDP per capita is $1. This is not a terribly realistic value for GDP per capita where the lowest value in the dataset is $241, so its not surprising that we get an unrealistically low life expectancy value. Now what happens if we raise GDP per capita by 1%? A 1% increase from a value of 1 would give us a value of 1.01. So: \\[\\hat{y}_i=4.9+7.2\\log{1.01}\\] From the Taylor series above, we can deduce that the \\(\\log{1.01}\\) is roughly equal to 0.01. So: \\[\\hat{y}_i=4.9+7.2*0.01=4.9+0.072\\] This same logic will apply to any 1% increase in GDP per capita. So, the model predicts that a 1% increase in GDP per capita is associated with a 0.072 year increase in the life expectancy. In general, you can interpret any coefficient \\(\\beta_j\\) with a logged independent variable but not logged dependent variable as: The model predicts that a a 1% increase in x is associated with a \\(\\beta_j/100\\) unit increase in y. One easy way to remember what the log transformation does is that it makes your measure of change relative rather than absolute for the variable transformed. When we logged the dependent variable before then our slope could be interpreted as the percentage change in the dependent variable (relative) for a one unit increase (absolute) in the independent variable. When we log the independent variable, the slope can be interpreted as the unit change in the dependent variable (absolute) for a 1% increase (relative) in the independent variable. Logging both independent and dependent variables: The elasticity model If logging the dependent variable will make change in the dependent variable relative, and logging the independent variable will make change in the independent variable relative, then what happens if you log transform both. Then you get a model which predicts relative change by relative change. This model is often called an “elasticity” model because the predicted slopes are equivalent to the concept of elasticity in economics: how much does of a percent change in \\(y\\) results from a 1% increase in \\(x\\). The relationship between age and wages may benefit from logging both variables. We have already seen a diminishing returns type relationship which suggests logging the independent variable. We also know that wages are highly right-skewed and so might also benefit from being logged. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=0.1)+ geom_smooth(se=FALSE, method=&quot;lm&quot;, color=&quot;red&quot;)+ geom_smooth(se=FALSE)+ scale_y_log10(labels = scales::dollar)+ scale_x_log10()+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ theme_bw() Figure 71: Is the relationship between age and hourly wage non-linear? Figure 71 shows the relationship between age and wages when we apply a log scale to both variables. It looks a little better although there is still some evidence of diminishing returns even on this scale. Lets go with it for now though. Lets take a look at the coefficients from this model: model &lt;- lm(log(wages)~log(age), data=earnings) coef(model) ## (Intercept) log(age) ## 1.1558386 0.5055849 One of the advantages of the elasticity model is that slopes can be easily interpreted. We can use the same techniques when logging independent variables to divide our slope by 100 to see that a one percent increase in age is associated with a 0.0051 increase in the dependent variable. However this dependent variable is the log of wages. T interpret the effect on wages, we need to exponentiate this value, subtract 1, and multiply by 100. However since the value is small, we know that this is going to be roughly a 0.51% increase. So: The model predicts that a one percent increase in age is associated with a 0.51% increase in wages, on average. In other words, we don’t have to make any changes at all. The slope is already the expected percent change in \\(y\\) for a one percent increase in \\(x\\). The STRIPAT model that Richard York helped to develop is an example of an elasticity model that is popular in environmental sociology. In this model, the dependent variable is some measure of environmental degradation, such as CO2 emissions. The independent variable can be things like GDP per capita, population size and growth, etc. All variables are logged so that model parameters can be interpreted as elasticities. The square root transformation The log transformation is very flexible and solves multiple problems at once (non-linearity, outliers, skewness), which explains its popularity. But it breaks down in one important situation: you cannot log a variable that has zero or negative values. The negative case is generally not as important although there are some exceptions (like net worth). On the other hand, there are numerous cases where a quantitative variable can be zero as well as positive. Lets run an elasticity model on box office returns, but this time lets predict returns by the Tomato Meter rather than the Tomato Rating. summary(lm(log(BoxOffice)~log(TomatoMeter), data=movies)) ## Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...): NA/NaN/Inf in &#39;x&#39; Oh no! We got an error. The problem is that the Tomato Meter has a few cases of zero values (when a movie received zero positive reviews). The log of zero is negative infinity and that simply won’t work when fitting a linear model. What can you do? It turns out that the square root transformation can do much the same work as the natural logarithm. It will pull in skewness and can make non-linear relationships more linear. Since the square root of zero is a real number (zero to be precise), it will also work on variables that have legitimate zero values. Figure 72 shows a scatterplot of tomato meter by box office returns with both variables on the log scale. Movies with a tomato meter of zero have been removed. Figure 73 shows the same figure but now with Tomato Meter on the square root scale. The figures looks pretty similar and fit a similar trend but the square root transformation can also include the zero values. ggplot(subset(movies, TomatoMeter&gt;0), aes(x=TomatoMeter, y=BoxOffice))+ geom_vline(xintercept = 0, linetype=2)+ geom_jitter(alpha=0.5)+ scale_y_log10()+ scale_x_log10()+ geom_smooth(se=FALSE)+ labs(x=&quot;Tomato Meter (square root scale)&quot;, y=&quot;Box Office Returns (log scale)&quot;)+ theme_bw() ## Warning: Transformation introduced infinite values in continuous x-axis Figure 72: Scatterplot of tomato meter by box office returns, using a log transformation of tomato meter ratings. ggplot(movies, aes(x=TomatoMeter, y=BoxOffice))+ geom_vline(xintercept = 0, linetype=2)+ geom_jitter(alpha=0.5)+ scale_y_log10()+ scale_x_sqrt()+ geom_smooth(se=FALSE)+ labs(x=&quot;Tomato Meter (square root scale)&quot;, y=&quot;Box Office Returns (log scale)&quot;)+ theme_bw() Figure 73: Scatterplot of tomato meter by box office returns, using a square root transformation of tomato meter ratings. The dotted vertical line shows cases with a zero tomato meter. We can estimate this model easily enough in R: model &lt;- lm(log(BoxOffice)~sqrt(TomatoMeter), data=movies) coef(model) ## (Intercept) sqrt(TomatoMeter) ## 1.4376939 0.1274002 There is one big downside to this transformation. Unlike log transformations, there is no clear and easy interpretation of how to intepret this effect. Since we know that the square root transformation is doing something close to a log transformation, we can think of this result as loosely close to an elasticity in this case, but that is only a loose approximation and may deteriorate as you get larger and larger values of the independent variable. Polynomial Models Another method that can be used to fit non-linear relationships is to fit polynomial terms. You may recall the the following formula from basic algebra: \\[y=a+bx+cx^2\\] This formula defines a parabola which fits not a straight line but a curve with one point of inflection. We can fit this curve in a linear model by simply including the square of a given independent variable as an additional variable in the model. Before we do this it is usually a good idea to re-center the original variable somewhere around the mean because this will reduce the correlation between the original term and its square. For example, I could fit the relationship between age and wages using polynomial terms: model &lt;- lm(wages~I(age-40)+I((age-40)^2), data=earnings) coef(model) ## (Intercept) I(age - 40) I((age - 40)^2) ## 26.92134254 0.31711167 -0.01783204 The coefficients for such a model are a little hard to interpret directly. This is a case where marginal effects can help us understand what is going on. The marginal effect can be calculated by taking the partial derivative using calculus of the model equation with respect to \\(x\\). I don’t expect you to know how to do this, so I will just tell how it turns out. In a model with a squared term: \\[\\hat{y}=\\beta_0+\\beta_1x+\\beta_2x^2\\] The marginal effect (or slope) is given by: \\[\\beta_1+2\\beta_2x\\] Note that the slope of \\(x\\) is itself partially determined by the value of \\(x\\). This is what drives the non-linearity. If we plug in the values for the model above, the relationship will be clearer: \\[0.3171+2(-0.0178)x=0.3171-0.0356x\\] So when \\(x=0\\), which in this case means age 40, a one year increase in age is associated with a $0.32 increase in hourly wage, on average. For every increase in the year of age past 40, that effect of a one year increase in age on wages decreases in size by $0.035. For every year below age 40, the effect increases by $0.035. Because \\(\\beta_1\\) is positive and \\(\\beta_2\\) is negative in this case it corresponds to a diminishing returns kind of relationship. Unlike the log transformation however, the relationship can ultimately reverse direction from positive to negative. I can figure out the inflection point at which it will transition from a positive to a negative relationship. I won’t delve into the details here, but this point is given by setting the derivative of this equation with respect to age equal to zero and solving for age. In general, this will give me: \\[\\beta_j/(-2*\\beta_{sq})\\] Where \\(\\beta_j\\) is the coefficient for the linear effect of the variable (in this case, age), and \\(\\beta_{sq}\\) is the coefficient for the squared effect. In my case, I get: \\[0.3171/(-2*-0.0178)=0.3171/0.0356=8.91\\] Remember that we re-centered age at 40, so this means that the model predict that age will switch from a positive to a negative relationship with wages at age 48.91. Often, the easiest way to really get a sense of the parabolic relationship we are modeling is to graph it. This can be done easily with ggplot by using the geom_smooth function. In the geom_smooth function you specify lm as the method, but also specify a formula argument that specifies this linear model should be fit as a parabola rather than a straight line. ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width=1)+ scale_y_continuous(labels = scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;hourly wages&quot;)+ geom_smooth(se=FALSE, method=&quot;lm&quot;, formula=y~x+I(x^2))+ geom_vline(xintercept = 48.91, linetype=2, color=&quot;red&quot;)+ theme_bw() Figure 74: A parabolic model fit to the relationship between age and wages. The dotted red line shows the maximum value of the curve where the relationship between wages and age goes from positive to negative Figure 74 shows the relationship between age and wages after adding a squared term to our linear model. I have also added a dashed line to indicate the inflection point at which the relationship shifts from positive to negative. Polynomial models do not need to stop at a squared term. You can also add a cubic term and so on. The more polynomial terms you add, the more inflection points you are allowing for in the data, but also the more complex it will be to interpret. Figure 75 shows the predicted relationship between GDP per capita and life expectancy in the gapminder data when using a squared and cubic term in the model. You can see two different inflection points in the data that allow for more complex curves than one could get using transformations. ggplot(subset(gapminder, year==2007), aes(x=gdpPercap, y=lifeExp))+ geom_point()+ labs(x=&quot;GDP per capita&quot;, y=&quot;life expectancy&quot;)+ geom_smooth(se=FALSE, method=&quot;lm&quot;, formula=y~x+I(x^2)+I(x^3))+ theme_bw() Figure 75: A model predicting life expectancy from GDP per capita with squared and cubic terms. Splines Another approach to fitting non-linearity is to fit a spline. Splines can become quite complex, but I will focus here on a very simple spline. In a basic spline model, you allow the slope of a variable to have a different linear effect at different cutpoints or “hinge” values of \\(x\\). Within each segment defined by the cutpoints, we simply estimate a linear model (although more complex spline models allow for non-linear effects within segments). Looking at the diminishing returns to wages from age in the earnings data, it might be reasonable to fit this effect with a spline model. It looks like age 35 might be a reasonable value for the hinge. In order to fit the spline model, we need to add a spline term. This spline term will be equal to zero for all values where age is equal to or below 35 and will be equal to age minus 35 for all values where age is greater than 35. We then add this variable to the model. I fit this model below. earnings$age.spline &lt;- ifelse(earnings$age&lt;35, 0, earnings$age-35) model &lt;- lm(wages~age+age.spline, data=earnings) pander(tidy(model)) term estimate std.error statistic p.value (Intercept) -6.039 0.3107 -19.44 4.723e-84 age 0.9472 0.01035 91.52 0 age.spline -0.9549 0.01405 -67.98 0 This spline will produce two different slopes. For individuals 35 years of age or younger, the predicted change in wages for a one year increase will be given by the main effect of age, because the spline term will be zero. For individuals over 35 years of age, a one year increase in age will produce both the main age effect and the spline effect, so the total effect is given by adding the two together. So in this case, I would say the following: The model predicts that for individuals 35 years of age and younger, a one year increase in age is associated with a $0.95 increase in wages on average. The model predicts that for individuals over 35 years of age, a one year increase in age is associated with a $0.0077 (0.9742-0.9549). We can also graph this spline model, but we cannot do it simply through the formula argument in geom_smooth. In order to do that, you will need to learn a new command. The predict command can be used to get predicted values from a model for a different dataset. The predict command needs at least two arguments: (1) the model object for the model you want to predict, and (2) a data.frame that has the same variables as those used in the model. In this case, we can create a simple “toy” dataset that just has one person of every age from 18 to 65, with only the variables of age and age.spline. We can then use the predict command to get predicted wages for these individuals and add that as a variable to our new dataset. We can than plot these predicted values as a line in ggplot by feeding in the new data into the geom_line command. The r code chunk below does all of this to produce Figure 76. The spline model seems to fit pretty well here and is quite close to the smoothed line. This kind of model is sometimes called a “broken arrow” model because the two different slopes produce a visual effect much like an arrow broken at the hinge point. predict_df &lt;- data.frame(age=18:65) predict_df$age.spline &lt;- ifelse(predict_df$age&lt;35, 0, predict_df$age-35) predict_df$wages &lt;- predict(model, newdata=predict_df) ggplot(earnings, aes(x=age, y=wages))+ geom_jitter(alpha=0.01, width = 1)+ geom_smooth(se=FALSE)+ geom_line(data=predict_df, color=&quot;red&quot;, size=1.5, alpha=0.75)+ theme_bw() Figure 76: Scatterplot of age and hourly wages, with a spline fit hinged on age 35 shown in red. A smoothing line is also shown in blue. "],
["the-iid-violation-and-robust-standard-errors.html", "The IID Violation and Robust Standard Errors", " The IID Violation and Robust Standard Errors Lets return to the idea of the idea of the linear model as a data-generating process: \\[y_i=\\underbrace{\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}}_{\\hat{y}_i}+\\epsilon_i\\] To get a given value of \\(y_i\\) for some observation \\(i\\) you feed in all of the independent variables for that obseravation into your linear model equation to get a predicted value, \\(\\hat{y}_i\\). However, you still need to take account of the stochastic element that is not predicted by your linear function. To do this, imagine reaching into a bag full of numbers and pulling out one value. Thats your \\(\\epsilon_i\\) which you add on to your predicted value to get the actual value of \\(y\\). What exactly is in that “bag full of numbers?” Each time you reach into that bag, you are drawing a number from some distribution based on the values in that bag. The procedure of estimating linear models makes two key assumption about how this process works: Each observation is drawing from the same bag. In other words, you are drawing values from the same distribution. The shape of this distribution does not matter, even though some people will claim that it must be normally distributed. However, regardless of the shape, all obervations must be drawing from an identically shaped distribution. Each draw from the bag must be independent. This means that the value you get on one draw does not depend in any way on other draws. Together these two assumptions are often called the IID assumption which stands for independent and identically distributed. What is the consequence of violating the IID assumption? The good news is that violating this assumption will not bias your estimates of the slope and intercept. The bad news is that violating this assumption will often produce incorrect standard errors on your estimates. In the worst case, you may drastically underestimate standard errors and thus reach incorrect conclusions from classical statistical inference procedures such as hypothesis tests. How do IID violations occur? Lets talk about a couple of common cases. Violations of independence The two most common ways for the independence assumption to be violated are by serial autocorrelation and repeated observations. Serial autocorrelation Serial autocorrelation occurs in longitudinal data, most commonly with time series data. In this case, observations close in time tend to be more similar for reasons not captured by the model and thus you observe a positive correlation between subsequent residuals when they are ordered by time. For example, the longley data that come as an example dataset in base R provide a classic example of serial autocorrelation, as illustrated in Figure 77. Figure 77: Scatterplot of GNP and number of workers unemployed, US 1947-1962 The number unemployed tends to be cyclical and cycles last longer than a year, so we get a an up-down-up pattern show in red that leads to non-independence among the residuals from the linear model (shown in blue). We can more formally look at this by looking at the correlation between temporally-adjacent residuals, although this requires a bit of work to set up. model &lt;- lm(Employed~GNP, data=longley) temp &lt;- augment(model)$.resid temp &lt;- data.frame(earlier=temp[1:15], later=temp[2:16]) head(temp) ## earlier later ## 1 0.33732993 0.26276150 ## 2 0.26276150 -0.64055835 ## 3 -0.64055835 -0.54705800 ## 4 -0.54705800 -0.05522581 ## 5 -0.05522581 -0.26360117 ## 6 -0.26360117 0.44744315 The two values in the first row are the residuals fro 1947 and 1948. The second row contains the residuals for 1948 and 1949, and so on. Notice how the later value becomes the earlier value in the next row. Lets look at how big the serial autocorrelation is: cor(temp$earlier, temp$later) ## [1] 0.1622029 We are definitely seeing some positive serial autocorrelation, but its of a moderate size in this case. Repeated observations Imagine that you have data on fourth grade students at a particular elementary school. You want to look at the relationship between scores on some test and a measure of socioeconomic status (SES). In this case, the students are clustered into different classrooms with different teachers. Any differences between classrooms and teachers that might affect test performance (e.g. quality of instruction, lower stugdent/teacher ratio, aid in class) will show up in the residual of your model. But because students are clustered into these classrooms, their residuals will not be independent. Students from a “good” classroom will tend to have higher than average residuals while students from a “bad” classroom will tend to have below average residuals. The issue here is that our residual terms are clustered by some higher-level identification. This problem most often appears when your data have a multilevel structure in which you are drawing repeated observations from some higher-level unit. Longitudinal or panel data in which the same respondent is interviewed at different time intervals inherently face this problem. In this case, you are drawing repeated observations on the same individual at different points in time and so you expect that if that respondent has a higher than average residual in time 1, they also likely have a higher than average residual in time 2. However, as in the classroom example above, it can occur in a variety of situations even with cross-sectional data. Heteroscedasticity Say what? Yes, this is a real term. Heteroscedasticity means that the variance of your residuals is not constant but depends in some way on the terms in your model. In this case, observations are not drawn from identical distributions because the variance of this distribution is different for different observations. One of the most common ways that heteroscedasticity arises is when the variance of the residuals increases with the predicted value of the dependent variables. This can easily be diagnosed on a residual by fitted value plot, where it shows up as a cone shape as in Figure 78. model &lt;- lm(BoxOffice~Runtime, data=movies) ggplot(augment(model), aes(x=.fitted, y=.resid))+ geom_point(alpha=0.7)+ scale_y_continuous(labels=scales::dollar)+ scale_x_continuous(labels=scales::dollar)+ geom_hline(yintercept = 0, linetype=2)+ theme_bw()+ labs(x=&quot;predicted box office returns&quot;, y=&quot;residuals from the model&quot;) Figure 78: Residual plot of model predicting box office returns by movie runtime Such patterns are common with right-skewed variables that have a lower threshold (often zero). In Figure 78, the residuals can never produce an actual value below zero, leading to the very clear linear pattern on the low end. The high end residuals are more scattered but also seem to show a cone pattern. Its not terribly surprising that as our predicted values grow in absolute terms, the residuals from this predicted value also grow in absolute value. In relative terms, the size of the residuals may be more constant. Fixing IID violations Given that you have identified an IID violation in your model, what can you do about it? The best solution is to get a better model. IID violations are often implicitly telling you that the model you have selected is not the best approach to the actual data-generating process that you have. Lets take the case of heteroscedasticity. If you observe a cone shape to the residuals, it suggests that the absolute size of the residuals is growing with the size of the predicted value of your dependent variable. In relative terms (i.e. as a percentage of the predicted value), the size of those residuals may be more constant. A simple solution would be to log our dependent variable because this would move us from a model of absolute change in the dependent variable to one of relative change. model &lt;- lm(log(BoxOffice)~Runtime, data=movies) ggplot(augment(model), aes(x=.fitted, y=.resid))+ geom_point(alpha=0.7)+ geom_hline(yintercept = 0, linetype=2)+ theme_bw()+ labs(x=&quot;predicted log of box office returns&quot;, y=&quot;residuals from the model&quot;) Figure 79: Residual plot of model predicting the log of box office returns by movie runtime I now no longer observe the cone shape to my residuals. I might be a little concerned about the lack of residuals in the lower right quadrant, but overall this plot does not suggest a strong problem of heteroscedasticity. Furthermore, loggin box office returns may help address issues of non-linearity and outliers and give me results that are more sensible in terms of predicting percent change in box office returns rather than absolute dollars and cents. Similarly, the problem of repeated observations can be addressed by the use of multilevel models that explicitly account for the higher-level units that repeated observations are clustered within. We won’t talk about these models in this course, but in aaddition to resolving the IID problem, these models make possible more interesting and complex models that analyze effects at different levels and across levels. In some cases, a clear and better alternative may not be readily apparent. In this case, two techniques are common. The first is the use of weighted least squares and the second is the use of robust standard errors. Both of these techniques mathematically correct for the IID violation on the existing model. Weighted least squares requires the user to specify exacty how the IID violation arises, while robust standard errors seemingly figures it out mathemagically. Weighted least squares To do a weighted least squares regression model, we need to add a weighting matrix, \\(\\mathbf{W}\\), to the standard matrix calculation of model coefficients and standard errors: \\[\\mathbf{\\beta}=(\\mathbf{X&#39;W^{-1}X})^{-1}\\mathbf{X&#39;W^{-1}y}\\] \\[SE_{\\beta}=\\sqrt{\\sigma^{2}(\\mathbf{X&#39;W^{-1}X})^{-1}}\\] This weighting matrix is an \\(n\\) by \\(n\\) matrix of the residuals. What goes into that weighting matrix depends on what kind of IID violation you are addressing. To deal with heteroscedasticity, you need to apply a weight to the diagonal terms of that matrix which is equal to the inverse of the variance of the residual for that observation. For violation of independence, you need to apply some value to the off-diagonal cells that indicates the correlation between those residuals. In practice, the values of this weighting matrix \\(\\mathbf{W}\\) have to be estimated by the results from a basic OLS regression model. Therefore, in practice, people actually use iteratively-reweighted estimating or IRWLS for short. To run an IRWLS: First estimate an OLS regression model. Use the results from that OLS regression model to determine weights for the weighting matrix \\(\\mathbf{W}\\) and run another model using the formulas above for weighted least squares. Repeat step (2) over and over again until the results stop changing from iteration to iteration. For some types of weighted least squares, R has some built-in packages to handle the details internally. The nlme package can handle IRWLS using the gls function for a variety of weighting structures. For example, we can correct the longley data above by using an “AR1” structure for the autocorrelation. The AR1 pattern (which stands for “auto-regressive 1”) assumes that each subsequent residual is only correlated with its immediate temporal predecessor. library(nlme) model.ar1 &lt;- gls(Employed~GNP+Population, correlation=corAR1(form=~Year),data=longley) summary(model.ar1)$tTable ## Value Std.Error t-value p-value ## (Intercept) 101.85813305 14.1989324 7.173647 7.222049e-06 ## GNP 0.07207088 0.0106057 6.795485 1.271171e-05 ## Population -0.54851350 0.1541297 -3.558778 3.497130e-03 Robust standard errors Robust standard errors have become popular in recent years, partly due (I believe) to how easy they are to add to standard regression models in Stata. We won’t delve into the math behind the robust standard error, but the general idea is that robust standard errors will give you “correct” standard errors even when the model is mis-specified due to issues such as non-linearity, heteroscedasticity, and autocorrelation. Unlike weighted least squares, we don’t have to specify much about the underlying nature of the IID violation. Robust standard errors can be estimated in R using the sandwich and lmtest packages, and specifically with the coeftest command. Within this command, it is possible to specify different types of robust standard errors, but we will use the “HC1” version which is equivalent to the robust standard errors produced in Stata by default. library(sandwich) library(lmtest) model &lt;- lm(BoxOffice~Runtime, data=movies) summary(model)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -101.031612 7.86167078 -12.85116 1.134182e-36 ## Runtime 1.389274 0.07378674 18.82824 3.754966e-74 coeftest(model, vcov=vcovHC(model, &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -101.03161 12.77190 -7.9105 3.786e-15 *** ## Runtime 1.38927 0.12659 10.9745 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the standard error has increased from 0.074 in the OLS regression model to 0.127 for the robust standard error. It is implicitly correcting for the heteroscedasticity problem. Robust standard errors are diagnostics not corrections The problem with robust standard errors is that the “robust” does not necessarily mean “better.” Robust standard errors will still be inefficient because they are implicitly telling you that your model specification is wrong. Intuitively, it would be better to fit the right model with regular standard errors than the wrong model with robust standard errors. From this perspective, robust standard errors can be an effective diagnostic tool but are generally not a good ultimate solution. To illustrate this, lets run the same model as above but now with the log of box office returns which we know will resolve the problem of heteroscedasticity: model &lt;- lm(log(BoxOffice)~Runtime, data=movies) summary(model)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.96019099 0.311641612 -6.289889 3.726077e-10 ## Runtime 0.04025076 0.002924953 13.761164 1.288695e-41 coeftest(model, vcov=vcovHC(model, &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.9601910 0.2897926 -6.7641 1.657e-11 *** ## Runtime 0.0402508 0.0025875 15.5556 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The robust standard error (0.026) is no longer larger than the regular standard error (0.029). This indicates that we have solved the underlying problem leading to inflated robust standard errors by better model specification. Whenever possible, you should try to fix the problem in your model by better model specification rather than rely on the brute-force method of robust standard errors. "],
["sample-design-and-weighting.html", "Sample Design and Weighting", " Sample Design and Weighting Thus far, we have acted as if all of the data we have analyzed comes from a simple random sample , or an SRS for short. For a sample to be a simple random sample, it must exhibit the following property: In a simple random random sample (SRS) of size \\(n\\), every possible combination of observations from the population must have an equally likely chance of being drawn. if every possible combination of observations from the population has an equally likely chance of being drawn, then every observation also has an equally likely chance of being drawn. This latter property is important. If each observation from the population has an equally likely chance to be drawn, then any statistic we draw from the sample should be unbiased - it should approximate the population parameter except for random bias. In simple terms, our sample should be representative of the population. However, SRS’s have an additional criteria above and beyond each observation having an equally likely chance of being drawn. Every possible combination of observations from the population must have an equally likely chance of being drawn. What does this mean? Imagine that I wanted to sample two students from our classroom. First I divide the class into two equally sized groups and then I sample one student from each group. In this case, each student has an equal likelihood of being drawn, specifically \\(2/N\\) where \\(N\\) is the number of students in the class. However, not every possible combination of two students from the class is possible. Students will never be drawn in combination with another student from their own cluster. This additional criteria for an SRS is necessary for the standard errors that we have been using to be accurate for measures of statistical inference such as hypothesis tests and confidence intervals. If this criteria is not true, then we must make adjustments to our standard errors to account for the sample design. These adjustments will typically lead to larger standarde errors. In practice, due to issues of cost and efficiency, most large scale samples are not simple random samples and we must take into account these issues of sample design. In addition, samples sometimes intentionally oversample some groups (typically small groups in the population) and thus statustical results must be weighted in order to calculate unbiased statistics. Cluster/multistage sampling Cluster or multistage sampling is a sampling technique often used to reduce the time and costs of surveying. When surveys are conducted over a large area such as the entire United States, simple random samples are not cost effective. Surveyors would have to travel extensively across every region of the United States to interview respondents chosen for the sample. Therefore, in practice most surveys use some form of multistage sampling, which is a particular form of cluster sampling. The idea of multistage sampling is that observations are first grouped into primary sampling units or PSUs. These PSUs are typically defined geographically. For example, the General Social Survey uses PSUs that are made up of a combination of metropolitan areas and clusters of rural counties. In the first stage of this sampling process, certain PSUs are sampled. Then at the second stage, individual observations are sampled within the PSUs. PSUs of unequal size are typically sampled relative to population size so that each observation still has an equally likely chance of being drawn. In practice, there may be even more than two stages of sampling. For example, one might sample a counties as the PSU and then within sample counties may sample census tracts or blocks, before then sampling households within those tracts. A sample of schoolchildren might first sample school districts, and then schools within the district before sampling individual students. If PSUs are sample with correct probability, then the resulting sample will still be representative because every observation in the population had an equally likely chance of being drawn. However, it will clearly violate the assumption of an SRS. The consequence of cluster sampling is that if the variable of interest is distributed differently across clusters, the resulting standard error for statistical inference will be greater than that calculated in our default formulas. To see why this happens, lets take a fairly straightforward example: what percent of the US population is Mormon? The General Social Survey (GSS) collects information on religious affiliation, so this should be a fairly straightforward calculation. To do this I have extracted a subset of GSS data from 1972-2014 that includes religious affiliation. For reasons that will be explained in more detail in the next section, religious affiliation is divided into three categories and the Mormon affiliation is in the “other” variable. Taking account of some missing value issues, I can calculate the percent mormon by year: relig$mormon &lt;- ifelse(is.na(relig$relig), NA, ifelse(!is.na(relig$other) &amp; relig$other==&quot;mormon&quot;, TRUE, FALSE)) mormons &lt;- tapply(relig$mormon, relig$year, mean, na.rm=TRUE) mormons &lt;- data.frame(year=as.numeric(rownames(mormons)), proportion=mormons, n=as.numeric(table(relig$year))) rownames(mormons) &lt;- NULL head(mormons) ## year proportion n ## 1 1972 0.009299442 1613 ## 2 1973 0.003989362 1504 ## 3 1974 0.005390836 1484 ## 4 1975 0.006711409 1490 ## 5 1976 0.006004003 1499 ## 6 1977 0.007189542 1530 Lets go ahead and plot this proportion up over time. Figure 80: Sample percent Mormon over time from the General Social Survey, with a red band indicating expected 95% confidence intervals under assumption of SRS In addition to the time series, I have included two other features. The dotted line gives the mean proportion across all years (1.19%). The red band gives the expected 95% confidence interval for sampling assuming that the mean across all years is correct and this proportion hasn’t changed over time. For each year this is given by: \\[0.0119 \\pm 1.96(\\sqrt{0.0119(1-0.0119)/n})\\] Where \\(n\\) is the sample size in that year. What should be clear from this graph is that the actual variation in the sample proportion far exceeds that expected by simple random sampling. We would expect 19 out of 20 sample proportions to fall within that red band and most within the red band to fall close to the dotted line, but most values fall either outside the band or at its very edge. Our sample estimate of the percent Mormon fluctuates drastically from year to year. Why do we have such large variation. The GSS uses geographically based cluster sampling and Mormons are not evenly distributed across the US. They are highly clustered in a few states (e.g. Utah, Idaho, Colorado, Nevada) and more commo in the western US than elsewhere. Therefore, in years where a Mormon PSU makes it into the sample we get overestimates of the percent Mormon and in years where a Mormon PSU does not make it into the sample, we get underestimates. Over the long term (assuming no change over time in the population) the average across years should give us an unbiased estimate of around 1.19%. Stratification In a stratified sample, we split our population into various strata by some characteristic and then sample observations from every stratum. For example, I might split households by household income strata of $25K. I would then sample observations from every single one of those income stratum. On the surface, stratified sampling sounds very similar to clustering sample, but it has one important difference. In cluster/multistage sampling, only certain clusters are sampled. In stratified sampling, observations are drawn from all stratum. The goal is not cost savings or efficiency but ensuring adequate coverage in the final dataset for some variable of interest. Stratified sampling also often has the opposite effect as clustered sampling. Greater similarity in some variable of interest within strata will actually reduce standard errors below that expected for simple random sampling, whereas greater similarity in some variable of interest in clusters will increase standard errors. Observations can be drawn from stratum relative to population size in order to maintain the representativeness of the sample. However, researchers often intentionally sample some stratum with a higher frequency than their population size in order to ensure that small groups are present in enough size in the sample that representative statistics can be drawn for that group and comparisons can be made to other groups. For example, it is often common to oversample racial minorities in samples to ensure adequate inferences can be made for smaller groups. This is called oversampling. If certain strata/groups are oversampled, then the sample will no longer be representative without applying some kind of weight. This leads us into a discussion of the issue of weights in sample design. Weights The use of weights in sample design can get quite complex and we will only scratch the surface here. The basic idea is that if your sample is not representative, you need to adjust all statistical results a sampling weight for each observation. The sampling weight for each observation should be equal to the inverse of that observation’s probability of being sampled \\(p_i\\). So: \\[w_i=\\frac{1}{p_i}\\] To illustrate this, lets create a fictional population and draw a sample from it. For this exercise, I want to sample from the Dr. Seuss town of Sneetchville. There are two kinds of Sneetches in Sneetchville, those with stars on their bellies (Star-Bellied Sneetches) and regular Sneetches. Star-bellied Sneetches are relatively more advantaged and make up only 10% of the population. Lets create our populations by sampling incomes from the normal distribution. reg_sneetch &lt;- rnorm(18000, 10, 2) sb_sneetch &lt;- rnorm(2000, 15, 2) sneetchville &lt;- data.frame(type=factor(c(rep(&quot;regular&quot;,18000), rep(&quot;star-bellied&quot;,2000))), income=c(reg_sneetch, sb_sneetch)) mean(sneetchville$income) ## [1] 10.47902 tapply(sneetchville$income, sneetchville$type, mean) ## regular star-bellied ## 9.973621 15.027597 ggplot(sneetchville, aes(x=type, y=income))+ geom_boxplot() Figure 81: Distribution of income in Sneetchville by Sneetch type for the full population We can see that Star-bellied Sneetches make considerably more than regular Sneetches on average. Because there are far more regular Sneetches, the mean income in Sneetchville of 10.5 is very close to the regular Sneetch mean income of 10. Those are the population values which we get to see because we are omnipotent in this case. But our poor researcher needs to draw a sample to figure out these numbers. Lets say the researcher decides to draw a stratified sample that includes 100 of each type of Sneetch. sneetch_sample &lt;- sneetchville[c(sample(1:18000, 100, replace=FALSE), sample(18001:20000, 100, replace=FALSE)),] mean(sneetch_sample$income) ## [1] 12.40761 tapply(sneetch_sample$income, sneetch_sample$type, mean) ## regular star-bellied ## 10.02163 14.79360 ggplot(sneetch_sample, aes(x=type, y=income))+ geom_boxplot() Figure 82: Distribution of income in Sneetchville by Sneetch type for statified sample When I separate mean income by Sneetch type, I get estimates that are pretty close to the population values of 10 and 15. However, when I estimate mean income for the whole sample, I overestimate the population parameter substantially. It should be about 10.5 but I get 12.4. Why? My sample contains equal numbers of Star-bellied and regular Sneetches, but Star-bellied Sneetches are only about 10% of the population. Star-bellied Sneetches are thus over-represented and since they have higher incomes on average, I systematically overestimate income for Sneetchville as a whole. I can correct for this problem if I apply weights that are equal to the inverse of the probability of being drawn. There are 2000 Star-bellied Sneetches in the population and I sampled 100, so the probability of being sampled is 100/2000. Similarly for regular Sneetches the probability is 100/18000. sneetch_sample$sweight &lt;- ifelse(sneetch_sample$type==&quot;regular&quot;, 18000/100, 2000/100) tapply(sneetch_sample$sweight, sneetch_sample$type, mean) ## regular star-bellied ## 180 20 The weights represent how many observations in the population a given observation in the sample represents. I can use these weights to get a weighted mean. A weighted mean can be calculated as: \\[\\frac{\\sum w_i*x_i}{\\sum w_i}\\] sum(sneetch_sample$sweight*sneetch_sample$income)/sum(sneetch_sample$sweight) ## [1] 10.49883 weighted.mean(sneetch_sample$income, sneetch_sample$sweight) ## [1] 10.49883 Our weighted sample mean is now very close to the population mean of 10.5. So why add this complication to my sample? Lets say I want to do a hypothesis test on the mean income difference between Star-bellied Sneetches and regular Sneetches in my sample. Lets calculate the standard error for that test: sqrt(sum((tapply(sneetch_sample$income, sneetch_sample$type, sd))^2/ table(sneetch_sample$type))) ## [1] 0.2974474 Now instead of a stratified sample, lets draw a simple random sample of the same size (200) and calculate the same standard error. sneetch_srs &lt;- sneetchville[sample(1:20000,200, replace = FALSE),] sqrt(sum((tapply(sneetch_srs$income, sneetch_srs$type, sd))^2/ table(sneetch_srs$type))) ## [1] 0.5086267 The standard errors on the mean difference are nearly doubled. Why? Remember that the sample size of each group is a factor in the standard error calculation. In this SRS we had the following: table(sneetch_srs$type) ## ## regular star-bellied ## 182 18 We had far fewer Star-bellied Sneetches because we did not oversample them and this led to higher standard errors on any sort of comparison we try to draw between the two groups. Post-stratification weighting In some cases where oversamples are drawn for particular cases and population numbers are well known, it may be possible to calculate simple sample weights as we have done above. However, in many cases, researchers are less certain about how the sampling procedure itself might have under-represented or over-represented some groups. In these cases, it is common to construct post-stratification weights by comparing sample counts across a variety of strata to that from some trusted population source and then constructing weights that adjust for discrepancies. These kinds of calculations can be quite complex and we won’t go over them in detail here. Weighting and standard errors Regardless of how sample weights are constructed, they also have an effect on the variance in sampling. The use of sampling weights will increase the standard error of estimates above and beyond what we would expect in simple random sampling and this must be accounted for in statistical inference techniques. The greater the variation in sample weights, the bigger the design effect will be on standard errors. Correcting for sample design in models Clustering, stratification and weighting can all effect the representativeness and sampling variability of our sample. In samples that are not simple random samples, we need to take account of these design effects in our models. To illustrate such design effects, we will consider the Add Health data that we have been using. This data is the result of a very complex sampling design. Schools were the PSU in a multistage sampling design. The school sample and the individual sample of students within schools were also stratified by a variety of characteristics. For example, the school sample was stratified by region of the US to ensure that all regions were represented. Certain ethnic/racial groups were also oversampled by design and post-stratification weights were also calculated. In the publically available Add Health data, we do not have information on stratification variables, but we do have measures of clustering and weighting. head(popularity[,c(&quot;cluster&quot;,&quot;sweight&quot;)]) ## cluster sweight ## 1 472 3663.6562 ## 2 472 4471.7647 ## 3 142 3325.3294 ## 4 142 271.3590 ## 5 145 3939.9402 ## 6 145 759.0284 The cluster variable is a numeric id for each school and tells us which students were sampled from which PSU. The sweight variable is our sampling weight variable and tells us how many students in the US this observation represents. Lets start with a naive model that ignores all of these design effects and just tries to predict number of friend nominations by our pseudoGPA measure: model_basic &lt;- lm(indegree~pseudoGPA, data=popularity) round(summary(model_basic)$coef, 3) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.181 0.217 10.063 0 ## pseudoGPA 0.837 0.074 11.304 0 This model is not necessarily representative of the population because it does not take account of the differential weight for each student. We can correct these estimates by adding in a weight argument to the lm command. This will essentially run a weighted least squares (WLS) model with these weights along the diagonal of the weight matrix. Intuitively, we are trying to minimize the weighted sum of squared residuals rather than the unweighted sum of squared residuals. model_weighted &lt;- lm(indegree~pseudoGPA, data=popularity, weights = sweight) round(summary(model_weighted)$coef, 3) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.267 0.220 10.312 0 ## pseudoGPA 0.826 0.075 11.009 0 In this case, the unweighted slope of 0.837 is not very differnet from the weighted slope of 0.826. This model give us more representative and generalizable results, but we are still not taking account of the design effects that increase the standard errors of our estimates. Therefore, we are underestimating standard errors in the current model. There are two sources of these design effects that we need to account for: The variance in sample weights The cluster sampling We can address the first issue of variance in sample weights by using robust standard errors: library(sandwich) library(lmtest) model_robust &lt;- coeftest(model_weighted, vcov=vcovHC(model_weighted, &quot;HC1&quot;)) model_robust ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.266634 0.268556 8.4401 &lt; 2.2e-16 *** ## pseudoGPA 0.826424 0.094991 8.7001 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the standard error increased from about 0.075 to 0.095. Robust standard errors will adequately address the design effect resulting from the variance of the sampling weights, but it does not address the design effect arising from clustering. Doing that is more mathematically complicated. We won’t go into the details here, but we will use the survey library in R that can account for a variety of design effects. To make use of this library, we first need to create a svydesign object that identifies the sampling design effects: library(survey) popularity_svy &lt;- svydesign(ids=~cluster, weight=~sweight, data=popularity) The survey library comes with a variety of functions that can be used to run a variety of models that expect this svydesign object rathre than a traditional data.frame. In this case, we want the svyglm command which can ge used to run “generalized linear models” which include our basic linear model. model_svy &lt;- svyglm(indegree~pseudoGPA, design=popularity_svy) round(summary(model_svy)$coef, 3) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.267 0.322 7.030 0 ## pseudoGPA 0.826 0.109 7.593 0 We now get standard errors of about 0.109 which should reflect both the sampling weight heterogeneity and the cluster sampling design effects. Note that our point estimate of the slope is so large that even with this enlarged standard error, we are pretty confident the result is not zero. Table 18: Linear models predicting number of friend nominations by GPA unweighted weighted robust SE survey: weights survey: weights+cluster Intercept 2.181*** 2.267*** 2.267*** 2.267*** 2.267*** (0.217) (0.220) (0.269) (0.269) (0.322) GPA 0.837*** 0.826*** 0.826*** 0.826*** 0.826*** (0.074) (0.075) (0.095) (0.095) (0.109) R2 0.028 0.027 Num. obs. 4397 4397 4397 4397 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 Table 18 shows all of the models estimated above plus an additional model that uses the survey library but only adjust for weights. All of the techniques except for the unweighted linear model produce identical and properly weighted estimates of the slope and intercept. They vary in the standard errors. The robust standard error approach produces results identical to the survey library method that only adjusts for weights. Only the final model using the survey library for weights and clustering addresses all of the design effects. "],
["missing-values.html", "Missing Values", " Missing Values Up until this point, we have analyzed data that had no missing values. In actuality, the data did have missing values, but I used an imputation technique to remove the missing values to simplify our analyses. However, we now need to think more clearly about the consequences of missing values in datasets, because the reality is that most datasets that you want to use will have some missing data. Howe you handle this missing data can have important implications for your results. Identifying Valid Skips A missing value occurs when a value is missing for an observation on a particular value. Its important to distinguish in your data between situations in which a missing value is actually a valid response, from situations in which it is invalid and must be addressed. It may seem unusual to think of some missing values as valid, but based on the structure of survey questionnaires is not unusual to run into valid missing values. In particular, valid skips are a common form of missing data. Valid skips occur when only a subset of respondents are asked a question that is a follow-up to a previous question. Typically to be asked the follow up question, respondents have to respond to the original question in a certain way. All respondents who weren’t asked the follow up question then have valid skips for their response. As an example, lets look at how the General Social Survey asks about a person’s religious affiliation. The GSS asks this using a set of three questions. First, all respondents are asked “What is your religious preference? Is it Protestant, Catholic, Jewish, some other religion, or no religion?” After 1996, the GSS also started recording more specific responses for the “some other religion” category, but for simplicity lets just look at the distribution of this variable in 1996. table(gss$relig, exclude=NULL) ## ## catholic jewish none other protestant &lt;NA&gt; ## 685 68 339 143 1664 5 On this variable, we have five cases of item nonresponse in which the respondent did not answer the question. They might have refused to answer it or perhaps they just did not know the answer. For most of the respondents, this is the only question on religious affiliation. However, those who responded as Protestant are then asked further questions to determine more information about their particular denomination. This denom question includes prompts for most of the large denominations in the US. Here is the distribution of that variable. table(gss$denom, exclude=NULL) ## ## afr meth ep zion afr meth episcopal am bapt ch in usa ## 8 11 28 ## am baptist asso am lutheran baptist-dk which ## 57 51 181 ## episcopal evangelical luth luth ch in america ## 79 33 15 ## lutheran-dk which lutheran-mo synod methodist-dk which ## 25 48 23 ## nat bapt conv of am nat bapt conv usa no denomination ## 12 8 99 ## other other baptists other lutheran ## 304 69 15 ## other methodist other presbyterian presbyterian c in us ## 12 19 34 ## presbyterian-dk wh presbyterian, merged southern baptist ## 11 13 273 ## united methodist united pres ch in us wi evan luth synod ## 190 29 11 ## &lt;NA&gt; ## 1246 Notice that we seem to have a lot of missing values here (1246). Did a lot of people just not know their denomination? No, in fact there is very little item nonresponse on this measure. The issue is that all of the people who were not asked this question because they didn’t respond as Protestant are listed as missing values here, but they are actually valid skips. We can see this more easily by crosstabbing the two variables based on whether the response was NA or not. table(gss$relig, is.na(gss$denom), exclude=NULL) ## ## FALSE TRUE ## catholic 0 685 ## jewish 0 68 ## none 0 339 ## other 0 143 ## protestant 1658 6 ## &lt;NA&gt; 0 5 You can see that almost all of the cases of NA here are respondents who did not identify as Protestants and therefore were not asked the denom question, i.e. valid skips. We have only six cases of individuals who were asked the denom question and did not respond. For Protestants who did not identify with one of the large denominations prompted in denom were identified as “other”. For this group, a third question (called other in the GSS) was asked in which the respondent’s could provide any response. I won’t show the full table here as the number of denominations is quite large, but we can use the same logic as above to identify all of the valid skips vs. item nonresponses. table(gss$denom, is.na(gss$other), exclude=NULL) ## ## FALSE TRUE ## afr meth ep zion 0 8 ## afr meth episcopal 0 11 ## am bapt ch in usa 0 28 ## am baptist asso 0 57 ## am lutheran 0 51 ## baptist-dk which 0 181 ## episcopal 0 79 ## evangelical luth 0 33 ## luth ch in america 0 15 ## lutheran-dk which 0 25 ## lutheran-mo synod 0 48 ## methodist-dk which 0 23 ## nat bapt conv of am 0 12 ## nat bapt conv usa 0 8 ## no denomination 0 99 ## other 300 4 ## other baptists 0 69 ## other lutheran 0 15 ## other methodist 0 12 ## other presbyterian 0 19 ## presbyterian c in us 0 34 ## presbyterian-dk wh 0 11 ## presbyterian, merged 0 13 ## southern baptist 0 273 ## united methodist 0 190 ## united pres ch in us 0 29 ## wi evan luth synod 0 11 ## &lt;NA&gt; 0 1246 We can see here that of the individuals who identified with an “other” Protestant denomination, only four of those respondents did not provide a write-in response. Valid skips are generally not a problem so long as we take account of them when we construct our final variable. In this example, we ultimately would want to use these three questions to create a parsimonious religious affiliation variable, perhaps by separating Protestants into evangelical and “mainline” denominations. In that case, the only item nonresponses will be the five cases for relig, the six cases for denom, and the four cases for other for a total of 15 real missing values. Kinds of Missingness In statistics, missingness is actually a word. It describes how we think values came to be missing from the data. In particular, we are concerned with whether missingness might be related to other variables that are related to the things we want to study. In this case, our results might be biased as a result of missing values. For example, income is a variable that often has lots of missing data. If missingness on income is related to education, gender, or race, then we might be concerned about how representative our results are in understanding the process. The biases that results from relationships between missingness and other variables can be hard to understand and therefore particularly challenging. Missingness is generally considered as one of three types. The best case scenario is that the data are missing completely at random (MCAR). A variable is MCAR if every observation has the same probability of missingness. In other words, the missingness of a variable has no relationship to other observed or unobserved variables. If this is true, then removing observations with missing values will not bias results. However, it pretty rare that MCAR is a reasonable assumption for missingness. Perhaps if you randomly spill coffee on a page of your data, you might have MCAR. A somewhat more realistic assumption is that the data are **missing at random* (MAR). Its totally obvious how this is different, right? A variable is MAR if the different probabilities of missingness can be fully accounted for by other observed variables in the dataset. If this is true, then various techniques can be used to produce unbiased results by imputing values for the missing values. A more honest assessment is that the data are not missing at random (NMAR). A variable is NMAR if the different probabilities of missingness depend both on observed and unobserved variables. For example, some respondents may not provide income data because they are just naturally more suspicious. This variation in suspiciousness among respondents is not likely to be observed directly even though it may be partially correlated with other observed variables. The key issue here is the unobserved nature of some of the variables that produce missingness. Because they are unobserved, I have no way to account for them in my corrections. Therefore, I can never be certain that bias from missing values has been removed from my results. In practice, although NMAR is probably the norm in most datasets, the number of missing values on a single variable may be small enough that even the incorrect assumption of MAR or MCAR might have little effect on the results. In my previous example, I had only 16 missing values on religious affiliation for 2904 respondents. So, its not likely to have a large effect on the results no matter how I deal with the missing values. How exactly does one deal with those missing values? The data analyst typically has two choices. You can either remove cases with missing values or you can do some form of imputation. These two techniques are tied to the assumptions above. Removing cases is equivalent to assuming MCAR while imputation is equivalent to MAR. To illustrate these different approaches below, we will look at a new example. We are going to examine the Add Health data again, but this time I am going to use a dataset that does not have imputed values for missing values. In particular, we are going to look at the relationship between parental income and popularity. Lets look at how many missing values we have for income. summary(addhealth$parentinc) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 23.00 40.00 45.37 60.00 200.00 1027 Yikes! We are missing 1027 cases of parental income. That seems like a lot. Lets go ahead and calculate the percent of missing cases for all of the variables in our data. temp &lt;- round(apply(is.na(addhealth), 2, mean)*100, 2) temp &lt;- data.frame(variable=names(temp), percent_missing=temp) rownames(temp) &lt;- NULL pander(as.data.frame(temp)) variable percent_missing indegree 0 race 0.05 sex 0 grade 1.18 pseudoGPA 1.91 honorsociety 0 alcoholuse 0.66 smoker 0.93 bandchoir 0 academicclub 0 nsports 0 parentinc 23.36 cluster 0 sweight 0 For most variables, we only have a very small number of missing values, typically less than 1% of cases. however, for parental income we are missing values for nearly a quarter of respondents. Removing Cases The simplest approach to dealing with missing values is to just drop observations (cases) that have missing values. In R, this is what will either happen by default or if you give a function the argument na.rm=TRUE. For example, lets calculate the mean income in the Add Health data: mean(addhealth$parentinc) ## [1] NA If we don’t specify how R should handle the missing values, R will report back a missing value for many basic calculations like the mean, median, and standard deviation. In order to get the mean for observations with valid values of income, we need to include the na.rm=TRUE option: mean(addhealth$parentinc, na.rm=TRUE) ## [1] 45.37389 The lm command in R behaves a little differently. It will automatically drop any observations that are missing values on any of the variables in the model. Lets look at this for a model predicting popularity by parental income and number of sports played: model &lt;- lm(indegree~parentinc+nsports, data=addhealth) summary(model) ## ## Call: ## lm(formula = indegree ~ parentinc + nsports, data = addhealth) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.7920 -2.6738 -0.7722 1.8585 22.2967 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.526054 0.114310 30.846 &lt; 2e-16 *** ## parentinc 0.012309 0.001884 6.534 7.39e-11 *** ## nsports 0.451061 0.048920 9.220 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.652 on 3367 degrees of freedom ## (1027 observations deleted due to missingness) ## Multiple R-squared: 0.04164, Adjusted R-squared: 0.04107 ## F-statistic: 73.14 on 2 and 3367 DF, p-value: &lt; 2.2e-16 Note the comment in parenthesis at the bottom that tells us we lost 1027 observations due to missingness. This approach is easy to implement but comes at a significant cost to the actual analysis. First, removing cases is equivalent to assuming MCAR, which is a strong assumption. Second, since most cases that you will remove typically have valid values on most of their variables, you are throwing away a lot of valid information as well as reducing your sample size. For example, all 1027 observations that were dropped in the model above due to missing values on parental income had valid responses the variable on number of sports played. If you are using a lot of variables in your analysis, then even with moderate amounts of missingness, you may reduce your sample size substantially. In situations where only a very small number of values are missing, then removing cases may be a reasonable strategy for reasons of practicality. For example, in the Add Health data, the race variable has missing values on two observations. Regardless of whether the assumption of MCAR is violated or not, the removal of two observations in a sample of 4,397 is highly unlikely to make much difference. Rather, than use more complicated techniques that are labor intensive, it probably makes much more sense remove them. Two strategies are available when you remove cases. You can either do available-case analysis or complete-case analysis. Both strategies refer to how you treat missing values in an analysis which will include multiple measurements that may use different numbers of variables. The most obvious and frequent case here is when you run multiple nested models in which you include different number of variables. For example, lets say we want to predict a student’s popularity by in a series of nested models: - Model 1: predict popularity by number of sports played - Model 2: add smoking and drinking behavior as predictors model 1 - Model 3: add parental income as a predictor to model 2 In this case, my most complex model is model 3 and it includes five variables: popularity, number of sports played, smoking behavior, drinking behavior, and parental income. I need to consider carefully how I am going to go about removing cases. Available-case analysis In available-case analysis (also called pairwise deletion) observations are only removed for each particular component of the analysis (e.g. a model) in which they are used. This is what R will do by default when you run nested models because it will only remove cases if a variable in that particular model is missing. model1.avail &lt;- lm(indegree~nsports, data=addhealth) model2.avail &lt;- update(model1.avail, .~.+alcoholuse+smoker) model3.avail &lt;- update(model2.avail, .~.+parentinc) As a sidenote, I am introducing a new function here called update that can be very useful for model building. The update function can be used on a model object to make some changes to it and then re-run it. In this case, I am changing the model formula, but you could also use it to change the data (useful for looking at subsets), add weights, etc. When you want to update the model formula, the syntax .~.+ will include all of the previous elements of the formula and then you can make additions. I am using it to build nested models without having to repeat the entire model syntax every time. Table 19: Models predicting number of friend nominations in Add Health data, using available-case analysis Model 1 Model 2 Model 3 (Intercept) 3.997*** 3.851*** 3.391*** (0.072) (0.079) (0.120) nsports 0.504*** 0.508*** 0.454*** (0.043) (0.043) (0.049) alcoholuseDrinker 0.707*** 0.663*** (0.155) (0.180) smokerSmoker 0.194 0.383* (0.162) (0.188) parentinc 0.012*** (0.002) R2 0.031 0.037 0.048 Num. obs. 4397 4343 3332 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Table 19 shows the results of the nested models in table form. Pay particular attention to the change in the number of observations in each model. In the first model, there are no missing values on nsports or indegree so we have the full sample size of 4397. In model 2, we are missing a few values on drinking and smoking behavior, so the sample size drops to 4343. In Model 3, we are missing a large number of observations, so the sample size drops to 3332. We lose nearly a quarter of the data from Model 2 to Model 3. The changing sample sizes across models make model comparisons extremely difficult. We don’t know if changes from model to model are driven by different combinations of independent variables or changes in sample size. Note for example that the effect of smoking on popularity almost doubles from Model 2 to Model 3. Was that change driven by the fact that we controlled for parental income or was it driven by the exclusion of nearly a quarter of the sample. We have no way of knowing. For this reason, available-case analysis is generally not the best way to approach your analysis. You don’t want your sample sizes to be changing across models. There are a few special cases (I will discuss one in the next section) where it may be acceptable and sometimes it is just easier in the early exploratory phase of a project, but in general you should use complete-case analysis. Complete-case analysis In complete-case analysis (also called listwise deletion) you remove observations that are missing on any variables that you will use in the analysis even for some calculations that may not involve those variables. This techniques ensures that you are always working with the same sample throughout your analysis so any comparisons (e.g. across models) are not comparing apples and oranges. In our example, I need to restrict my sample to only observations that have valid responses for all five of the variables that will be used in my most complex model. I can do this easily with the na.omit command which will remove any observation that is missing any value in a given dataset: addhealth.complete &lt;- na.omit(addhealth[,c(&quot;indegree&quot;,&quot;nsports&quot;,&quot;alcoholuse&quot;,&quot;smoker&quot;,&quot;parentinc&quot;)]) model1.complete &lt;- lm(indegree~nsports, data=addhealth.complete) model2.complete &lt;- update(model1.complete, .~.+alcoholuse+smoker) model3.complete &lt;- update(model2.complete, .~.+parentinc) Table 20: Models predicting number of friend nominations in Add Health data, using complete-case analysis Model 1 Model 2 Model 3 (Intercept) 4.051*** 3.876*** 3.391*** (0.085) (0.091) (0.120) nsports 0.490*** 0.494*** 0.454*** (0.049) (0.049) (0.049) alcoholuseDrinker 0.717*** 0.663*** (0.181) (0.180) smokerSmoker 0.372* 0.383* (0.189) (0.188) parentinc 0.012*** (0.002) R2 0.029 0.037 0.048 Num. obs. 3332 3332 3332 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Table 20 shows the results for the models using complete-case analysis. Note that the sample size for each model is the same and is equivalent to that for the most complex model in the available-case analysis above. It is now apparent that the change in the effect of smoking on popularity above was driven by the removal of cases and not the controls for parental income. I can see this because now that I have performed the same sample restriction on Model 2, I get a similar effect size for smoking. Its worth noting that this is not really a great thing, because it suggests that by dropping cases, I am biasing the sample in some way, but at least I can make consistent comparisons across models. When only a small number of cases are missing, then complete-case analysis is a reasonable option that is practical and not labor intensive. For example, I wouldn’t worry too much about the 54 cases lost due to the inclusion of smoking and drinking behavior in the models above. However, when you are missing substantial numbers of cases, either from a single variable or from the cumulative effects of dropping a moderate number of observations for lots of variables, then both case removal techniques are suspect. In those cases, it is almost certainly better to move to an imputation strategy. Imputation Imputing a “fake” value for a missing value may seem worse than just removing it, but in many cases imputation is a better alternative than removing cases. First, imputation allows us to preserve the original sample size as well as all of the non-missing data that would be thrown way for cases with some missing values. Second, depending on methodology, imputation may move from a MCAR assumption to a MAR assumption. There are a variety of imputation procedures, but they can generally be divided up along two dimensions. First, predictive imputations use information about other variables in the dataset to impute missing values, while non-predictive imputations do not. In most cases, predictive imputation uses some form of model structure to predict missing values. Predictive imputation moves the assumption on missingness from MCAR to MAR, but is more complex (and thus prone to its own problems of model mis-specification) and more labor-intensive. Second, random imputations include some randomness in the imputation procedure, while deterministic imputations do not include randomness. Including randomness is necessary in order to preserve the actual variance in the variable that is being imputed. Deterministic imputations will always decrease this variance and thus lead to under-estimation of inferential statistics. However, random imputation identify an additional issue because random imputations will produce slightly different results each time the imputation procedure is conducted. To be accurate, the researcher must account for this additional imputation variabilty in their results. In general, the gold standard for imputation is called multiple imputation which is a predictive imputation that include randomness and adjusts for imputation variability. However, this technique is also complex and labor-intensive, so it may not be the best practical choice when the number of missing values is small. Below, I outline several of these different approaches, from simplest to most complex. Non-predictive imputation The simplest form of imputation is to just replace missing values with a single value. Since the mean is considered the “best guess” for the non-missing cases, it is customary to impute the mean value here and so this technique is generally called mean imputation. We can perform mean imputation in R simply by using the bracket-and-boolean approach to identify and replace missing values: addhealth$parentinc.meani &lt;- addhealth$parentinc addhealth$incmiss &lt;- is.na(addhealth$parentinc) addhealth$parentinc.meani[addhealth$incmiss] &lt;- mean(addhealth$parentinc, na.rm=TRUE) I create a separate variable called parentinc.meani for the mean imputation so that I can compare it to other imputation procedures later. I also create an incmiss variable that is just a boolean indicating whether income is missing for a given respondent. Figure 83: The effect of mean imputation of parental income on the scatterplot between parental income and number of friend nominations Figure 83 shows the effect of mean imputation on the scatterplot of the relationship between parental income and number of friend nominations. All of the imputed values fall along a vertical line that corresponds to the mean parental income for the non-missing values. The effect on the variance of parental income is clear. Because this imputation procedure fits a single value, its imputation does not match the spread of observations along parental income. We can also see this by comparing the standard deviations of the original variable and the mean imputed one: sd(addhealth$parentinc, na.rm=TRUE) ## [1] 33.6985 sd(addhealth$parentinc.meani) ## [1] 29.50069 Since the variance of the independent variable is a component of the standard error calculation (see the first section of this chapter for details), underestimating the variance will lead to underestimates of the standard error in the linear model. A similar method that will account for the variance in the independent variable is to just randomly sample values of parental income from the valid responses. I can do this in R using the sample function. addhealth$parentinc.randi &lt;- addhealth$parentinc addhealth$parentinc.randi[addhealth$incmiss]&lt;-sample(addhealth$parentinc[!addhealth$incmiss], sum(addhealth$incmiss), replace = TRUE) I create a new variable for this imputation. The sample function will sample from a given data vector (in this case the valid responses to parental income) a specified number of times (in this case the number of missing values). I also specify that I want to sample with replacement, which means that I replace values that I have already sampled before drawing a new value. Figure 84: The effect of random imputation of parental income on the scatterplot between parental income and number of friend nominations Figure 84 shows the same scatterplot as above, but this time with random imputation. You can see that my imputed values are now spread out across the range of parental income, thus preserving the variance in that variable. Although random imputation is preferable to mean imputation, both methods have a serious drawback. Because I am imputing values without respect to any other variables, these imputed values will by definition be uncorrelated with the dependent variable. You can see this in figure 84 where I have drawn the best fitting line for the imputed and non-imputed values. The red line for the imputed values is basically flat because there will be no association except for that produced by sampling variability. Because imputed values are determined without any prediction, non-predictive imputation will always bias estimates of association between variables downward. Table 21: Comparison of measures of assocation between parental income and number of friend nominations and variance of parental income in Add Healt data, using different imputation techniques. sample correlation slope sd Valid cases 0.132 0.0146 33.7 Valid cases + mean imputed 0.117 0.0146 29.5 Valid cases + random imputed 0.105 0.0114 33.9 Table 21 compares measures of association and variance in the Add Health data for these two techniques to the same numbers when only valid cases are used. The reduction in correlation in the two imputed samples is apparent, relative to the case with only valid responses. The reduction in variance of parental income is also clear for the case of mean imputation. Interestingly, the mean imputation does not downwardly bias the measure of slope because the reduction in correlation is perfectly offset by the reduction in variance. However, this special feature only holds when there are no other independent variables in the model and is also probably best thought of as a case of two wrongs (errors) not making a right (correct measurement). In general, because of these issues with downward bias, non-predictive imputation is not and advisable technique. However, in some cases it may be preferable as a “quick and dirty” method that is useful for initial exploratory analysis or because the number of missing cases is moderate. It may also be reasonable if the variable that needs to be imputed is intended to serve primarily as a control variable and inferences are not necessarily going to be drawn for this variable itself. In these cases where non-predictive imputation serves as a quick and dirty method, one additional technique, which I call mean imputation with dummy is advisable. In this case, the researcher performs mean imputation as above but also includes the missingness boolean variable into the model specification as above. The effect of this dummy is to estimate a slope and intercept for the imputed variable that is unaffected by the imputation, while at the same time producing a measure of how far out mean imputation is off relative to where the model expects the missing cases to fall. Since, we already have an incmiss dummy variable, we can implement this model very easily. Lets try it in a model that predicts number of friend nominations by parental income and number of sports played. model.valid &lt;- lm(indegree~parentinc+nsports, data=addhealth) model.meani &lt;- lm(indegree~parentinc.meani+nsports, data=addhealth) model.mdummyi &lt;- lm(indegree~parentinc.meani+nsports+incmiss, data=addhealth) Table 22: Models predicting number of friend nominations in Add Health data, using different methods of imputation No imputation Mean imputation Mean imputation w/dummy Parent income 0.01231*** (0.00188) Parent income, mean imputed 0.01220*** 0.01221*** (0.00186) (0.00186) Number of sports played 0.45106*** 0.47134*** 0.46945*** (0.04892) (0.04292) (0.04297) Income missing dummy -0.12163 (0.12911) Intercept 3.52605*** 3.47950*** 3.50954*** (0.11431) (0.10678) (0.11144) R2 0.04164 0.03999 0.04018 Num. obs. 3370 4397 4397 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Table 22 shows the results of these models. In this case, the results are very similar. The coefficient on the income missing dummy basically tells us how far the average number of friend nominations was off for cases with income missing relative to what was predicted by the model. In this case, the model is telling us that with a mean imputation the predicted value of friend nominations for respondents with missing income data is 0.12 nominations higher than where it is in actuality. If we believe our model, this might suggest that the parental income of those with missing income is somewhat lower than those who are not missing, on average. This quick and dirty method is often useful in practice, but keep in mind that it still has the same problems of underestimating variance and assuming MCAR. A far better solution would be to move to predictive imputation. Predictive imputation In a predictive imputation, we use other variables in the dataset to get predicted values for cases that are missing. In order to do this, an analyst must specify some sort of model to determine predicted values. A wide variety of models have been developed for this, but for this course, we will only discuss the simple case of using a linear model to predict missing values (sometimes called regression imputation). In the case of the Add Health data, we have a variety of other variables that we can consider using to predict parental income including race, GPA, alcohol use, smoking, honor society membership, band/choir membership, academic club participation, and number of sports played. We also could consider using the dependent variable (number of friend nominations) here but there is some debate on whether dependent variables should be used to predict missing values, so we will not consider it here. There is really no reason not to use as much information as possible here, so I will use all of these variables. However, in order to model this correctly, I want to consider the skewed nature of the my parental income data. To reduce the skewness and produce better predictions, I will transform parental income in my predictive model by square rooting it. I can then fit a model predicting parental income: addhealth$parentinc.regi &lt;- addhealth$parentinc model &lt;- lm(sqrt(parentinc)~race+pseudoGPA+honorsociety+alcoholuse+smoker +bandchoir+academicclub+nsports, data=addhealth) predicted &lt;- predict(model, addhealth) addhealth$parentinc.regi[addhealth$incmiss] &lt;- predicted[addhealth$incmiss]^2 summary(addhealth$parentinc.regi) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.00 25.19 39.83 43.87 54.20 200.00 39 After fitting the model, I just use the predict command to get predicted parental income values (square rooted) for all of my observations. I remember to square these predicted values when I impute them back. One important thing to note is that my imputed parental income still has 39 missing values. That is a lot less than 1027, but how come we didn’t remove all the missing values? The issue here is that some of my predictor variables (particularly smoking, alcohol use, and GPA) also have missing values. Therefore, for observations that are missing on those values as well as parental income, I will get missing values in my prediction. I will address how to handle this problem further below. Figure 85: The relationship between predicted values of imputed parental income and actual or imputed values, using regression imputation This regression imputation only accounts for variability in parental income that is accountable for in the model and ignores residual variation. This can be clearly seen in Figure 85 where the actual values of parental income are widely dispersed around the predicted values, except for cases where missing values were imputed. This imputation procedure therefore underestimates the total variance in parental income with similar consequences as above. To address this problem, we can use random regression imputation which basically adds a random “tail” onto each predicted value based on the variance of our residual terms. It is also necessary to define a distribution from which to sample our random tails. We will use the normal distribution. addhealth$parentinc.rregi &lt;- addhealth$parentinc model &lt;- lm(sqrt(parentinc)~race+pseudoGPA+honorsociety+alcoholuse+smoker +bandchoir+academicclub+nsports, data=addhealth) predicted &lt;- predict(model, addhealth) addhealth$parentinc.rregi[addhealth$incmiss] &lt;- (predicted[addhealth$incmiss]+ rnorm(sum(addhealth$incmiss), 0, sigma(model)))^2 The rnorm function samples from a normal distribution with a given mean and standard deviation. We use the sigma function to pull the standard deviation of the residuals from our model. Figure 86: The relationship between predicted values of imputed parental income and actual or imputed values, using random regression imputation Figure 86 shows that the variation of our imputed values now mimics that of the actual values. We can also compare the standard deviations across imputations to see that we have preserved the variation in our valid data: sd(addhealth$parentinc, na.rm=TRUE) ## [1] 33.6985 sd(addhealth$parentinc.regi, na.rm=TRUE) ## [1] 30.10024 sd(addhealth$parentinc.rregi, na.rm=TRUE) ## [1] 32.99728 Chained equations As we saw above, one downside of using a model to predict missing values is that missing values on other variables in the dataset can cause the imputation procedure to not impute all of the missing values. There are a few different ways of handling this but the most popular technique is to use an iterative procedure called **multiple imputation by chained equations* or MICE for short. The basic procedure is as follows: All missing values are given some placeholder value. This might be the mean value, for example. For one variable, the placeholder values are removed and missing values put back in. These missing values are then predicted and imputed by some model for this variable. Step 2 is repeated for all variables with missing values. When all variables have been imputed, we have completed one iteration. Steps 2-3 are then repeated again for some number of iterations. The number of iterations necessary may vary by data, but five iterations is typical. The purpose of the iterations is to improve the imputation by moving further and further away from the mean imputation that was used to get an initial fit. This procedure is complicated to implement in practice, but luckily the mice library in R already provides an excellent implementation. Furthermore, the mice library has a variety of modeling options that include the ability to handle missing values in both quantitative and categorical models. For quantitative variables, it does not by default use regression imputation, but rather a technique called predictive mean matching that is more flexible and resistant to mis-specification. The basic command in the mice library to run MICE is … mice: library(mice) addhealth.complete &lt;- mice(addhealth[,c(&quot;indegree&quot;,&quot;race&quot;,&quot;sex&quot;,&quot;grade&quot;,&quot;pseudoGPA&quot;, &quot;honorsociety&quot;,&quot;alcoholuse&quot;,&quot;smoker&quot;,&quot;bandchoir&quot;, &quot;academicclub&quot;,&quot;nsports&quot;,&quot;parentinc&quot;)], m=5) ## ## iter imp variable ## 1 1 race grade pseudoGPA alcoholuse smoker parentinc ## 1 2 race grade pseudoGPA alcoholuse smoker parentinc ## 1 3 race grade pseudoGPA alcoholuse smoker parentinc ## 1 4 race grade pseudoGPA alcoholuse smoker parentinc ## 1 5 race grade pseudoGPA alcoholuse smoker parentinc ## 2 1 race grade pseudoGPA alcoholuse smoker parentinc ## 2 2 race grade pseudoGPA alcoholuse smoker parentinc ## 2 3 race grade pseudoGPA alcoholuse smoker parentinc ## 2 4 race grade pseudoGPA alcoholuse smoker parentinc ## 2 5 race grade pseudoGPA alcoholuse smoker parentinc ## 3 1 race grade pseudoGPA alcoholuse smoker parentinc ## 3 2 race grade pseudoGPA alcoholuse smoker parentinc ## 3 3 race grade pseudoGPA alcoholuse smoker parentinc ## 3 4 race grade pseudoGPA alcoholuse smoker parentinc ## 3 5 race grade pseudoGPA alcoholuse smoker parentinc ## 4 1 race grade pseudoGPA alcoholuse smoker parentinc ## 4 2 race grade pseudoGPA alcoholuse smoker parentinc ## 4 3 race grade pseudoGPA alcoholuse smoker parentinc ## 4 4 race grade pseudoGPA alcoholuse smoker parentinc ## 4 5 race grade pseudoGPA alcoholuse smoker parentinc ## 5 1 race grade pseudoGPA alcoholuse smoker parentinc ## 5 2 race grade pseudoGPA alcoholuse smoker parentinc ## 5 3 race grade pseudoGPA alcoholuse smoker parentinc ## 5 4 race grade pseudoGPA alcoholuse smoker parentinc ## 5 5 race grade pseudoGPA alcoholuse smoker parentinc In this case, I specified all of the variables that I wanted to go into my multiple imputation. If you want all of the variables in your dataset to be used, you can just feed in the dataset with no list of variable names. You can see that mice went through five iterations of imputation, with a list of the variables imputed and the order in which they were imputed. You can also see that it cycled through something called “imp.” This value corresponds to the m=5 argument I specified. This is the number of times that mice performed the entire imputation procedure. In this case, I now have five complete datasets (datasets with no missing values) in the object addhealth.complete. To access any one of them, I need to use the complete function with an id number: summary(complete(addhealth.complete, 1)) ## indegree race sex ## Min. : 0.000 White :2638 Length:4397 ## 1st Qu.: 2.000 Black/African American :1145 Class :character ## Median : 4.000 Latino : 400 Mode :character ## Mean : 4.551 Asian/Pacific Islander : 161 ## 3rd Qu.: 6.000 Other : 27 ## Max. :30.000 American Indian/Native American: 26 ## grade pseudoGPA honorsociety alcoholuse ## Min. : 7.000 Min. :1.000 Mode :logical Non-drinker:3669 ## 1st Qu.: 8.000 1st Qu.:2.250 FALSE:3990 Drinker : 728 ## Median :10.000 Median :3.000 TRUE :407 ## Mean : 9.507 Mean :2.832 ## 3rd Qu.:11.000 3rd Qu.:3.500 ## Max. :12.000 Max. :4.000 ## smoker bandchoir academicclub nsports ## Non-smoker:3732 Mode :logical Mode :logical Min. :0.000 ## Smoker : 665 FALSE:3340 FALSE:3578 1st Qu.:0.000 ## TRUE :1057 TRUE :819 Median :1.000 ## Mean :1.098 ## 3rd Qu.:2.000 ## Max. :6.000 ## parentinc ## Min. : 0.00 ## 1st Qu.: 22.00 ## Median : 40.00 ## Mean : 44.83 ## 3rd Qu.: 60.00 ## Max. :200.00 I can run a model on my first complete dataset like so: model &lt;- lm(indegree~parentinc, data=complete(addhealth.complete,1)) summary(model) ## ## Call: ## lm(formula = indegree ~ parentinc, data = complete(addhealth.complete, ## 1)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.0257 -2.6372 -0.7634 1.7695 25.0963 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.921902 0.092148 42.561 &lt;2e-16 *** ## parentinc 0.014026 0.001646 8.523 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.662 on 4395 degrees of freedom ## Multiple R-squared: 0.01626, Adjusted R-squared: 0.01604 ## F-statistic: 72.64 on 1 and 4395 DF, p-value: &lt; 2.2e-16 Addressing imputation variability Why did I run five different imputations using the mice function in the example above? To explore this question, lets first look at the slope predicting the number of friend nominations by parental income across all five complete imputations: coefs &lt;- NULL for(i in 1:5) { coefs &lt;- c(coefs, coef(lm(indegree~parentinc, data=complete(addhealth.complete,i)))[2]) } coefs ## parentinc parentinc parentinc parentinc parentinc ## 0.01402563 0.01459309 0.01415435 0.01400471 0.01483226 I get different values on each complete imputation. This is because there is an element of randomness in my imputation procedure. That randomness is needed to correctly impute while preserving the variance in the underlying variables, but it adds an additional layer of uncertainty to understanding my results. Which of these values should I use for my results? This additional layer of uncertainty is called imputation variability. Imputation variability adds additional uncertainty above and beyond sampling variability when attempting to make statistical inferences. To accurately account for imputation variability, we need to use multiple imputation. The multiple imputation process is as follows: Use imputation process with random component to impute missing values and repeat this process to produce m separate complete datasets. Each of these datasets will be somewhat different due to the randomization of imputation. Usually m=5 is sufficient. Run m separate parallel models on each imputed dataset. As a result, you will have m sets of regression coefficients and standard errors. Pool the regression coefficients across datasets by taking the mean across all m datasets. Pool standard errors by taking the average standard errors across all m datasets plus the between model standard deviation in coefficients. The formula for the overall standard error is: \\[SE_{\\beta}=\\sqrt{W+B+\\frac{B}{m}}\\] Where \\(W\\) is the squared average standard error across all \\(m\\) datasets and \\(B\\) is the variance in coefficient estimates across all \\(m\\) models. The mice library provides some nice utilities for accounting for multiple imputation in basic linear models. However, it is worth learning how to to do this procedure by hand, so I will first show you how to do it without using built-in functions. The first step is to loop through each imputation, calculate the model and extract the coefficients and standard errors: b &lt;- se &lt;- NULL for(i in 1:5) { model &lt;- lm(indegree~parentinc+smoker+alcoholuse+nsports, data=complete(addhealth.complete,i)) b &lt;- cbind(b, coef(model)) se &lt;- cbind(se, summary(model)$coef[,2]) } The b and se objects are just matrices of the regression coefficients and standard errors across imputations. For example: round(b,3) ## [,1] [,2] [,3] [,4] [,5] ## (Intercept) 3.392 3.365 3.376 3.383 3.360 ## parentinc 0.011 0.012 0.012 0.011 0.012 ## smokerSmoker 0.227 0.210 0.203 0.214 0.181 ## alcoholuseDrinker 0.599 0.620 0.640 0.624 0.630 ## nsports 0.468 0.468 0.469 0.471 0.465 With the application of the apply command we can estimate averages for these values across imputations as well as the variability in regression coefficients across models. b.pool &lt;- apply(b,1,mean) between.var &lt;- apply(b,1,var) within.var &lt;- apply(se^2,1,mean) se.pool &lt;- sqrt(within.var+between.var+between.var/5) t.pool &lt;- b.pool/se.pool pvalue.pool &lt;- (1-pnorm(abs(t.pool)))*2 results &lt;- data.frame(b.pool, se.pool, t.pool, pvalue.pool) round(results,4) ## b.pool se.pool t.pool pvalue.pool ## (Intercept) 3.3751 0.1037 32.5503 0.0000 ## parentinc 0.0118 0.0017 6.9758 0.0000 ## smokerSmoker 0.2071 0.1604 1.2909 0.1967 ## alcoholuseDrinker 0.6226 0.1546 4.0269 0.0001 ## nsports 0.4681 0.0430 10.8793 0.0000 We now have results that make use of a complete dataset and also adjust for imputation variability. We can also use the results above to estimate the relative share of the between and within variation to our total uncertainty. Figure 87 shows the relative contribution of between and within variability to our total uncertainty. Because most variables in this model are missing few or no values, the contribution of between sample imputation variability is quite small for most cases. However, for parental income which was missing a quarter of observations, it is a substantial share of around 30% of the total variability. Figure 87: Share of variability for each estimated coefficient that comes from between sample imputation variability vs. within sample sampling variabilty The for-loop approach above is an effective method for pooling results for multiple imputation and is flexible enough that you can utilize a wide variety of techniques within the for-loop, such as adjustments for sample design. However, if you are just running vanilla lm commands, then the mice library also has an easier approach: model.mi &lt;- pool(with(addhealth.complete, lm(indegree~parentinc+smoker+alcoholuse+nsports))) summary(model.mi) ## estimate std.error statistic df p.value ## (Intercept) 3.37512372 0.103689400 32.550325 3182.639 0.000000e+00 ## parentinc 0.01175002 0.001684391 6.975823 1090.345 5.265122e-12 ## smokerSmoker 0.20712655 0.160449662 1.290913 3630.841 1.968160e-01 ## alcoholuseDrinker 0.62262913 0.154619376 4.026851 3792.738 5.763470e-05 ## nsports 0.46810932 0.043027384 10.879335 4337.942 0.000000e+00 As I said earlier, multiple imputation is the gold standard for dealing with missing values. However, it is important to realize that it is not a bulletproof solution to the problem of missing values. It still assumes MAR and if the underlying models used to predict missing values are not good, then it may not produce the best results. Ideally, when doing any imputation procedure you should also perform an analysis of your imputation results to ensure that they are reasonable. These techniques are beyond the scope of this class, but further reading links can be found on Canvas. "],
["multicollinearity-and-scale-creation.html", "Multicollinearity and Scale Creation", " Multicollinearity and Scale Creation The problem of multicollinearity arises when the independent variables in a linear model are highly correlated with one another. In such a case it is possible to predict with reasonable accuracy the value of one independent variable based on all of the other independent variables in the model. Multicollinearity creates problem for model estimation in two ways. First, multicollinearity between independent variables will increase the standard error on regression coefficients making it difficult to identify statistically significant effects. Second, in nested models with highly collinear variables, results from model to model can be highly variable and inconsistent. It is not unusual that two highly collinear variables both produce substantively large and statistically significant effects when put in a model separately, but when put in together, both variables fail to produce a statistically significant effect. The intuition behind why this happens is fairly straightforward. If two independent variables are highly correlated with one another, then when they are both included in a model, it is difficult to determine which variable is actually driving the effect, because there are relatively few cases where the two variables are different. Thus, the two variables are effectively blowing each other up in the full model. Avoid the Singularity Structural multicollinearity occurs when one variable is perfectly predicted by some combination of other variables. In this situation, a model with all of these variables cannot be estimated unless one variable is dropped. To provide a simple example of perfect multicollinearity, I have coded a variable in the crime dataset for the percent female in a state. This term is perfectly collinear with the variable percent male in a state and thus it will cause problems for model estimation if I include them both: crimes$PctFemale &lt;- 100-crimes$PctMale summary(lm(Violent~PctMale+PctFemale, data=crimes)) ## ## Call: ## lm(formula = Violent ~ PctMale + PctFemale, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -293.57 -104.01 -54.44 88.98 783.97 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4317.26 1628.20 2.652 0.0108 * ## PctMale -79.75 33.01 -2.416 0.0195 * ## PctFemale NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 186.5 on 49 degrees of freedom ## Multiple R-squared: 0.1064, Adjusted R-squared: 0.08819 ## F-statistic: 5.836 on 1 and 49 DF, p-value: 0.01948 Note that the result for PctFemale is missing and the note above the coefficient table tells us that one variable is not defined because of a “singularity.” The singularity here is a matrix singularity which technically means that no inverse matrix exists for the design matrix of \\(X\\). However, the intuition here is more straightforward: PctFemale contains no information that is not already contained in PctMale so it is impossible to predict separate effects for each variable. A 1% increase in the percent male means a 1% increase in the percent female, and vice versa. We have already seen an example of perfect multicollinearity in the case of dummy variables for categorical variables. We always have to leave one dummy variable out of the model to serve as the reference category. This is because once we know the results for all but one category, we also know the results for that category. Lets say I have four categories of marital status: never married, married, widowed, and divorced. If I know you are not married, widowed, or divorced, then by process of elimination you must be never married. Structural multicollinearity is generally a specification error by the researcher rather than a true problem with the data. What we are more concerned about is data-based multicollinearity in which one variable can be predicted with high but not perfect accuracy by other variables in the model. Detecting Data-Based Multicollinearity As an example for this section, we will use data that I have collected with Nicole Marwell on the the contracting out of social services in New York City. The data were collected from 1997-2001 and contain information on the amount of dollars per capita going to neighborhoods in NYC for social services that were contracted out to non-profit organization. Our key concern here is with the distribution of this money and whether or not it was it going to more socioeconomically disadvantaged neighborhoods or more socioeconomically advantaged neighborhoods. The unit of analysis for the dataset is a “health area” which is a bureaucratic unit used by the city government that loosely corresponds to a neighborhood. The dataset has four variables: amtcapita: the amount of dollars received by a health area divided by population size. This variable is heavily right skewed so we log it in all of the models. poverty: the poverty rate in the health area. unemployed: the unemployment rate in the health area. income: the median household income in the health area. This is also right-skewed so we log it. Median income, the poverty rate, and the unemployment rate are all good measures of the socioeconomic status of a neighborhood, but we also have reason to be concerned that multicollinearity might be a problem here. The correlation matrix A first step to detecting data-based multicollinearity is to examine the correlation matrix between the independent variables. This can be done simply with the cor command: nyc$lincome &lt;- log(nyc$income) cor(nyc[,c(&quot;poverty&quot;,&quot;unemployed&quot;,&quot;lincome&quot;)]) ## poverty unemployed lincome ## poverty 1.0000000 0.6982579 -0.9120680 ## unemployed 0.6982579 1.0000000 -0.7419472 ## lincome -0.9120680 -0.7419472 1.0000000 Another more visual-pleasing approach to looking at these correlations is to graph a correlogram, which is a figure that shows the correlations between a set of variables. The excellent corrgram package in R will do this for us, and it also includes a variety of customizations that allow us to visualize this correlation in different ways. library(corrgram) corrgram(nyc[,c(&quot;poverty&quot;,&quot;unemployed&quot;,&quot;lincome&quot;)], upper.panel=&quot;panel.cor&quot;, lower.panel=&quot;panel.pts&quot;) Figure 88: A correlogram showing the relationship between three measures of neighborhood socioeconomic statsu in NYC contracting out data In this case, I am showing the correlation coefficients in the upper panel and the full bivariate scatterplots in the lower panel. the numbers are also shaded somewhat by their strength of association, although in this case the correlations are so high that the shading is not very apparent. The correlation between these three variables is very high and raises immediate concerns about a potential problem of multicollinearity if we were to try to fit these models. Examining model results Examining correlations between independent variables prior to running models is advisable. However, we can also sometimes detect multicollinearity by looking at model results across nested models. To show how this works, I have run all the possible models that involve some combination of these three variables and reported them in Table 23 below. Table 23: Models predicting the amount per capita in contracted out social services (logged) by three measures of socioeconomic status, NYC health areas Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Model 7 Intercept 4.142*** 4.135*** 10.238*** 4.121*** 3.078 -24.699*** -29.597*** (0.172) (0.231) (2.052) (0.226) (3.272) (4.941) (5.309) Poverty rate 0.039*** 0.038*** 0.127*** 0.124*** (0.007) (0.010) (0.017) (0.017) Unemployment rate 0.091*** 0.005 0.100** 0.080* (0.024) (0.033) (0.036) (0.033) Median HH income (log) -0.493* 0.091 2.516*** 2.912*** (0.191) (0.282) (0.431) (0.458) R2 0.080 0.041 0.019 0.080 0.041 0.164 0.178 Num. obs. 341 341 341 341 341 341 341 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 The presence of multicollinearity is most easily detectable in models with both the poverty rate and median household income. Compare the results from Model 5 to those from Models 1 and 3. The effect on poverty rate, more than triples in size and the standard error is doubled. The results for income completely reverse direction and become very large and the standard error more than doubles. These are clear signs of multicollinearity. These same patterns are visible in Model 7. Unemployment is less affected by these issues, but its standard error also increases fairly substantially in models with either of the other two variables. In particular the effects of income here seem particularly sensitive to the other variables. These are all hints that multicollinearity is a serious problem in the models. Calculating the variance inflation factor We can more formally assess the problem of multicollinearity in models by calculating the variance inflation factor or VIF. VIF measures the degree to which the variance in estimating an independent variable is inflated in a model due to the correlation with other independent variables in the model. For a given independent variable \\(i\\), the VIF is defined as: \\[VIF_i=\\frac{1}{1-R_i^2}\\] where \\(R_i^2\\) is the r-squared value from a model in which the \\(i\\) independent variable is predicted by all of the other independent variables in the model. For example, if we wanted to calculate the VIF for the poverty rate in a model that also included the unemployment rate and the median household income, we would do the following: model &lt;- lm(poverty~unemployed+lincome, data=nyc) 1/(1-summary(model)$r.squared) ## [1] 5.984485 So, the VIF is 5.98. This number is in terms of variance, so if we want to know the inflation in the standard errors, we need to square root it. In this case, the square root produces a value of 2.45 indicating that the standard errors for the poverty rate will be more than double in a model with both unemployment and log income, as a result of multicollinearity. The car library has a vif function in which you can feed in a model object and get out the VIF for all independent variables in the model: library(car) model.full &lt;- lm(log(amtcapita)~poverty+unemployed+lincome, data=nyc) vif(model.full) ## poverty unemployed lincome ## 5.984485 2.238379 6.822173 Since a VIF of four implies a doubling of the standard error, this is often used as a rule of thumb to identify problematic levels of VIF. However, even a smaller increase in the standard error can produce problems for model estimation, so this rule of thumb should be exercised with care. Addressing Multicollinearity Given that you have identified data-based multicollinearity, what do you do about it? A simple approach is to remove some of the highly collinear variables. However, this approach has some drawbacks. First, you have to decide which variable(s) to keep and which to remove. Because the effects of these variables are not easily distinguished, making this decision may not be easy. If the variables operationalize different conceptual ideas, you don’t have a strong empirical rationale for preferring one over the other. Second, because the variables are not perfectly correlated, you are throwing away some information. Another approach is to run separate models with only one of the highly collinear variables in each model. The primary advantage of this approach over removal is that you are being more honest with yourself (and potential readers) about the indeterminacy over which variable properly predicts the outcome. However, in this case you are also underestimating the total effects of these variables collectively and their effect as control variables in models with additional variables. It also can become unworkably complex if you have multiple sets of highly correlated variables. If the variables that are highly collinear are all thought to represent the same underlying conceptual variable, then another approach is to combine these variables into a single scale. In my example, for instance, all three of my independent variables are thought to represent I turn to the mechanics of such scale creation in the next section. Creating Scales We can create a simple summated scale simply by summing up the responses to a set of variables. However, there are several issues that we need to consider before doing this summation. First, we need to consider how to handle the case of categorical variables. If we have a set of yes/no questions, we might simply sum up the number of “yes” responses. For example, the GSS has a series of questions about whether people would allow certain groups (e.g. atheists, communists, racists, militarists) to (1) speak in their community, (2) teach at a university, or (3) have a book in a library. Many researchers have summed up the number of “yes” responses across all groups and questions to form an index of “tolerance.” Ordinal variables with more than two categories are more difficult. A typical example is a Likert-scale question that asks for someone’s level of agreement with a statement (e.g. “strongly disagree”, “disagree”,“neither”,“agree”,“strongly agree”). A typical approach is to score responses (e.g. from 1-5 for five categories) and then to sum up those scores. However, since we are dealing with ordinal categorical responses, we are making an assumption that a change of one on this scale is the same across all levels. Second, we need to consider that some variables might be measured on different unit scales. If we are counting up a score on a set of Likert-scale questions that are all scored 1-5, then this is not usually considered an important issue. However, if we want to combine variables measured in very different units or with different spreads, then these variables will not be represented equally in th scale unless we first standardize them. The most common approach to standardization is to take the z-score of a variable by subtracting its mean and dividing by its standard deviation: \\[z_i=\\frac{x_i-\\bar{x}}{s_x}\\] When variables are re-scaled in this way, they will all have a mean of zero and a standard deviation of one. This re-scaling is easy to do manually in R, but the scale function will also do it for you: nyc$poverty.z &lt;- scale(nyc$poverty) nyc$unemployed.z &lt;- scale(nyc$unemployed) mean(nyc$unemployed.z) ## [1] -1.45899e-16 sd(nyc$unemployed.z) ## [1] 1 Even when re-scaling is not necessary because variables are all measured on the same scale, I think re-scaling can be useful so that the final summated scale is easier to interpret. Third, we need to be careful to make sure that all of the variables are coded in the same direction relative to the underlying concept. In my case, median household income is coded in the opposite direction as poverty and unemployment. Whereas higher poverty and unemployment indicates greater disadvantage, higher household income indicates less disadvantage. in these cases, you need to reverse the coding of some of the variables so that higher values are associated with being higher on the underlying conceptual variable you are trying to measure. In my case, I can do this for income by multiplying by -1 after re-scaling: nyc$lincome.z &lt;- -1 * scale(nyc$lincome) Once all of these considerations are done, I can add up my re-scaled and properly coded variables to create a single scale of “neighborhood deprivation.” It is not necessary but I usually like to re-scale the final summated scale again so that one unit equals a one standard deviation change. nyc$deprivation.summated &lt;- scale(nyc$poverty.z+nyc$unemployed.z+nyc$lincome.z) And now lets use this summated scale in the model: model.summated &lt;- lm(log(amtcapita)~deprivation.summated, data=nyc) pander(tidy(model.summated)) term estimate std.error statistic p.value (Intercept) 4.947 0.08807 56.18 8.329e-174 deprivation.summated 0.3748 0.0882 4.249 2.775e-05 The model predicts that a one standard deviation increase in a neighborhood’s deprivation score is associated with about a 45% increase (\\(100 * (e^{0.3748}-1)\\)) in the amount of per-capita funding for social services. Assessing a scale’s internal consistency One important measure of the quality of a scale composed of multiple variables is the internal consistency (also sometimes called internal reliability) of the scale. Internal consistency is assessed by the degree to which variables within the scale are correlated with one another. Although examination of the full correlation matrix provides some measure of this internal consistency, the most common single measure used to test for internal consistency is Cronbach’s alpha. We won’t go into the details of how to calculate Cronbach’s alpha here. It ranges from a value of 0 to 1, with higher values indicating greater internal consistency. Intuitive explanations for what Cronbach’s alpha measures are somewhat lacking, but it gives something like the expected correlation between two items measuring the underlying construct. Cronbach’s alpha can be easily in R with the alpha function in the psych library. You can feed all the variables that make up the scale into Cronbach’s alpha or just the correlation matrix between these variables. The variables do not need to be re-scaled but we do not to ensure they are all coded in the same direction. library(psych) alpha(cor(nyc[,c(&quot;poverty.z&quot;,&quot;unemployed.z&quot;,&quot;lincome.z&quot;)])) ## [1] &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; ## [7] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; Cronbach’s alpha ranges from a value of 0 to 1. As a rule of thumb, many people use a value of 0.7 as a benchmark for a good Cronbach’s alpha, although as with all rules of thumb, caution should be taken in interpreting this too literally. In this case, the “raw alpha” of 0.92 is very good and suggests that the internal consistency of this deprivation measure is high. This function also returns a sensitivity analysis telling us how much Cronbach’s alpha would change if we were to remove a given variable from the scale. In this case, the results suggest that we would get even higher internal consistency if we removed the unemployment rate. However, since the alpha on all three was so high, it seems unnecessary to drop it in this case. Cronbach’s alpha is often thought of as a test of whether the individual items all measure the same “latent” construct. While, it can be thought of as a test of whether items measure a construct vs no construct at all, it does not directly address another issue which is that items used as a measure of a single scale may instead be related to multiple latent constructs. In order to address that issue, we must move to factor analysis. Factor Analysis Generally speaking, factor analysis is a broad set of techniques that are designed to test whether specific variables are related to one or more underlying latent constructs. Most of the original work on factor analysis comes out of psychology where researchers were interested in whether items though to capture underlying latent concepts like “anxiety” and “depression” were actually correlated with some underlying constructs and whether these constructs were Differentiable. So, if you give respondents a survey with several items measuring anxiety and several other items measuring depression, you want the anxiety measures to all correlate highly and the depression measures to all correlate highly, but for the two groups to be distinguishable from each other. Factor analysis is so broadly deployed across disciplines that there is some terminological confusion in the way people talk about specific techniques. There are two separate approaches methodologically to factor analysis. In the first approach, often called principal factors analysis or principal axis analysis, the researcher only uses information about the shared variance (the communality in factor analysis speak) between items to conduct a factor analysis. In the second approach, principal component analysis (PCA), the researcher uses the entire variance across all items. To add greater confusion, people often refer to factor analysis as the first technique and treat PCA as something distinct, although factor analysis should properly be thought of as encompassing both techniques. In practice, the results across these two techniques are usually quite similar. The argument for using one approach or the other is whether the researcher is interested primarily in data reduction (simplifying analyses by combining similar items) or truly measuring a latent construct. For our purposes, I will outline principal factor analysis here, but the pca command in the psych package can be used in an almost identical fashion to conduct a PCA. To begin, lets assume a simple model with three items (\\(z_1\\), \\(z_2\\), and \\(z_3\\)) which we believe are measured by a single latent construct, \\(F\\). All items are standardized as z-scores. We believe that the outcome for a given observation on these three items is a function of the latent construct and some component unique to each item (\\(Q_1\\), \\(Q_2\\), and \\(Q_3\\)). We then can construct a system of equations like so: \\[z_{i1}=b_1F_i+u_1Q_1\\] \\[z_{i2}=b_2F_i+u_2Q_2\\] \\[z_{i3}=b_3F_i+u_3Q_3\\] Each of the \\(b\\) values below is the correlation coefficient between the underlying facto (\\(F\\)) and the given item. These are called factor loadings. The \\(F_i\\) values are the factor scores underlying the common latent factor and the \\(Q\\) values are the unique component to each item that is not explained by the common factor. Factor analysis solves this system of equations to find factor loadings and factor scores. The factor loadings are particularly important because they provide information about how strongly correlated the individual items are to the underlying factor and thus the validity of the assumption that they are all represented by a single common factor. The fa command in the pysch library will conduct a factor analysis. Just as for alpha, you can feed in raw data or just the correlation matrix. However, if you want the factor analysis to estimate factor scores, you need to feed in the raw data. Lets do a factor analysis of the three measures of neighborhood deprivation in the NYC data. factor_nyc &lt;- fa(nyc[,c(&quot;poverty.z&quot;,&quot;unemployed.z&quot;,&quot;lincome.z&quot;)], nfactors=1) loadings(factor_nyc) ## ## Loadings: ## MR1 ## poverty.z 0.926 ## unemployed.z 0.754 ## lincome.z 0.984 ## ## MR1 ## SS loadings 2.396 ## Proportion Var 0.799 The loadings function lets us extract the factor loadings. We can see that the factor loadings here are all very high, although the correlation for the unemployment rate is lower than the other two. We can also see that this single factor can account for nearly 80% of the variation across the three items. We can also display what this factor analysis looks like graphically with the fa.diagram function in the psych library. fa.diagram(factor_nyc) In this case its not terribly helpful, because we only had a single factor and the factor loadings were all high. When the factor loadings are below some threshold the lines are typically not drawn which can make this diagrams really useful for sorting out relationships with cases of multiple factors. Factor analysis with multiple common factors The example above assumed only one common factor, but we can also explore models that allow for more common factors. The only hard rule is that you have to have less common factors than items. To generalize the equations above, lets say we have a set of \\(J\\) observed variables that we want to divide between \\(m\\) common factors. We then have a set of \\(J\\) equations where the equation for the \\(j\\)th observed variable is: \\[z_{ji}=b_{j1}F_{1i}+b_{j2}F_{2i}+b_{jm}F_{mi}+u_jQ_{ji}\\] This means that we will have \\(m\\) factor loadings for each item. If our variables really do split into the expected number of common factors, then we should observe high factor loadings for each item on only one of the underlying common factors. The factor loadings that are estimated in factor analysis also depend on what is called a rotation. We won’t delve into the technical details of rotation here, but the basic idea is that there are actually an infinite number of ways to display the same set of factor loadings in \\(n\\)-dimensional space for a set of \\(n\\) common factors. So, we need to choose some way to “rotate” those loadings in n-dimensional space in order to understand them better. You can either rotate them orthogonally so that the common factors are uncorrelated or obliquely so that correlation between the common factors is allowed. The “varimax” rotation is the most popular orthogonal rotation because it chooses factor loadings that either maximize or minimize the loadings on certain variables, easing interpretation. The “oblimin” rotation is popular for the oblique case and is the default setting in the fa function. Personally, I don’t see a lot of practical value in assuming orthogonal factors, so I tend to prefer “oblimin.” To illustrate how this all works, lets look at another example. This example comes from Pew data collected on Muslim respondents in thirty-five different countries between 2008-2012. One set of questions in the survey asked respondents to rate the morality of a variety of stigmatized behaviors. Respondents could either select “morally acceptable”, “depends/not a moral value”, or “morally wrong.” Respondents were asked about the following behaviors: divorce polygamy fertility restriction alcohol euthanasia suicide abortion prostitution premarital sex homosexuality I would like to explore to what extent answers to these responses reflect an underlying common factor of “social conservativeness.” In order to do that I need to show that these responses fit well for a single common factor and that they fit better for a single common factor than for multiple factors. First, I need to scale my ordinal variables into quantitative variables with a score from 1 to 3. Since they are already ordered correctly, I can just use the as.numeric to cast them to numeric values. morality &lt;- cbind(moral_divorce=as.numeric(pew$moral_divorce), moral_fertility=as.numeric(pew$moral_fertility), moral_alcohol=as.numeric(pew$moral_alcohol), moral_euthansia=as.numeric(pew$moral_euthanasia), moral_suicide=as.numeric(pew$moral_suicide), moral_abortion=as.numeric(pew$moral_abortion), moral_prostitution=as.numeric(pew$moral_prostitution), moral_premar_sex=as.numeric(pew$moral_premar_sex), moral_gay=as.numeric(pew$moral_gay)) head(morality) ## moral_divorce moral_fertility moral_alcohol moral_euthansia moral_suicide ## [1,] 3 2 2 2 3 ## [2,] 3 1 2 2 3 ## [3,] 3 2 3 2 2 ## [4,] 3 2 3 3 2 ## [5,] 2 1 2 2 2 ## [6,] 3 1 2 2 2 ## moral_abortion moral_prostitution moral_premar_sex moral_gay ## [1,] 3 1 3 3 ## [2,] 3 2 2 2 ## [3,] 2 3 2 2 ## [4,] 2 3 3 3 ## [5,] 3 3 3 3 ## [6,] 3 3 3 3 Lets start with a look at the correlation matrix: library(corrgram) corrgram(morality, upper.panel=&quot;panel.cor&quot;, lower.panel=&quot;panel.shade&quot;) Figure 89: A correlogram showing the relationship between measures of morality in Pew data We can already see here that two of these behaviors (divorce and fertility) do not look like the others. Although they are somewhat weakly correlated with one another, they have very weak correlation with the other variables. Among the remaining variables, we do see a fairly substantial pattern of intercorrelation. I also want to examine Cronbach’s alpha, but before I do that I want to take advantage of the fact that Cronbach’s alpha and the factor analysis only requires the correlation matrix rather than the dataset itself. My data has missing values and this is one area where available-case analysis is probably preferable to complete-case analysis. For the correlation matrix, I can use all of the valid data for each pairwise comparison. That will lead to different sample sizes on each correlation coefficient, but will allow me to use the data to its fullest. I can do that in R by including the use=\"pairwise.complete.obs\" argument in cor: morality_r &lt;- cor(morality, use=&quot;pairwise.complete.obs&quot;) Lets look at Cronbach’s alpha before proceeding to full-blown factor analysis. alpha(morality_r) ## [1] &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [7] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; ## [13] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [19] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [25] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [31] &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [37] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; ## [43] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [49] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [55] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [61] &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [67] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; ## [73] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [79] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; A Cronbach’s alpha of 0.71 is decent, but the results also suggest that I would improve Cronbach’s alpha by removing either divorce or fertility from my set of measures. For the factor analysis, I will try out a one factor, two factor, and a three factor solution. morality_fa1 &lt;- fa(morality_r, 1) morality_fa2 &lt;- fa(morality_r, 2) morality_fa3 &lt;- fa(morality_r, 3) Lets compare the diagrams graphically: Figure 90: Three different factor solutions for the Pew morality data As you can see, for the single factor solution, the factor loadings were so low that divorce and fertility are dropped from the common factor. The two factor solution adds them back in as a separate factor, but even here the factor loadings of 0.4 are pretty low. The three factor solution splits my main group into two separate groups where one group seems to measure death in some way (suicide, euthanasia, and abortion) while the first group measures mostly sex with the addition of alcohol - suggesting alcohol use is viewed similarly to stigmatized sexual behavior. Although the division in the three factor solution is interesting, this level of detail does not necessarily seem warranted and leads to some low factor loadings. What seems clear above all else, is that the fertility limitation and divorce questions do not belong with the others. Therefore, a summated scale that included all variables except for these two is reasonably justified. To check this, lets see how Cronbach’s alpha does if we remove the fertility and divorce questions. alpha(cor(morality[,c(-1,-2)], use=&quot;pairwise.complete.obs&quot;)) ## [1] &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [7] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [13] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; ## [19] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [25] &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [31] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [37] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#000000&quot; &quot;#FFFFFF00&quot; ## [43] &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; &quot;#FFFFFF00&quot; ## [49] &quot;#000000&quot; We now get a stronger alpha of around 0.76 and little evidence that more removals would improve the internal consistency. In closing, I should note that factor analysis is a huge area of research and I am only touching on the surface of it here. On the Canvas pages, I provide links for additional information. "],
["model-selection.html", "Model Selection", " Model Selection In practice, researchers often have to decide between a variety of possible models. The different models available to a researcher can derive from a number of factors, but most often arise because the researcher has a wide set of possible control variables that could be added to the model. In addition, these models could be added as simple additive effects, or the researcher could add additional interaction terms, squared terms, splines, etc. In practice, the number of possible models one could construct from a set of variables is quite large. How does one go about choosing the “right” model? For example, Table 24 shows a variety of models that could be used to predict violent crime rates across US states. Note that the model estimates for some variables such as percent male differ substantially across models. How can we possibly decide which of these models is the best model? Table 24: Models predicting violent crime rate Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 Intercept 7534.81*** 619.07 604.71 128.07 -6.09 -638.35 (1724.03) (2995.68) (3031.21) (2972.28) (113.33) (599.66) Median Age -38.56*** -27.84* -28.06* -27.84* -24.90* (10.89) (10.94) (11.28) (11.02) (9.40) Percent Male -115.67*** -23.28 -23.21 -12.29 (31.39) (44.75) (45.23) (44.59) Gini 43.01** 43.71* 39.04* 38.68** (15.68) (17.36) (17.15) (11.67) Poverty Rate -0.88 -6.09 (8.91) (9.17) Unemployment Rate 25.47 45.89*** 23.02 (14.18) (12.99) (13.14) R2 0.29 0.39 0.39 0.43 0.20 0.42 Num. obs. 51 51 51 51 51 51 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 There is no “right” model By definition, models are not a 100% accurate representation of reality. Therefore, its silly to think that a single model can be the “right” one. In this chapter, we will learn some tools that can help with making decisions about model selection, but the decision about model selection is one that must be driven first and foremost by the research question at hand. For example, if my goal is to understand the relationship between percent male and violent crimes, then only models 1-4 from Table 24 are relevant, because only these models include the percent male variable as a predictor. Within that set, it probably makes more sense to think about how robust my results are to several different model specifications than to focus on a single model. Generally, so long as issues of multicollinearity do not arise it might be better to control for more variables rather than less in this context. A different research question might focus on two different competing explanations for an outcome. In this case, I might need to directly compare the results of two different models to address my research question. In this case, some of the model selection tools we develop below will be more crucial in making this decision, but I still might need to consider how robust my results are to the inclusion of different control variables. Ultimately, model selection can never be simplified into an automated process that protects the researcher from having to make decisions. There is no one way to make such decisions and must be considering such things as the research question, theoretical arguments about the process, the results of prior research, and the sensitivity of results to alternative specifications. The accuracy vs. parsimony tradeoff If we have an additional variable avaliable to us, why not throw it into the model? More control variables allow us to eliminate potential sources of confounding in the relationships we care about, so isn’t more always better than less? There are two reasons, why we don’t always want to use this kitchen sink approach. First, as discussed in the previous section, adding in additional variables can lead to problems of multicollinearity. Collinearity between your independent varibles can increase standard errors on your estimates and lead you to a situation in wich in spite of strong point estimates of association, none of your results are statistically significant. For this reason, you should never just look at statistically significance when you observe that the addition of another variable made the relationship between \\(x\\) and \\(y\\) go away. If might have become non-significant because of an inflation in standard errors rather than a reducation in the correlation. Second, we generally value the concept of parsimony as well as accuracy. Accuracy is measured by goodness-of-fit statistics like \\(R^2\\). Your model is more accurate if it can account for more variation in the outcome variable \\(y\\). However, an additional independent variable can never make \\(R^2\\) decrease and will almost always make it increase by some amount. Therefore, if we only prioritize accuracy we will continue to add variables into our models indefinitely. We also prize parsimony. We want a model that gets the most “bang for our buck” so to speak. We prefer simpler explanations and models to more complex ones if those simpler models can be almost as accurate as the more complex model. One important feature of parsimony and the very idea of scientific “models” in general is that we are not simply trying to reproduce reality. If we wanted to do that, we could simply create a dummy variable for each observation and produce a model with an \\(R^2=1\\). Yet, such a model would tell us nothing about the underlying process that produces variation in our outcome - we would just reproduce the world as it is. In principle, the very idea of scientific modeling is to reduce the complexity of the actual data in order to hopefully understand better the process. Including more and more variables into a model is rarely theoretically interesting and may in fact be misleading or confusing as to the underlying causal processes we hope to uncover. In practice, we must balance these two features of models. Gains in parsimony will be offset by reductions in accuracy, and vice versa. There is no precise way to say where the threshold for that tradeoff should be, but as a practical researcher, you should always understand that this is the tradeoff you are making when you choose between models. Null vs. saturated model Two conceptual models illustrate the extreme ends of the accuracy vs. parsimony spectrum. The most parsimonious model that we can ever have is one without any independent variables. In this case, our predicted value for the outcome variable would always equal the mean of the outcome variable. Formally, we would have: \\[\\hat{y}_i=\\bar{y}\\] This null model has zero accuracy. It doesn’t even pretend to be accurate. We can estimate this model easily enough in R: model_null &lt;- lm(Violent~1, data=crimes) coef(model_null) ## (Intercept) ## 384.5039 summary(model_null)$r.squared ## [1] 0 At the other end of the spectrum, we have the saturated model. In the saturated model, we have a number of variables equal to the number of observations minus one. The saturated model is not something we typically observe in practice because these independent variables must be uncorrelated enough that we don’t observe perfect collinearity. A simple example would be to simply include a dummy variable for each observation. Since we have the state as a variable in the crimes dataset, we can actually calculate the saturated model here: model_saturated &lt;- lm(Violent~State, data=crimes) summary(model_saturated)$r.squared ## [1] 1 In practice, all real models fall somewhere between these two extremes. Most of the model selection tools that we discuss below will make use of these two conceptual models theoretically. A not-very-useful tool: the F-test The F-test is not terribly useful in practice, but we will build some important principles from it for later tools in this module and the next module. The F-test is a classic tool that uses the framework of hypothesis testing to adjudicate between models. The F-test can compare any two nested models. For one model to be nested within another, it must contain all of the original variables in the other model plus additional variables. When models are built by starting with a few variables and then adding in additional variables in a sort of reverse stairstep, then we have nested models. Models 1-4 from Table 24 above are nested models, while models 5-6 are not nested within models 1-4. If we take any two nested models, the null hypothesis of the F-test is that none of the additional variables in the more complex model are correlated with the outcome (e.g. the \\(\\beta\\) slope is zero on all additional variables). To test this, we calculate the F-statistic: \\[F=\\frac{(SSR_1-SSR_2)/g}{SSR_2/(n-g-k-1)}\\] Where \\(SSR\\) is the sum of squared residuals for the given model, \\(g\\) is the number of additional variables in model 2 and \\(k\\) is the number of variables in model 1. Basically we are taking the reduction in SSR per variable added in model 2 divided by the SSR left in model 2 per remaining degree of freedom. If the null hypothesis is correct, we expect this number to be one. The larger the F-statistic gets, the more suspicious we are of the null hypothesis. We can formalize this by calculating the p-value from the right tail of an F-distribution for a given F-statistic. This F-distribution will As an example, lets calculat the F-statistic between the null model predicting violent crimes and a model that predicts violent crimes by the median age and percent male in a state. model_null &lt;- lm(Violent~1, data=crimes) model_demog &lt;- lm(Violent~MedianAge+PctMale, data=crimes) SSR1 &lt;- sum(model_null$residuals^2) SSR2 &lt;- sum(model_demog$residuals^2) g &lt;- length(model_demog$coef)-length(model_null$coef) k &lt;- length(model_null$coef)-1 n &lt;- length(model_null$residuals) Fstat &lt;- ((SSR1-SSR2)/g)/(SSR2/(n-g-k-1)) Fstat ## [1] 9.869375 That seems quite a bit bigger than one. We can find out the probability of getting an F-statistic this large or larger if median age and percent male were really uncorrelated with violent crime rates in the population by getting the area of the right tail for a corresponding F-distribution: 1-pf(Fstat, g, n-g-k-1) ## [1] 0.0002568606 There is a very small probability that we would have gotten an F-statistic this large or larger if the null hypothesis was true. Therefore we reject the null hypothesis and prefer the model with median age and percent male to the null model. Rather than doing this by hand, we could have used the anova command in R to make the comparison: anova(model_null, model_demog) ## Analysis of Variance Table ## ## Model 1: Violent ~ 1 ## Model 2: Violent ~ MedianAge + PctMale ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 50 1907432 ## 2 48 1351616 2 555817 9.8694 0.0002569 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ANOVA stands for Analysis of Variance and can be used to conduct F-tests between models. We can also use it to compare more than two models at the same time, so long as they are all nested: model_gini &lt;- update(model_demog, .~.+Gini) model_complex &lt;- update(model_gini, .~.+Poverty+Unemployment) anova(model_null, model_demog, model_gini, model_complex) ## Analysis of Variance Table ## ## Model 1: Violent ~ 1 ## Model 2: Violent ~ MedianAge + PctMale ## Model 3: Violent ~ MedianAge + PctMale + Gini ## Model 4: Violent ~ MedianAge + PctMale + Gini + Poverty + Unemployment ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 50 1907432 ## 2 48 1351616 2 555817 11.5056 9.209e-05 *** ## 3 47 1165120 1 186496 7.7211 0.007935 ** ## 4 45 1086941 2 78179 1.6183 0.209554 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The F-test prefers model 2 to model 1 and model 3 to model 2, but does not prefer model 4 to 3. The additional reduction in SSR from model 3 to 4 (78179) is not enough higher than what could be expected by chance if these two additional parameters were not correlated with violent crime rates. However, you need to be careful about making model selections here because the order in which things are entered can affect the results. Lets say for example that we included unemployment with the gini coefficient and then added poverty rates separately in model 4. model_giniplus &lt;- update(model_demog, .~.+Gini+Unemployment) model_complex &lt;- update(model_gini, .~.+Poverty) anova(model_null, model_demog, model_giniplus, model_complex) ## Analysis of Variance Table ## ## Model 1: Violent ~ 1 ## Model 2: Violent ~ MedianAge + PctMale ## Model 3: Violent ~ MedianAge + PctMale + Gini + Unemployment ## Model 4: Violent ~ MedianAge + PctMale + Gini + Poverty ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 50 1907432 ## 2 48 1351616 2 555817 11.6471 8.08e-05 *** ## 3 46 1097590 2 254026 5.3231 0.008327 ** ## 4 46 1164871 0 -67281 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now we prefer to have unemployment in the model. However, it gets even worse than this. Lets take a look at the actual results of the third model: summary(model_giniplus) ## ## Call: ## lm(formula = Violent ~ MedianAge + PctMale + Gini + Unemployment, ## data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -288.18 -107.61 -5.40 56.58 508.18 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 272.90 2946.20 0.093 0.9266 ## MedianAge -26.50 10.76 -2.462 0.0176 * ## PctMale -13.98 44.25 -0.316 0.7534 ## Gini 35.23 16.06 2.193 0.0334 * ## Unemployment 22.50 13.37 1.682 0.0993 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 154.5 on 46 degrees of freedom ## Multiple R-squared: 0.4246, Adjusted R-squared: 0.3745 ## F-statistic: 8.485 on 4 and 46 DF, p-value: 3.249e-05 Percent male is not statistically significant and has a small substantive effect, because its large negative effect was driven by an indirect association with the gini coefficient. Unemployment has a marginally significnat but also small effect. But we never tested a model with just median age and the gini coefficient. What happens if we try that? model_another &lt;- lm(Violent~MedianAge+Gini, data=crimes) anova(model_another, model_giniplus) ## Analysis of Variance Table ## ## Model 1: Violent ~ MedianAge + Gini ## Model 2: Violent ~ MedianAge + PctMale + Gini + Unemployment ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 48 1171830 ## 2 46 1097590 2 74240 1.5557 0.2219 Now we prefer another model that was nested within our more complex model but not nested within the original model structure. F-tests can easily lead you into situations like this. If you add in variables one at a time, the F-test will also produce the exact same result as a t-test on the additional variable: model_one &lt;- lm(Violent~MedianAge, data=crimes) model_two &lt;- update(model_one, .~.+Poverty) anova(model_one, model_two) ## Analysis of Variance Table ## ## Model 1: Violent ~ MedianAge ## Model 2: Violent ~ MedianAge + Poverty ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 49 1734016 ## 2 48 1574243 1 159773 4.8716 0.03212 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(model_two)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 891.15575 465.632320 1.913861 0.06161074 ## MedianAge -20.64100 11.346820 -1.819100 0.07513705 ## Poverty 18.64995 8.449701 2.207173 0.03211668 Notice that the p-value for the poverty variable in model 2 is identical to the p-value for the F-test between models 1 and 2. Thus the F-test does not really do much for us that we couldn’t already do with a t-test one variable at a time. Aside from the issues highlighted above, the primary problem with the F-test is that its decisions are driven by the logic of hypothesis testing and the search for “statistically significant” results. Putting aside that this logic makes no sense for the crime data because they are not a sample, this is not a good practice in general. If we decide what to prioritize by statistical significance, then in smaller datasets we will prefer different, more parsimonious, models than in larger datasets. It would be better to choose models based on theoretical principles that are consistent across sample size than to mechanistically determine them in this way. Tools with a parsimony penalty Two model selection tools allow us to more formally consider the parsimony vs. accuracy tradeoff. We can think of the \\(R^2\\) value as a measure of accuracy. These two tools then apply a “parsimony penalty” to this measure of accuracy to get a single score that measures the quality of the model. In both cases, lack of parsimony is reflected by \\(p\\), the number of independent variables in the model. Adjusted \\(R^2\\) The adjusted \\(R^2\\) subtracts a certain value from \\(R^2\\) based on the number of independent variables in the model. Formally: \\[R^2_{adj}=R^2-(\\frac{p}{n-p-1})(1-R^2)\\] The parsimony penalty is based on a scaling of the lack of goodness of fit \\((1-R^2)\\) and the number of variables proportionate to the size of the sample. Adjusted \\(R^2\\) is provided by default in the summary of an lm object: summary(model_demog) ## ## Call: ## lm(formula = Violent ~ MedianAge + PctMale, data = crimes) ## ## Residuals: ## Min 1Q Median 3Q Max ## -379.94 -93.06 -32.30 68.80 572.02 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7534.81 1724.03 4.370 6.62e-05 *** ## MedianAge -38.56 10.89 -3.540 0.000901 *** ## PctMale -115.67 31.39 -3.685 0.000581 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 167.8 on 48 degrees of freedom ## Multiple R-squared: 0.2914, Adjusted R-squared: 0.2619 ## F-statistic: 9.869 on 2 and 48 DF, p-value: 0.0002569 summary(model_demog)$adj.r.squared ## [1] 0.26187 The parsimony penalty in the adjusted \\(R^2\\) will be smaller as \\(n\\) increases. Therefore this tool will tend to prefer more complex models in larger samples. This feature distinguishes its approach from the Bayesian Information Criterion (BIC) below. Bayesian Information Criterion The Bayesian Information Criterion, or BIC for short, is based implicitly on a Bayesian logic to statistical inference that we are going to gloss over for now. In practice, there are two forms of the BIC: BIC, which is implicitly compared to the saturated model. The R command BIC will give you this version. BIC’, which is implicitly compared to the null model. There is no R command for BIC’ but we will create our own function below. I find that the BIC’ formulation is more pleasing and so we will focus on that version here. However, you can compare any two non-null models directly using either criterion and the differences between them will be the same. The formula for \\(BIC&#39;\\) is: \\[BIC&#39;=n\\log(1-R^2)+p\\log(n)\\] The first term includes \\(R^2\\) and is our measure of accuracy. Because we are actually loggin something less than one, we will get a negative value here. For the BIC, lower scores are better and negative scores are the best. bove, I said that each BIC type was implicitly being compared to either the saturated or the null model. If the respective BIC score is below zero then this model is preferred to the null or saturated model, respectively. The second term \\(p\\log(n)\\) is the parsimony penalty. In this case, we actually weight the parsimony penalty higher in larger samples. As a result, BIC will tend to prefer the same model more consistently in different sample sizes. Since, R does not give us a BIC’ calculations, lets go ahead and create our own with a function: bic_null &lt;- function(model) { rsq &lt;- summary(model)$r.squared n &lt;- length(model$resid) p &lt;- length(model$coef)-1 return(n*log(1-rsq)+p*log(n)) } bic_null(model_demog) ## [1] -9.703675 The negative value for BIC’ tells us that we prefer the model with median age and percent male as predictors to the null model. We can also compare two non-null models directly by comparing their BIC or BIC’ scores. Whichever is lower is the preferred model. These models do not have to be nested. bic_null(model_demog) ## [1] -9.703675 model_econ &lt;- lm(Violent~Poverty+Unemployment+Gini, data=crimes) bic_null(model_econ) ## [1] -9.184312 BIC(model_demog) ## [1] 679.8933 BIC(model_econ) ## [1] 680.4127 In this situation, we slightly prefer the demographic explanation to the economic one. Note that although bic_null and BIC give us different values, the differences between them in different models are the same: bic_null(model_demog)-bic_null(model_econ) ## [1] -0.5193631 BIC(model_demog)-BIC(model_econ) ## [1] -0.5193631 You can use the general rule of thumb to evaluate differences in BIC: Difference in BIC Strength of Preference 0-2 Weak 2-6 Moderate 6-10 Strong greater than 10 Very Strong Model Averaging None of the tools mentioned above can fully address the complexity of model selection. One of the key difficulties is that given a set of potential variables, it includes not only what gets included but the order of inclusion. As we have seen above, percent male seems to have a large negative effect on violent crime rates until we consider a model that includes Gini coefficients. Where and how we include these variables in sequential models might affect our ultimate model selection. One way of addressing this is to use a technique of model averaging. In the model averaging approach, we use a computer to iteratively fit all of the possible models from a set of candidate variables. We then use some procedure to average results across these models to account more formally for model uncertainty in our final conclusion. There are a variety of ways in which such model averaging can be done. The BMA package in R will use a Bayesian model averaging approach that implicitly uses a Bayesian logic and the BIC to adjudicate between models. Lets try that with our crime data: library(BMA) model.bma &lt;- bic.glm(crimes[,c(&quot;MedianAge&quot;,&quot;PctMale&quot;,&quot;PctLessHS&quot;, &quot;MedianIncomeHH&quot;,&quot;Unemployment&quot;, &quot;Poverty&quot;, &quot;Gini&quot;)], crimes$Violent, glm.family=gaussian) summary(model.bma) ## ## Call: ## bic.glm.data.frame(x = crimes[, c(&quot;MedianAge&quot;, &quot;PctMale&quot;, &quot;PctLessHS&quot;, &quot;MedianIncomeHH&quot;, &quot;Unemployment&quot;, &quot;Poverty&quot;, &quot;Gini&quot;)], y = crimes$Violent, glm.family = gaussian) ## ## ## 11 models were selected ## Best 5 models (cumulative posterior probability = 0.8 ): ## ## p!=0 EV SD model 1 model 2 model 3 ## Intercept 100 -1592.518 735.881 -1699.048 -1848.696 -1913.727 ## MedianAge 12.4 -1.683 6.761 . . . ## PctMale 9.6 -1.676 10.509 . . . ## PctLessHS 20.2 -2.728 7.327 . -12.841 . ## MedianIncomeHH 94.3 19.518 6.419 20.804 22.719 18.595 ## Unemployment 12.4 1.501 6.671 . . . ## Poverty 94.3 65.115 21.788 67.908 82.593 59.191 ## Gini 16.2 3.730 12.147 . . 10.052 ## ## nVar 2 3 3 ## BIC -141.527 -138.995 -138.020 ## post prob 0.448 0.126 0.078 ## model 4 model 5 ## Intercept -1629.940 -849.020 ## MedianAge . . ## PctMale . -15.701 ## PctLessHS . . ## MedianIncomeHH 19.585 20.130 ## Unemployment 8.553 . ## Poverty 62.549 65.134 ## Gini . . ## ## nVar 3 3 ## BIC -137.950 -137.883 ## post prob 0.075 0.072 The BMA approahc has given us what it considers the five most likely models. The first column (p!=0) gives us the probaility that the parameter is actually not zero across models. Two variables, the poverty rate and median household income have high probabilities here. The second term, (EV) gives us the expected value of that slope across the most likely models. This is basically the average across models. The BMA results also show us the five most likely models. Note that they are ordered by BIC’ which is the primary technique this procedure uses to evaluate models. Note that none of these models are even close to what we were estimating before. We were not seeing high effects of poverty in the models and didn’t consider median household income. This model averaging approach suggests that we might want to reconsider our approach. Remember the advice from the beginning of this section. The model averaging approach is seductive because it seems to do the hard work for us, but it should only be used as a tool for the researcher to ultimately make that decision. "],
["modeling-categorical-outcomes.html", "Modeling Categorical Outcomes", " Modeling Categorical Outcomes In this chapter, we will extend the concept of the linear model to include the case where we want to predict categorical outcomes on the left-hand side of our model equation. In order to do this, we will have to learn an extension of the OLS regression model called the generalized linear model which will provide a more flexible way to specify linear models, including ones with categorical outcomes. By the end of this chapter, you will be able to run full linear models with dichotomous (two categories) and polytomous (many categories) variables as outcomes. Slides for this module can be found here. "],
["dichotomous-outcomes-and-the-binomial-distribution.html", "Dichotomous Outcomes and The Binomial Distribution", " Dichotomous Outcomes and The Binomial Distribution We now have a great many tools to produce more complex and realistic specifications for the right-hand side of the linear model formula. We can add dummy variables, interaction terms, transformations, splines, and polynomial terms. For outcomes, however, we are still stuck with quantitative variables, which eliminates a lot of potential outcomes that we care about in the social sciences. Lets take the Titanic data as an example. The obvious outcome of interest is whether someone survived or died on the Titanic. We already know how to examine a bivariate relationship between survival and one other variable. We can do this using a two-way table if the other variable is categorical and mean differences if both variables are quantitative. However, we can’t put this into a model framework where we can control for multiple variables simultaneously, look at interaction terms, model non-linearity, etc. For example, we know that women were more likely to survive the Titanic than men and we know that higher-class passengers were more likely to survive than lower-class passengers. However, we also know that women were more likely to be higher-class passengers than men. How much of the gender difference in survival might be accounted for by these underlying class differences? Alternatively, we might be interested in interacting gender and passenger class to look at how the gender difference in survival varies by passenger class. Both of these tasks would be easier if we could use the same model framework we have developed to predict survival and death. It turns out that we can use this model framework, but in order to do so we need to develop more understanding of the data-generating process that underlies the survivals and deaths on the Titanic, and by extension any dichotomous outcome in which there is a choice between two categorical possibilities. It turns out that this data-generating process is the binomial distribution. The binomial distribution The binomial distribution is a theoretical probability distribution that arises when we have some process that follows these rules: We perform \\(n\\) repeated independent trials where the result of each trial is either a success or failure. The language of “success”\" and “failure”\" here is purely aesthetic. We can use the binomial distribution in any case where there are two possible outcomes. The probability of success on each trial is given by \\(p\\). Because there are only two possible outcomes, the probability of a failure must therefore be \\(1-p\\). The binomial distribution governs how many successes we can expect to see in these \\(n\\) trials. We consider that number of successes to be a random variable and traditionally write it as \\(X\\). One of the simplest example of a binomial distribution would be to count the number of heads in a certain number of coin tosses. In this case, \\(p=0.5\\). However that example is boring so instead we will consider the basic dice mechanic from the popular Shadowrun tabletop role-playing game. In that game, players determine whether their characters succeed at a task by rolling a “dice pool” which is a certain number of six-sided die. They then count the number of 5’s or 6’s that they roll on this dice. This count then determines whether they succeed and by how much. On an evenly-weighted die, the probability of rolling 5 or 6 should be two out of six, which simplifies to one out of three, so we have a binomial distribution with \\(n\\) equal to how many dice the character has in their pool and \\(p=1/3\\). The binomial distribution formula below will tell us the probability that \\(X\\) equals some number of successes \\(k\\): \\[P(X=k)={n \\choose k}p^k(1-p)^{n-k}\\] This formula may look complex, but its actually fairly intuitive if we break down into its parts. The binomial formula basically has two parts: \\(p^k(1-p)^{n-k}\\) defines the probability of getting any particular sequence of \\(k\\) successes and \\(n-k\\) failures. \\({n \\choose k}\\) tells us the number of unique ways that we can combine \\(k\\) successes and \\(n-k\\) into a sequence. Lets start with the first part. Lets assume that our Shadowrun player had a dice pool of five dice, so \\(n=5\\). What is the probability that they rolled the following sequence: success, success, failure, failure, failure? One important feature of probabilities is that when events are independent, then the probability that they all happen is given by multiplying the individual probabilities together. In this case, the probability of a success is \\(1/3\\) and the probability of a failure is \\(2/3\\). Therefore the probability of getting that exact sequence is given by: \\[(1/3)(1/3)(2/3)(2/3)(2/3)=(1/3)^2(2/3)^3=0.033\\] I just multiply the probabilities together to get the probability of the exact sequence. Because I am multiplying the same number together, I can collect these terms together by using powers. The probability of getting this exact sequence is 0.033 or 3.3%. Note that this is also the probability of getting the sequence of success, failure, success, failure, failure because the order of the multiplication can be moved around and will still come out to \\((1/3)^2(2/3)^3\\). So, this is the probability of getting any particular sequence of two successes and three failures. I can generalize this to any \\(n\\) and \\(k\\) by just replacing the numbers with the abstract values. So: \\[p^k(1-p)^{n-k}\\] is the probability of any particular sequence of \\(k\\) successes and \\(n-k\\) failures in \\(n\\) trials. Notice that I keep saying “any particular sequence.” If we are only interested in the total number of successes, we don’t care what order they come in. However, the equation above, only gives us the probability of getting a particular order of \\(k\\) successes and \\(n-k\\) failures. To consider the total probability of \\(k\\) successes, we have to consider all the possible ways we could get a sequence giving us \\(k\\) successes. For example, to continue our example of the Shadowrun dice pool of five dice, how many ways could we combine two successes and three failures. Here are all the ways: SSFFF SFSFF SFFSF SFFFS FSSFF FSFSF FSFFS FFSSF FFSFS FFFSS There are ten possible sequences (or permutations) that would give us two successes in five trials. Each of these permutations has a probability of 0.033. To get the overall probability we need to add them up or just take \\(10*0.033=0.33\\). So, the actual probability of rolling two successes in five trials is about 33%. Thats much better than 3.3%! The lesson here is that permutations matter. How can I determine the number of possible permutations systematically? Thats what the \\({n \\choose k}\\) or “\\(n\\) choose \\(k\\)” formula answers. This formula is given by: \\[{n \\choose k}=\\frac{n!}{k!(n-k)!}\\] If you are wondering what all the exclamations points are about, its not because I am really excited (although I am). These are called factorials. A factorial indicates that a number should be multiplied by all of the descending integers down to one. So, \\(4!\\) is actually: \\[4*3*2*1\\] In practice, many of the numbers in the n choose k formula actually cancel out so it typically involves less math than you would think. Here is the n choose k formula for \\(n=5\\) and \\(k=2\\): \\[{5 \\choose 2}=\\frac{5!}{2!(5-2)!}=\\frac{5!}{2!3!}=\\frac{5*4*3*2}{2*3*2}=5*2=10\\] When we put these two parts together, we get the full binomial formula: \\[P(X=k)={n \\choose k}p^k(1-p)^{n-k}\\] Lets try it out for all the possible values in our Shadowrun dice pool: n &lt;- 5 k &lt;- 0:n p &lt;- 1/3 prob &lt;- choose(n,k)*p^k*(1-p)^(n-k) ggplot(data.frame(k,prob), aes(x=k, y=prob))+ geom_col()+ scale_y_continuous(labels=scales::percent)+ labs(x=&quot;hits (number of fives or sixes) in five dice rolls&quot;, y=&quot;probability&quot;)+ theme_bw() Figure 91: Probabilities of the number of hits (rolling a five or six) in a five dice pool We have about a one in three chance of rolling either one or two successes and about a 12% chance of getting no successes. At the other end of the spectrum, it is very unlikely to get five successes in five trials. Go ahead and pause here and play this game for yourself if you like to see how your results stack up. Al you need is five dice. Go ahead, I will wait. What would happen if we had a dice pool of twenty dice? Lets try it: n &lt;- 20 k &lt;- 0:n p &lt;- 1/3 prob &lt;- choose(n,k)*p^k*(1-p)^(n-k) ggplot(data.frame(k,prob), aes(x=k, y=prob))+ geom_col()+ scale_y_continuous(labels=scales::percent)+ labs(x=&quot;hits (number of fives or sixes) in twenty dice rolls&quot;, y=&quot;probability&quot;)+ theme_bw() Figure 92: Probabilities of the number of hits (rolling a five or six) in a twenty dice pool Of course, the most likely number of successes is higher because we are rolling more dice. The shape is also starting to look more like a normal distribution. This is not a coincidence. As \\(n\\) increases the shape of the binomial distribution will look more and more like a normal distribution. Expected value and variance The expected value of a random variable given by \\(E(X)\\) is the same as the mean of its probability distribution. In the case of the binomial distribution, the expected value is: \\[E(X)=np\\] If you think about it for a second, this value is completely intuitive. The expected number of successes is equal the probability of a success on any given trial multiplied by the number of trials. We can also calculate the variance of the random variable \\(V(X)\\). For the binomial distribution, this is given by: \\[V(X)=np(1-p)\\] This formula has an important implication. First, the variance depends on the underlying probability of success. Second, for a given \\(n\\), this probability will be maximized at a certain value of \\(p\\). To see what value of \\(p\\) that is, lets go ahead and calculate the variance for our example with \\(n=5\\) for every possible \\(p\\) at 0.01 intervals: p &lt;- seq(from=0.001,to=0.999, by=.001) v &lt;- 5*p*(1-p) ggplot(data.frame(p,v), aes(x=p, y=sqrt(v)))+ geom_line()+ labs(x=&quot;probability of success&quot;, y=&quot;standard deviation in number of successes for n=5&quot;)+ theme_bw() Figure 93: Standard deviation for binomial distribution with five trials by different probabilities of success The variance will always be at its greatest when the probability of success is 50%. As you get closer to probabilities of 0% or 100%, you will get less variance because most trials will be failures or successes, respectively. The Bernoulli distribution The Bernoulli distribution is a special case of the binomial distribution with just a single trial \\((n=1)\\). Alternatively, a binomial distribution can be thought of as the sum of \\(n\\) independent Bernoulli distributions. The Bernoulli distribution is particularly important for our purposes because each observation in our data typicallly only has one trial. The expected value of the bernoulli distribution is simply \\(p\\) and the variance is \\(p(1-p)\\). The binomial distribution as a data-generating process Let us now return to the Titanic example and consider the process that generated our actual data. In our actual data we only have a record of “successes” (i.e. survival) and “failures” (i.e. deaths). However, underlying this data, we can imagine that each passenger had their own very personal (and stressful) Bernoulli trial. Each passenger had some underlying probability of surviving the Titanic. We can refer to that underlying probability as \\(p_i\\). Importantly, it is subset by \\(i\\) because we imagine that probability was different for each passenger. Given that \\(p_i\\), each passenger was then given one bernoulli trial and the result was either survival or death. Although survival and death is what we observe, what we actually want to know about is \\(p_i\\). Furthermore, we would like to know how \\(p_i\\) was affected by other characteristics of the passenger such as gender, passenger class, age, and fare paid. In the next section, we will make our first attempt at estimating these \\(p_i\\) values in a model. "],
["linear-probability-model.html", "Linear Probability Model", " Linear Probability Model In the previous section, I said that you cannot fit a linear model by OLS regression if the dependent variable is categorical. This is mostly true, but not exactly true. I can brute-force an OLS regression model by turning a dichotomous dependent variable into numeric values of 0 (for failure) and 1 (for success). In R, I can do this by turning my dependent variable into a boolean statement. Lets try it with a model that predicts survival on the Titanic by fare paid. model_lpm &lt;- lm((survival==&quot;Survived&quot;)~fare, data=titanic) coef(model_lpm) ## (Intercept) fare ## 0.305551559 0.002296519 I now have a model that works, but what do the results mean? One way we can try to understand this model is by visualizing it on a scatterplot. ggplot(titanic, aes(x=fare, y=as.numeric(survival==&quot;Survived&quot;)))+ geom_point(alpha=0.1)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ scale_y_continuous(breaks = c(0,1), labels=c(&quot;0&quot;,&quot;1&quot;))+ labs(x=&quot;fare paid in pounds&quot;, y=&quot;Titanic survival as numeric value&quot;)+ theme_bw() Figure 94: Scatterplot of fare paid by survival on the Titanic with linear probability model fit. Points are shown with semi-transparency to address overplotting. All of the points fall on two horizontal lines at \\(y=0\\) (deaths) and \\(y=1\\) (survivors). We can sort of see the positive relationship in that the deaths are more tightly clustered around low values of fare, while the survivors are more spread out. The blue line plots the OLS regression line I just calculated above. If we were to take all the passengers at any interval, we could calculate the proportion who survived by simply taking the means of the zeros and ones for the survival variable. Lets try that for all passengers who paid between ten and twenty pounds: mean(subset(titanic, fare&gt;=10 &amp; fare&lt;=20)$survival==&quot;Survived&quot;) ## [1] 0.3793103 So about 37.9% of the passengers paying between 10 and 20 pounds survived. Figure 95 below shows this point graphically. You can see that it falls pretty close to the best-fitting line. This best-fitting line is estimating the same exact thing: the predicted proportion of survivors at a given value of fare. ggplot(titanic, aes(x=fare, y=as.numeric(survival==&quot;Survived&quot;)))+ annotate(&quot;rect&quot;, xmin = 10, xmax = 20, ymin = 0, ymax = 1, alpha = 0.2)+ geom_point(alpha=0.1)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_point(data=data.frame(fare=15, y=0.3793103), aes(y=y), color=&quot;red&quot;, size=2)+ scale_y_continuous(breaks = c(0,1), labels=c(&quot;0&quot;,&quot;1&quot;))+ labs(x=&quot;fare paid in pounds&quot;, y=&quot;Titanic survival as numeric value&quot;)+ theme_bw() Figure 95: Scatterplot of fare paid by survival on the Titanic with linear probability model fit. Red dot shows proportion of surviving passengers within the grey band. This blue line is giving us the linear probability model. Formally, the linear probability model in this case gives us: \\[\\hat{p}_i=0.3059+0.0023(fare_i)\\] The outcome, \\(\\hat{p}_i\\) is the predicted probability of survival for the \\(i\\)th passenger. When fare paid is zero, we expect that probability to be 0.3059 of 30.59%. The model predicts that each additional pound of fare paid is associated with a 0.23 percentage point increase in the probability of survival. At first glance, this model seems to give us exactly what we said we wanted from the last section – the predicted probability of survival for each passenger. However, it turns out there are two significant problems with the linear probability model that make it a less than ideal model. Heteroscedasticity The first problem is one we have seen before – heteroscedasticity. However, we will now see it in a new form. From the previous section we know that the variance of the actual survival outcome for a passenger should be given by: \\[p_i(1-p_i)\\] The problem here is that the variance of the outcome is itself a function of the value of \\(p_i\\) and \\(p_i\\) in our model will be different for each passenger. So, we have unequal variance in the residuals of our outcome and violate the assumption of identical distributions. Each passenger is reaching into their own very personal Bernoulli distribution to decide whether they live or die. This problem is theoretically correctable, via the iteratively reweighted least squares approach that we used in the previous module. In this case, we need to apply weights to our results that are the inverse of the variance for each observation. In this case, the weight for each observation should be: \\[w_i=\\frac{1}{\\hat{p}_i(1-\\hat{p}_i)}\\] Where \\(\\hat{p}_i\\) is the estimated probability from our initial model. We can then iterate through models until our estimates stop changing. Lets try it out using our initial model from above as the starting point. model_next &lt;- model_lpm for(i in 1:10) { phat &lt;- model_next$fitted.values weight &lt;- 1/(phat*(1-phat)) model_next &lt;- update(model_next, w=weight) } ## Error in lm.wfit(x, y, w, offset = offset, singular.ok = singular.ok, : missing or negative weights not allowed It looks like something went wrong. The problem is that some of the values for \\(\\hat{p}_i\\) are greater than one. You can look at the line in Figure 94 to confirm this issue. That causes problems in our approach because it makes some of the estimated weights negative. There is no particularly good approach to solving this problem. We could eliminate observations where \\(\\hat{p}_i&gt;1\\) or we could truncate those values to some value like 0.99, but if we choose 0.999 rather than 0.99, we will get different results. The fundamental issue is that we are running into the second problem with linear probability models detailed below – they can give you nonsense values for predicted probabilities outside the range of zero to one. One solution to this problem if we still want to fit this model and deal with heteroscedastictity is to apply robust standard errors as we saw in the last module: library(lmtest) library(sandwich) coeftest(model_lpm, vcov=vcovHC(model_lpm, &quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.30555156 0.01525321 20.0319 &lt; 2.2e-16 *** ## fare 0.00229652 0.00025975 8.8413 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This approach should give us correct standard errors. However, as is often the case with robust standard errors, we are getting correct standard errors for a bad model because we have not been able to address the much more important problem. Nonsense values By definition, the linear probability model fits probabilities via a straight line. The thing about straight lines is they just keep going up and up (and down and down) at a constant slope with no upper or lower limits on the values that they can take. However, a probability does have a very clear theoretical upper and lower bounds. Probabilities cannot be below zero or above one. You cannot have a -20% or 150% chance of surviving the Titanic. In some cases, you may by happenstance get a line that fits within the theoretical range for the scope of your independent variable. However, there is no guarantee of this. Figure 96 below clearly shows that we get predicted probabilities above one within the scope of the data for our linear probability model of survival on the Titanic by passenger class. Figure 96: Scatterplot of fare paid by survival on the Titanic with linear probability model fit. Points are shown with semi-transparency to address overplotting. Nonsense values above one and below zero are shaded in red. While we can fix heteroscedasticity, there is no fix for this problem within the framework of the linear probability model. The linear probability model is not a very good model because it does not respect the underlying data generation process. Logit transformation to the rescue In order to resolve this problem, we need some kind of transformation of the \\(p_i\\) values that will cause them to be contained within the interval from zero to one. It turns out that the transformation we are looking for is the logit transformation. The logit transformation converts a probability into the log-odds. Formally, \\[logit(p)=log(\\frac{p}{1-p})\\] There are really two parts to this transformation. First, we convert from probabilities to odds by taking \\(p/(1-p)\\). We have seen odds before in this course, in the section on two-way tables. There we learned how to calculate the odds ratio. The odds is the ratio of the expected number of successes to failures. So, if \\(p=0.75\\), we expect that three out of every four trials will produce successes, on average. In terms of the odds, we expect three successes for every one failure. To convert: \\[O=\\frac{p}{1-p}=\\frac{0.75}{1-0.75}=\\frac{0.75}{0.25}=3\\] Why do we convert from probabilities to odds? The advantage of the odds is that it has no upper limit. As the probability gets closer and closer to onem, the odds will approach infinity, with no finite limit. Therefore, any non-negative number for the odds can be converted back into a probability that will give sensible values between zero and one. I can convert back to a probability by: \\[p=\\frac{O}{1+O}\\] So, for the case of \\(O=3\\) above: \\[p=\\frac{3}{1+3}=\\frac{3}{4}=0.75\\] Lets choose a really high odds like \\(O=100,000\\). If we convert back to a probability: \\[p=\\frac{100000}{1+100000}=\\frac{1}{100001}=0.99999\\] We get a very high probability, but its still less than one. This partially helps us with our problems. If we were to look at a linear relationship between the odds of success and our independent variables we would get sensible probabilities no matter how high the predicted odds. However, it only partially helps us because it would still be possible to get negative odds from such a linear model which would be nonsensical. The second step of logging the odds will get us all the way there. If I log a value below one, I will get a negative value and that logged value will approach negative infinity as the original value approaches zero. So a log-odds can always be converted back to a probability that will lie between zero and one. To convert from a log-odds \\(g\\) to a probability, I take: \\[p=\\frac{e^g}{1+e^g}\\] The value \\(e^g\\) converts from log-odds to odds and then I just use the formula for converting from an odds to a probability. In essence the log-odds, or logit, transformation stretches out my probabilities across the entire number line. Lets see what this looks like graphically for a sequence of probabilities from 1% to 99%: p &lt;- seq(from=0.01, to=0.99, by=0.001) logit &lt;- log(p/(1-p)) ggplot(data.frame(p, logit), aes(x=logit, y=p))+ geom_line()+ geom_hline(yintercept = c(0, 1), linetype=2, color=&quot;red&quot;)+ labs(x=&quot;logit transformation&quot;, y=&quot;probability&quot;)+ theme_bw() Figure 97: The relationship between probability and the logit transformation The “S” curve shown here is often called the logistic curve. There are a couple of things to note about this curve: The curve approaches but never crosses the horizontal lines for \\(p=0\\) and \\(p=1\\). Thats because all finite logit values will produce probabilities within the correct theoretical range. A probability of 50% corresponds to a logit of zero. Logit values below zero indicate probabilities less than 50% and logit values above zero indicate probabilities greater than 50%. So, it seems like we now have a potential solution to our problem with the linear probability model. If we were to transform our dependent variable from predicting probabilities to predicting log-odds, we could then get predicted log-odds for each passenger on the Titanic. When converted back to probabilities, we would be ensured that our predicted probabilities never strayed beyond zero and one. That all sounds great, except there is an important catch. titanic$survived &lt;- titanic$survival==&quot;Survived&quot; model_better &lt;- lm(I(log(survived/(1-survived)))~fare, data=titanic) ## Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...): NA/NaN/Inf in &#39;y&#39; We cannot actually apply this transformation directly to our dependent variable, as we have done in the past. Why not? The problem is that we want to apply this transformation directly to \\(p_i\\) but we don’t have \\(p_i\\). We only have the actual recorded outcome as either a zero (died) or a one (survived). But there are no proper finite values of the logit at exactly zero and one. So, this approach will not work using OLS regression techniques. In order to get what we want, we need to move to a new model estimation technique – the Generalized Linear Model. "],
["generalized-linear-model.html", "Generalized Linear Model", " Generalized Linear Model Generalized linear models (GLM) will allow us to extend the basic idea of our linear model to incorporate more diverse outcomes and to specify more directly the data generating process behind our data. To better understand what GLMs do, I want to return to a particular set-up of the linear model. In this set-up, there are two equations. The first equation partitions the value of an actual outcome \\((y_i)\\) into the part accounted for by our model \\((\\hat{y}_i)\\) and the random residual “leftover” bits \\((\\epsilon_i)\\): \\[y_i=\\hat{y}_i+\\epsilon_i\\] We then have a second equation that details how the structural model part \\((\\hat{y}_i)\\) is specified by a linear function of the independent variables: \\[\\hat{y_i}=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] We should have a good sense of the second equation by now as its the basic linear model set-up. I want to focus more on the first equation right now. Its important to remember that the error terms \\((\\epsilon_i)\\) are randomly drawn from some distribution. Its not important what that distribution is so long as all of the \\(\\epsilon_i\\) are drawn from the same distribution independently. However, for the purposes of illustration lets assume that the \\(\\epsilon_i\\) are being drawn from a normal distribution. We can then describe that distribution mathematically as: \\[\\epsilon_i \\sim N(0, \\sigma)\\] The \\(\\sim\\) sign means “distributed as.” In this case, our error terms are distributed as a normal distribution that is centered on zero and has some standard deviation \\(\\sigma\\). It doesn’t really matter what that \\(\\sigma\\) is for our purposes here. Think about this from a data-generating perspective. To get an actual value of \\(y\\) for the \\(i\\)th observation: We feed all of that observation’s values for \\(x\\) into our linear function which gives us a predicted value of \\(y\\), \\(\\hat{y}_i\\). This is only the structural part. All observations with the same values of \\(x\\) will get the same \\(\\hat{y}_i\\) because we have not added the random bit. Reach into our normal distribution and pick out a residual that we add onto the end of our predicted value to get the actual value. This residual adjusts for all the random factors not accounted for in our model that might cause variation between observations with the exact same predicted value. One way of thinking about the second part is that instead of reaching into a normal distribution centered on zero for the residual, we are reaching into a normal distribution centered on \\(\\hat{y}_i\\) because all we are going to do is add the constant value of \\(\\hat{y}_i\\) to whatever we pull out of that distribution. Therefore, we can actually re-write the first equation parsing \\(y\\) into the structural and stochastic components as: \\[y_i \\sim N(\\hat{y}_i, \\sigma)\\] We can now have all the pieces to reformulate this linear model in the framework of the generalized linear model. Generalized linear model framework The generalized linear model requires two components: the error distribution and the link function. The error distribution specifies how the outcome that we actually measure in our data is distributed. In this case, the error distribution is given by: \\[y_i \\sim N(\\hat{y}_i, \\sigma)\\] Therefore, the error distribution is normal. The normal distribution is also sometimes called the **gaussian* distribution and that is how we will specify it in R below. The link function specifies how the linear function of the independent variables is related to the key parameter of the error distribution. In this case the key parameter of the error distribution is \\(\\hat{y}_i\\) and our linear function is related directly to that parameter: \\[\\hat{y_i}=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] In this case, the link function is somewhat invisible because there is not really a link function at all. We are simply relating the linear function of the independent variables directly to the key parameter of the error distribution. In practice this si called the identity link. So within the generalized linear model framework, we can express our traditional linear model as using a gaussian error distribution and an identity link. Lets go ahead and try that out. The command glm in R will estimate a generalized linear model. We will talk later in this section about how that estimation works, but for now I just want to focus on the results. summary(glm(indegree~nsports, data=popularity, family=gaussian(identity)))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.9969128 0.07221383 55.34830 0.000000e+00 ## nsports 0.5044705 0.04282271 11.78044 1.462129e-31 To specify the error distribution and link function, I used the family argument in the glm command. Lets compare this result to the traditional lm command that is estimated via OLS regression: summary(lm(indegree~nsports, data=popularity))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.9969128 0.07221383 55.34830 0.000000e+00 ## nsports 0.5044705 0.04282271 11.78044 1.462129e-31 The results are identical. Keep in mind that these two commands used completely different estimation strategies. We know that OLS regression minimizes the sum of squared residuals. The GLM uses a technique called maximum likelihood estimation that we will learn about later in this section. However, the key point is that they both produced the same estimate. When we specify the data-generating process as a guassian error distribution with an identity link, we are estimating a traditional linear model. At this point, this new framework and estimation approach hasn’t really done much for us. The lm command will work fine and we also don’t need to make the assumption that our residual terms are normally distributed for OLS regression models to be valid. So why set up this more complex framework? The answer is that by changin the error distribution and link function, we can accomodate a broad set of models that are cannot be estimated well by OLS regression techniques. Most of those models involve categorical variables. The most common model is the logit model (also called the logistic regression model) that can be used for dichotomous outcomes. A GLM for dichotomous outcomes In order to help us understand the logit model better, I want to start this in reverse. We will play god and actually generate the data. Then we can use the model to help recover the process we used to generate the data. For this example, I want to stick with the theme of ocean liner disasters, but I want to create my own disaster. To do that, we are going to use the fictional example of the Good Ship Lollipop. The Good Ship Lollipop has a large number of 10,000 passengers. We know two things about these passengers: their gender and the amount they paid in fare. To create the passengers of the Good Ship Lollipop I am going to use some handy functions in R for producing random outcomes: good_ship &lt;- data.frame(gender=sample(c(&quot;Male&quot;,&quot;Female&quot;),10000,replace=TRUE), fare=rgamma(10000, 1,0.01)) summary(good_ship) ## gender fare ## Female:5057 Min. : 0.0031 ## Male :4943 1st Qu.: 28.4345 ## Median : 69.1671 ## Mean : 99.8009 ## 3rd Qu.:137.6850 ## Max. :872.0241 The data looks pretty reasonable. Unfortunately, the Good Ship Lollipop is going to hit an iceberg and sink on its first voyage because the crew were too busy singing and drinking spiked Shirley Temples to keep an eye out. Some passengers will survive this sinking and some will not. Since I am playing god, the first thing I need to do is figure out the underlying \\(p_i\\) probability of survival for each passenger. I want \\(p_i\\) to be a function of gender and fiare paid. However, I know that its not safe or sensible to make it a direct linear function because I am end up with nonsensical values of \\(p_i\\). If I instead make the log-odds (or logit) of survival be a linear function of gender and fare paid, then I will be assured that when the log-odds are converted to probabilities, all the probabilities will fall between zero and one. So I set up my model: \\[log(\\frac{p_i}{1-p_i})=0.05-0.40(male_i)+0.005(fare_i)\\] The numbers I put in here aren’t particular important and I could vary them if I wanted to change how they related to survival. Right now, I am saying that men were less likely to survive and fare was positively associated with survival. The baseline log-odds of survival for a man who paid no fare is 0.05 which works out to a probability of 0.512 or 51.2%. Lets go ahead and feed this equation into my data to get predicted log-odds. We can then convert from those log-odds to probabilities: good_ship$log_odds &lt;- 0.05-0.4*(good_ship$gender==&quot;Male&quot;)+0.005*good_ship$fare good_ship$odds &lt;- exp(good_ship$log_odds) good_ship$probs &lt;- good_ship$odds/(1+good_ship$odds) The probs vector gives us the probability of survival for every passenger. In order to complete this process I now need to have every passenger make their Bernoulli trial to see if they survive the disaster. I can do this easily in R by using the rbinom function: good_ship$survived &lt;- rbinom(10000, 1, good_ship$probs) Lets use ggplot to visualize how this all played out: ggplot(good_ship, aes(x=fare, color=gender))+ geom_point(aes(y=survived), alpha=0.2)+ geom_line(aes(y=probs))+ labs(x=&quot;fare paid&quot;, y=&quot;probability of survival&quot;)+ theme_bw() Figure 98: Life and death on the Good Ship Lollipop. The lines show the underlying probabilities of survival by gender and survival. The dots show actual outcomes after drawing a bernoulli trial for each passenger. We can see that women were more likely to survive. This difference in survival shrank as fare paid increased because both groups started to approach the 100% probability threshold. We can also see from the dots that more women ended up in the survivor group as we would expect and that people were more likely to survive at higher fares paid. What we are seeing in Figure 98 is the data-generating process. The lines give us the underlying probabilities and the dots show us the realization of those probabilities into the ones and zeros we actually would have in the data. Can we reverse this data-generating process to recover the values I used to construct the probabilities? We can do so using a GLM approach. In this case we know the error distribution and link function. The error distribution tells us how our dependent variable is distributed. In this case, we either have a 1 (survived) or a 0 (died). Each of these values was produced by a binomial distribution with \\(n=1\\) and a probability equal to \\(p_i\\). So: \\[y_i \\sim binom(1, p_i)\\] The key parameter in this error distribution is \\(p_i\\). The link function should tell us how we relate \\(p_i\\) to the linear function of the independent variables of gender and fare. In this case, the relationship is not direct. We related the linear function of the independent variables to the log-odds or logit of the probability of survival: \\[log(\\frac{p_i}{1-p_i})=\\beta_0+\\beta_1(male_i)+\\beta_2(fare_i)\\] So to run this model we need to specify a binomial error distribution and a logit link function in a glm framework. Lets try it out: model.glm &lt;- glm(survived~gender+fare, data=good_ship, family=binomial(link=logit)) summary(model.glm)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.043243079 0.0366186671 1.180903 2.376414e-01 ## genderMale -0.405344191 0.0418497872 -9.685693 3.468482e-22 ## fare 0.005410885 0.0002589993 20.891509 6.396671e-97 It worked! Note that my coefficient estimates are very close to the actual values I used when playing god. They differ slightly because there is some inherent randomness in choosing survivors and deaths by the binomial distribution. We now have a framework for a model that predicts dichotomous outcomes. This is called the logit model or logistic regression model. In practice, it is one specification of the generalized linear model. However, we still need to understand two things. First, how are those values actually estimated in the generalized linear model? We will take that up below. Second, how do we interpret those results? We will take that up with a more thorough discussion of the logit model in the next section. Maximum likelihood estimation The technique that the generalized linear model uses to estimate parameters (i.e. intercept and slopes) is called maximum likelihood estimation. This technique is different than the technique we learned for OLS regression model which sought to minimize the sum of squared residuals, although it turns out that OLS regression technique is the maximum likelihood estimate for a simple linear model. However, we now have a more complex model that does not even have residuals in the sense that we have been using up to this point. The basic principle of maximum likelihood estimation (MLE) is fairly straightforward: We choose the parameters that maximize the likelihood of observing the data that we actually have. More formally, the process of maximum likelihood estimation operates as follows: We have some data-generating process that produces a set of observed outcomes. In the case of the Titanic of the Good Ship Lollipop, these outcomes are the vectors of zero and one that indicate death or survival. This data-generating process is governed by some unknown parameters. In our case, those unknown parameters are the intercept and slopes of the linear function in our link function. In general, we call the set of unknown parameters \\(\\theta\\) as a shorthand. Based on the data-generating process, we can construct a likelihood function, \\(L(\\theta)\\) that determines the probability of observing the data that we actually observe, assuming some \\(\\theta\\). The important thing to note about the likelihood function is that its a function of the unknown parameters \\(\\theta\\), not the actual data, which are known. We determine what values of \\(\\theta\\) would produce the maximum possible value in the likelihood function \\(L(\\theta)\\). These are the maximum likelihood estimates. As usual to find maximums of functions, we need to use calculus. In practice, we typically log the likelihood function to get the log-likelihood function and find the maximum for that function which will give us the same result. We prefer the log-likelihood function because it makes multiplicative functions additive, which simplifies the math. This is all very abstract, so lets take a simple example. Lets say I flip a coin 50 times and observe 20 heads. What is my maximum likelihood estimate of \\(p\\), the probability that we get a head on a single coin toss? Now, you may be saying to yourself, isn’t the probability of a head on a coin toss 50%? It is true that we have strong prior beliefs that this should be so. However, the principle of maximum likelihood estimation is to produce estimates purely driven by the data at hand, not prior beliefs. So we are going to act naively and estimate \\(p\\) via MLE. What is our data-generating process. We know that this is a basic binomial process. However, we are making some changes here from how we usually conceptualize this binomial process. Before, we have always known \\(p\\) and \\(n\\) (number of trials) and then considered the probability of getting \\(k\\) successes. Now, we know \\(n\\) and \\(k\\) and want to figure out \\(p\\). The good news is that our likelihood function is familiar: \\[L(p)={50 \\choose 20}p^{20}(1-p)^{30}\\] The likelihood function is just the binomial formula. Except now it is a function of \\(p\\) with the values for \\(n\\) and \\(k\\) filled in. Remember that a likelihood function gives us the probability of getting the data that we actually have. This function will give us the probability of getting 20 heads in 50 coin tosses for a given value of \\(p\\). We just need to figure out which value of \\(p\\) will actually give us the maximum possible value for \\(L(p)\\). We can do this by brute force by just using R to try every possible value of \\(p\\) from 0.001 to 0.999: p &lt;- seq(from=.001, to=.999, by=.001) likelihood &lt;- choose(50,20)*p^20*(1-p)^30 ggplot(data.frame(p, likelihood), aes(x=p, y=likelihood))+ geom_line()+ labs(y=&quot;L(p)&quot;)+ theme_bw() Figure 99: Likelihood function for p from binomial process with 50 trials and 20 successes. It is clearly maximized at p=0.4. Figure 99 shows the likelihood function for \\(p\\) calculated by hand. It is clear that the reasonable range of values of \\(p\\) are between about 0.25 and 0.55 and \\(L(p)\\) is maximized at \\(p=0.4\\). This should not be surprising. If we have a binomial process with 50 trials and 20 successes, then our best guess for the probability of a success is \\(20/50=0.4\\). Now lets try to derive the \\(p\\) that maximized \\(L(p)\\) more formally. To find the maximum of a function we need to take the derivative of that function and solve for zero1. As I noted earlier, it is usually easier to take the derivative of a log-likelihood function, so lets first log our likelihood function: \\[ \\begin{aligned} \\log L(p)&amp;=\\log({50 \\choose 20}p^{20}(1-p)^{30})\\\\ \\log L(p)&amp;=\\log {50 \\choose 20} + 20 \\log(p) + 30\\log(1-p)\\\\ \\end{aligned} \\] Now, we just need to take the derivative of this function with respect to \\(p\\). If you know calculus, this is a mildly complex and enjoyable calculation that you can try out, but for others I will tell you mathemagically, the derivative comes out to: \\[\\frac{\\partial \\log L(p)}{\\partial p}=\\frac{20}{p}-\\frac{30}{1-p}\\] We now just need to set this equal to zero on the left-hand side and solve for \\(p\\). Thats just alegbra: \\[ \\begin{aligned} 0&amp;=\\frac{20}{p}-\\frac{30}{1-p}\\\\ \\frac{30}{1-p}&amp;=\\frac{20}{p}\\\\ 30p&amp;=20(1-p)\\\\ 30p&amp;=20-20p\\\\ 50p&amp;=20\\\\ p&amp;=20/50=0.4 \\end{aligned} \\] In general, if you replace 20 and 50 with \\(n\\) and \\(k\\), then this result will give you the MLE solution of \\(p=k/n\\). MLE for GLMs The previous example is fairly simple and we are able to get a closed-form solution that is satisfying and straightforward. Maximum likelihood estimation for GLMs is not so easy. Lets consider our case for the logit model. Each observation gets a bernoulli trial to determine success where the outcome of 0 or 1 is recorded at \\(y_i\\). Therefore, the likelihood \\(L_i\\) for a single observation is given by: \\[L_i=p_i^{y_i}(1-p_i)^{1-y_i}\\] This formula may look complicated, but its fairly straightforward. If the passenger survived \\((y_i=1)\\), then this formula just simplifies to \\(p_i\\),the probability of survival for that passenger. If the person did not survive \\((y_i=0)\\), then this formula just simplifies to \\((1-p_i)\\), the probabiility of not surviving for that passenger. As usual, lets take the log of that likelihood to get the log-likelihood: \\[\\log L_i=y_i\\log(p_i)+(1-y_i) \\log (1-p_i)\\] To get the log-likelihood of all observations, we just need to add up these log-likelihoods. Why do we add them? On the original scale of likelihoods, we would multiply them together, so on the log-scale we add them. \\[\\log L= \\sum_{i=1}^n \\log L_i= \\sum_{i=1}^n y_i\\log(p_i)+(1-y_i) \\log (1-p_i)\\] Now, the only problem we have at the moment is that our goal is to maximize the slopes and intercept of our linear model but we don’t have those in the formulat the moment. However, we can add that by remembering that we were predicting the log-odds of success as a linear function of the independent variables: \\[log(\\frac{p_i}{1-p_i})=\\mathbf{x_i&#39;\\beta}\\] I am writing the linear function in matrix notation here \\((\\mathbf{x_i&#39;\\beta})\\) for brevity. We can convert this back to \\(p\\) by using the formula for converting from log-odds to probabilities: \\[{p}_i=\\frac{e^{\\mathbf{x_i&#39;\\beta}}}{1+e^{\\mathbf{x_i&#39;\\beta}}}\\] We can then plug this into our log-likelihood function for \\(p_i\\): \\[\\log L(\\beta) = \\sum_{i=1}^n y_i\\log(\\frac{e^{\\mathbf{x_i&#39;\\beta}}}{1+e^{\\mathbf{x_i&#39;\\beta}}})+(1-y_i) \\log (1-\\frac{e^{\\mathbf{x_i&#39;\\beta}}}{1+e^{\\mathbf{x_i&#39;\\beta}}})\\] We now have the full log-likelihood function for a logit model. To figure out \\(\\beta\\) we just have the rather difficult task of finding the \\(\\beta\\) values that maximize this function. Finding the MLE for this log-likelihood function is no easy task and there are no closed-form solutions as there were for the simple binomial process case. We need to use iterative estimation procedures to estimate the best values for \\(\\beta\\). Several algorithms have been developed for finding MLE for this case and for other forms of the GLMs. The technique that I will show you below uses a version of iteratively-reweighted least squares (IRLS). You will never typically have to do this as the glm command will do all of the heavy-lifting here, but its worthwhile to have some familiarity with how glm is doing what it does. I will show you below how this procedure works using the case of the Titanic where we predict survival by fare paid. Lets first set up our outcome vector of zeros and ones: y &lt;- as.vector(as.numeric(titanic$survival==&quot;Survived&quot;)) head(y) ## [1] 1 1 0 0 0 1 Beause we will also be using matrix notation, I am also going to set up my design matrix of independent variables, taking consideration to add a column of ones for my intercept. X &lt;- as.matrix(cbind(rep(1,nrow(titanic)), titanic[,&quot;fare&quot;])) head(X) ## [,1] [,2] ## [1,] 1 211.3375 ## [2,] 1 151.5500 ## [3,] 1 151.5500 ## [4,] 1 151.5500 ## [5,] 1 151.5500 ## [6,] 1 26.5500 The basic IRLS procedure is to estimate via an iterative procedure. To get the process started we first fit the null model as our logit model. The null model just assumes that every observation had an equal probability of survival. mean(y) ## [1] 0.381971 #convert to log-odds lodds &lt;- log(mean(y)/(1-mean(y))) lodds ## [1] -0.4811908 Based on the proportion of survivors, the log-odds of survival as a whole was -0.4811908. So my null model is: \\[\\log(\\frac{p_i}{1-p_i})=-0.4811908\\] Lets set this up as my initial \\(\\beta\\) vector: beta &lt;- c(lodds, 0) I can matrix-multiply this vector by my design matrix to get predicted log-odds which I can convert to \\(p\\). At this point, they all should give me the same value. pred_lodds &lt;- X%*%beta p &lt;- exp(pred_lodds)/(1+exp(pred_lodds)) head(p) ## [,1] ## [1,] 0.381971 ## [2,] 0.381971 ## [3,] 0.381971 ## [4,] 0.381971 ## [5,] 0.381971 ## [6,] 0.381971 I can also use that p vector to calculate my log-likelihood for the current model: logL &lt;- sum(y*log(p)+(1-y)*log(1-p)) logL ## [1] -870.5122 Now I want to iterate this process and improve my estimation. At each iteration \\(t\\) of the process, I estimate the best-fitting \\(\\beta\\) vector for \\(t+1\\) by the following formula: \\[\\beta^{(t+1)}=\\mathbf{(X&#39;W^{(t)}X)^{-1}X&#39;W^{(t)}z^{(t)}}\\] This formula is somewhat similar to the OLS regression formula we saw in the previous module. We can see the design matrix \\(\\mathbf{X}\\) of independent variables. We also have a \\(\\mathbf{X}\\) weighting matrix to consider. You will also notice a \\(z\\) vector where we would usually have a \\(y\\) vector. The weighting matrix only has non-zero values along the diagonal. These values are given by: \\[w_i=\\hat{p}_i(1-\\hat{p}_i)\\] Where \\(hat{p}_i\\) is estimated probability of success for observation \\(i\\) from the current iteration of the model. I can estimate this weighting matrix from my initial model as follows: W &lt;- matrix(0, nrow(X), nrow(X)) diag(W) &lt;- p*(1-p) The \\(z\\) vector accounts for the transformation of the dependent variable and is estimated as: \\[z_i=\\mathbf{x_i&#39;\\beta}+\\frac{y_i-\\hat{p}_i}{\\hat{p}_i(1-\\hat{p}_i)}\\] I can calculate this from my null model as: z &lt;- pred_lodds + (y-p)/(p*(1-p)) I now have all the pieces for my first iteration: beta &lt;- solve(t(X)%*%W%*%X)%*%(t(X)%*%W%*%z) beta ## [,1] ## [1,] -0.804907036 ## [2,] 0.009728163 pred_lodds &lt;- X%*%beta p &lt;- exp(pred_lodds)/(1+exp(pred_lodds)) logL &lt;- sum(y*log(p)+(1-y)*log(1-p)) logL ## [1] -828.965 You can see that I now have a non-zero slope for fare paid and a less negative log-likelihood which means a higher overall likelihood. I can keep iterating this procedure until my \\(\\beta\\) values stop moving around. Lets try a total of six iterations from the top: #initial null model lodds &lt;- log(mean(y)/(1-mean(y))) beta &lt;- c(lodds, 0) pred_lodds &lt;- X%*%beta p &lt;- exp(pred_lodds)/(1+exp(pred_lodds)) logL &lt;- sum(y*log(p)+(1-y)*log(1-p)) beta.prev &lt;- beta for(i in 1:6) { w &lt;- p*(1-p) z &lt;- pred_lodds + (y-p)/w W &lt;- matrix(0, nrow(X), nrow(X)) diag(W) &lt;- w beta &lt;- solve(t(X)%*%W%*%X)%*%(t(X)%*%W%*%z) beta.prev &lt;- cbind(beta.prev, beta) pred_lodds &lt;- X%*%beta p &lt;- exp(pred_lodds)/(1+exp(pred_lodds)) logL &lt;- c(logL, sum(y*log(p)+(1-y)*log(1-p))) } beta.prev ## beta.prev ## [1,] -0.4811908 -0.804907036 -0.87722472 -0.88394804 -0.88402353 -0.88402354 ## [2,] 0.0000000 0.009728163 0.01218691 0.01246678 0.01247006 0.01247006 ## ## [1,] -0.88402354 ## [2,] 0.01247006 logL ## [1] -870.5122 -828.9650 -827.4084 -827.3924 -827.3924 -827.3924 -827.3924 By the sixth iteration, my intercept and slope values have stopped cahnging down to the eighth decimal place and the log-likelihood has also stabilized. Lets compare our results to the glm command to see how they compare: model.survival &lt;- glm((survival==&quot;Survived&quot;)~fare, data=titanic, family=binomial(logit), control=glm.control(trace=TRUE)) ## Deviance = 1658.387 Iterations - 1 ## Deviance = 1654.791 Iterations - 2 ## Deviance = 1654.785 Iterations - 3 ## Deviance = 1654.785 Iterations - 4 coef(model.survival) ## (Intercept) fare ## -0.88402354 0.01247006 We get the exact same values from the glm command. Technically we also need to show that the second derivative is negative at this value of \\(p\\), but we will skip that step here. You can try it as an exercise at home if you are so inclined↩ "],
["logit-model.html", "Logit Model", " Logit Model Now that we have developed the logic behind generalized linear models and the maximum likelihood estimation procedure, we can focus more concretely on the GLM we used to predict survival in the previous section. This GLM is the logit model (also called the logistic regression model) and it can be used to predict dichotomous outcomes. Formally: \\[y_i \\sim binom(1, \\hat{p}_i)\\] \\[log(\\frac{\\hat{p}_i}{1-\\hat{p}_i}) = \\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] These two functions define the error distribution and link function, respectively. Each outcome \\(y_i\\) (success or failure) is the result of a binomial distribution with a single trial and an estimated \\(\\hat{p}_i\\) probability of success for the \\(i\\)th observation. The linear function of the independent variables is related to the log-odds of success, which is a transformation of those \\(\\hat{p}_i\\). We use MLE to then estimate the \\(\\beta\\) values by selecting the \\(\\beta\\) values that produce \\(\\hat{p}_i\\) that maximize the likelihood of actually observing the success and failures \\((y_i)\\) that we actually have. Interpreting results Because of the transformation in the link function, we have to be careful about interpreting the results from our model. We have already seen something like this before in the previous module in which we log-transformed the dependent variable. The effect here is similiar, but is also compounded by the use of odds rather than probabilities in the link function. In short, we have to consider two things: We are modeling differences or effects on the odds of success rather than on the probability of success. Because of the non-linear connection between these two concepts, it is no easy matter to go between them. We will take this more up below in a discussion of the “marginal effects” from logit models. Because of the natural log in the logit transformation, the actual model we care about is multiplicative rather than additive. In other words, our ultimate interpretations will be about relative change (e.g. 7% lower, twice as large) on the odds of success. A simple example Lets try out a simple example. I want to use my new logit model framework to predict survival on the Titanic by gender. model.gender &lt;- glm((survival==&quot;Survived&quot;)~(sex==&quot;Female&quot;), data=titanic, family=binomial(logit)) round(summary(model.gender)$coef,3) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.444 0.088 -16.476 0 ## sex == &quot;Female&quot;TRUE 2.425 0.136 17.832 0 My model is given by: \\[log(\\frac{\\hat{p}_i}{1-\\hat{p}_i}) = -1.444+2.425(female_i)\\] But what do the values for the intercept and the slope really mean? These are literally given me predicted changes in the log odds of survival, but log odds is not very intuitive. Lets use our usual trick of exponentiating both sides to convert the left-hand side to and odds. \\[\\begin{align*} e^{log(\\frac{\\hat{p}_i}{1-\\hat{p}_i})} &amp; = e^{-1.444+2.425(female_i)}\\\\ \\frac{\\hat{p}_i}{1-\\hat{p}_i} &amp;= (e^{-1.444})(e^{2.425(female_i)})\\\\ \\frac{\\hat{p}_i}{1-\\hat{p}_i} &amp;= (0.236)(11.30)^{(female_i)} \\end{align*}\\] As you can see, this equation became a multiplicative equation rather than an additive equation, just as it did for log-transformed quantitative variables in the previous module. In this case, we can figure out what these numbers mean very easily by plugging in values for \\(female_i\\) for our two cases. For men, \\(female_i=0\\), so: \\[\\frac{\\hat{p}_i}{1-\\hat{p}_i} = (0.236)(11.30)^{0}=0.236(1)=0.236\\] So the exponential of the intercept value \\((e^{-1.444}=0.236)\\) gives us the odds of survival for men. This value is often called the baseline odds and represents the odds of success for an observation when all values of the independent variable are zero. For women, \\(female_i=1\\), so: \\[\\frac{\\hat{p}_i}{1-\\hat{p}_i} = (0.236)(11.30)^{1}=0.236(11.30)\\] We could work out the final number here, but this formulation is more important because it gives us the odds ratio of 11.3. This is the value we multiply the baseline odds by in order to get the odds for women. In this case, we can see that women had odds of surviving the Titanic that were 11.3 times higher than men’s odds. In general the exponential of the slopes gives us an odds ratio, although interpreting it correctly depends on whether you have a categorical or quantitative independent variable. Before discussing the quantitative case, lets revisit a method we already knew for calculating this odds ratio. We can do this by a two-way table. table(titanic$sex, titanic$survival) ## ## Survived Died ## Female 339 127 ## Male 161 682 The odds of survival for men are given by the number of survivals over the number of deaths: 161/682 ## [1] 0.2360704 You can see that this number matches the baseline odds we just calculated from the logit model. We can also calculate the odds ratio by taking the cross-product of the table: 339*682/(161*127) ## [1] 11.30718 The odds ratio between men and women is the same as what we just estimated from the logit model. The logit model and the two-way table both agree on the relevant odds and odds ratios. So why use the logit model? The logit model will allow us to estimate much more complex models by including quantitative variables, controlling for other variables, adding interaction terms, non-linear effects, and all of the other fun techniques we have been developing for the right-hand side of a linear function. Before turning to a more complex model, lets look at how to interpret logit models with a quantitative independent variable. Interpreting results with quantitative independent variables Lets return to the example of predicting survival by fare paid: model.fare &lt;- glm((survival==&quot;Survived&quot;)~fare, data=titanic, family=binomial(logit)) coef(model.fare) ## (Intercept) fare ## -0.88402354 0.01247006 So our model is: \\[log(\\frac{\\hat{p}_i}{1-\\hat{p}_i}) = -0.884+0.012(fare_i)\\] We can follow the same process as before and convert this to the odds scale by exponentiating both sides: \\[\\begin{align*} e^{log(\\frac{\\hat{p}_i}{1-\\hat{p}_i})} &amp; = e^{-0.884+0.012(fare_i)}\\\\ \\frac{\\hat{p}_i}{1-\\hat{p}_i} &amp;= (e^{-0.884})(e^{0.012(fare_i)})\\\\ \\frac{\\hat{p}_i}{1-\\hat{p}_i} &amp;= (0.413)(1.012)^{(fare_i)} \\end{align*}\\] Again we have a multiplicative model. To see how we can interpret this multiplicative effect, lets consider the predicted odds for three passengers who respectivly paid 0,1, and 2 pound(s) in fare: \\[\\begin{align*} \\frac{\\hat{p}_i}{1-\\hat{p}_i}&amp;=(0.413)(1.012)^{0}=0.413\\\\ \\frac{\\hat{p}_i}{1-\\hat{p}_i}&amp;=(0.413)(1.012)^{1}=0.413(1.012)\\\\ \\frac{\\hat{p}_i}{1-\\hat{p}_i}&amp;=(0.413)(1.012)^{2}=0.413(1.012)(1.012)\\\\ \\end{align*}\\] Each time we increase a pound in fare, we multiply the odds we had before by 1.012. In other words, the odds increase by 1.2% for every one pount increase in fare. So, for a quantitative variable, we can also talk about the percentage or multiplicative change in the odds, but this time for a one unit increase in \\(x\\) rather than comparing a one category to a reference category. Also note that because our slope of 0.012 was small, the exponential of this value followed the approximation of \\(e^x\\approx1+x\\), and was simply 1.012. Therefore, in this case, we could have just looked directly at the log odds ratio of 0.012 and concluded that every one pound increase in fare paid is associated with a 1.2% increase in the odds of survival. Graphing predicted probabilities We can look at how fare changes the predicted probability of survival by using the predict command. In this case, we will feed in a fake dataset of passengers paying every fare between 0 and 512 (the maximum actual value) in one unit increments. We just have to remember that the predicted values from predict will be log-odds which need to be converted to probabilities. predict_df &lt;- data.frame(fare=0:512) lodds &lt;- predict(model.fare, newdata=predict_df) predict_df$probs &lt;- exp(lodds)/(1+exp(lodds)) ggplot(predict_df, aes(x=fare, y=probs))+ geom_line()+ labs(x=&quot;fare paid&quot;, y=&quot;probability of survival&quot;)+ theme_bw() We could also plot a more complicated example for a model that looks at both gender and fare. The trick here is we can use a handy function called expand.grid to set up our prediction dataframe. The expand.grid function will create a dataframe that includes an observation for every possible combination of the variables that you feed in. model.both &lt;- update(model.fare, .~.+sex) predict_df &lt;- expand.grid(fare=0:512, sex=c(&quot;Female&quot;,&quot;Male&quot;)) lodds &lt;- predict(model.both, newdata=predict_df) predict_df$probs &lt;- exp(lodds)/(1+exp(lodds)) ggplot(predict_df, aes(x=fare, y=probs, color=sex))+ geom_line()+ labs(x=&quot;fare paid&quot;, y=&quot;probability of survival&quot;)+ theme_bw() A more complex example Now lets try a more complex example that can take advantage of several of the features of linear models we have already developed. model.smoking &lt;- glm(smoker~I(pseudoGPA-2)+I(grade-10)+I((grade-10)^2)+sex*honorsociety, data=popularity, family=binomial(logit)) round(summary(model.smoking)$coef, 3) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.086 0.086 -12.697 0.000 ## I(pseudoGPA - 2) -0.631 0.062 -10.173 0.000 ## I(grade - 10) 0.299 0.030 9.914 0.000 ## I((grade - 10)^2) -0.013 0.019 -0.715 0.475 ## sex Male -0.133 0.091 -1.471 0.141 ## honorsocietyTRUE -0.741 0.265 -2.801 0.005 ## sex Male:honorsocietyTRUE 0.845 0.372 2.272 0.023 What all am I doing in this model. I am predicting whether an adolescent is a frequent smoker by: GPA in school. I have centered this on a C average. Grade (e.g. 8th, 9th) in school. I have centered this on 10th grade and allowed for a non-linear trend by including a polynomial squared term. gender and honor society membership. I have interacted these two variables so that honor society can have a different effect for boys and girls. In order to interpret the results, I need to exponentiate my coefficients: exp(coef(model.smoking)) ## (Intercept) I(pseudoGPA - 2) I(grade - 10) ## 0.3376500 0.5321197 1.3490978 ## I((grade - 10)^2) sex Male honorsocietyTRUE ## 0.9867364 0.8752565 0.4765042 ## sex Male:honorsocietyTRUE ## 2.3275203 The baseline odds of frequent smoking for a 10th grade girl who earned a C average and is not a member of honor society is about 0.34. So roughly, I expect to see 3 non-frequent smokers for every frequent smokers among this group. Holding constant gender, grade in school, and honor society membership, each letter grade increase in GPA is associated with nearly a halving (47% decline) in the odds of frequent smoking. The grade effects are a little harder to directly interpret because of the squared term applied in a multiplicative fashion. The basic trend is that increases in grade are associated with fairly large increases in the odds of frequent smoking. The negative effect on the squared term suggests a slightly diminishing effect at higher grade levels, but this is not statistically significant, so we are probably better off estimating a model without the squared term. To see how this complex age effect works, its probably easiest to calculate odds ratios at each of the grades from 7th through 12th: grades &lt;- 7:12 odds &lt;- exp(-1.086+0.299*(grades-10)-0.013*((grades-10)^2)) odds[2:6]/odds[1:5] ## [1] 1.439074 1.402141 1.366155 1.331092 1.296930 We can see that the biggest odds ratio increase is from 7th to 8th grade at 43.9% and the smallest estimated odds ratio increase is from 11th to 12th at 29.6%. Although the magnitudes vary in size, the risk of smoking increases substantially in size with increases in grade at all grade levels. The effects of gender and honor society are also somewhat more complex to interpret because of the interaction term. The main effects give us the effect when the other variable is held at the reference. So, among students who are not in honor society, and controlling for grade and GPA, boys have 12.5% lower odds \\((100*(1-0.875))\\) of frequently smoking than girls. Among girls, honor society membership reduces the odds of smoking by about 52% \\((100*(1-0.48))\\). To interpret the interaction term, lets go ahead and calculate the honor society effect for boys: exp(-0.741+0.845) ## [1] 1.1096 Among boys, those in honor society have 11% higher odds of smoking frequently than those not in honor society. So we see a very different effect of honor society here by gender. Notice that I could have gotten this same value by multiplying the exponentiated values of the main effect and interaction term: 0.4765042*2.3275203 ## [1] 1.109073 Marginal effects The coefficients of the logit model give the marginal effect on the log-odds of success, but we rarely want this value. Exponentiated coefficients will give the odds ratio of success for a one unit increase in \\(x\\) for all values of \\(x\\). Often times, however, what people really want when they refer to the “marginal effects” in a logit model is the marginal effect of \\(x\\) on the probability. Before proceeding with such a calculation, it is worth pausing for thought here. We converted to the odds scale because of the fundamental problem of probabilities being bound between zero and one. Because of this, the effect of an independent variable must be smaller when we are close to this bounds. This has the unfortunate effect of mixing up “intecept effects” with relationships between variables when we focus on probabilities. An “intercept effect” is something that affects all the cases and raises or lowers their probability of success. For example, the Costa Concordia sinking off the coast of Italy had a much lower death rate than the Titanic for a variety of reasons (e.g. modern safeguards, temperature of the water, proximity to shore). Thes factors raised everyone’s probability of surviving much higher, such that differences between passengers by gender had to be smaller. However, we should not conclude from this that the smaller difference between men and women’s survival was driven by anything to do with gender itself, but rather to the ceiling effect of approaching 100% survival. The odds and odds ratio gets us away from this problem, by removing the “intercept effect” from our estimates of the effect of a given independent variable on the outcome. We can directly compare the odds of survival by gender between the Titanic and Costa Concordia, but not the probabilities. Nonetheless, recently many academics have become convinced that the only proper way to report parameters from a logit model is to convert them to marginal effects on the probabilities. I am not a fan of this Logit Panic, but I do think its worthwhile for you to know how to do this procedure. For more information on why I think concerns about the logit model are misplaced, see this excellent article. The first problem with calculating marginal effects on the probabilities is the question of where you want to calculate them. Because the relationship between the log-odds and probabilities is a curve (the logic curve, to be precise), the marginal effects vary depending on the predicted probability of success, \\(\\hat{p}_i\\) for a given \\(x\\) or vector of \\(X\\) in the case of multiple independent variables. Lets take the example of the effect of fare paid on surviving the Titanic. Figure @(fig:plot-marginal) below shows three different marginal effects at fares of 50, 200, and 500 pounds. These marginal effects are given by the tangent line to the curve at each of these values of fare. The tangent line is the line that just “kisses” the curve at each value of fare, and gives us the slope of the curve at that exact moment. We can see that the steepness of the curve changes. As fare increases, the probability of surviving also increases and thus the marginal effect of fare decreases because we are approaching one. Figure 100: Different marginal effects of a one pound increase in fare paid at fares of 50, 200, and 500 pounds. Marginal effects are given by the tangent line to the curve at a given value of fare. To calculate the marginal effect of a given variable \\(k\\) with a coefficient of \\(\\beta_k\\), you just need to use the formula: \\[\\hat{p}(1-\\hat{p})\\beta_k\\] where \\(\\hat{p}\\) is the predicted probability of success for a given set of values for the independent variables. We can calculate that value by using our formula to get predicted probabilities for a given vector \\(X\\) of values: \\[\\hat{p}=\\frac{e^\\mathbf{x&#39;\\beta}}{1+e^{\\mathbf{x&#39;\\beta}}}\\] For example, lets calculate the marginal effect of paying an additional pound of fare when we are currently paying fifty pounds. Our model of survival by fare paid is given by: \\[log(\\frac{\\hat{p}_i}{1-\\hat{p}_i}) = -0.884+0.012(fare_i)\\] First, we plug in 50 for fare to get our predicted log-odds of survival: \\[-0.884+0.012*50=-0.284\\] Now we convert from log-odds to probabilities: \\[\\hat{p}=\\frac{e^{-0.284}}{1+e^{-0.284}}=0.429\\] We can now calculate the marginal effect: \\[\\hat{p}(1-\\hat{p})\\beta_k=0.429(1-0.429)0.012=0.0029\\] So, an increase in fare paid from fifty to fifty-one pounds is associated with an increase in the probability of survival of 0.29%. We can also do this with models that have more than one variable, but now we will have to estimate a \\(\\hat{p}\\) for values of more than one variable. The standard approach in these cases is to estimate the marginal effects on ther probability when at the mean for all independent variables. Lets try it out for a model that uses both fare and age to predict survival. model &lt;- glm((survival==&quot;Survived&quot;)~fare+age, data=titanic, family=binomial(logit)) #get predicted probabilty at mean fare and age df &lt;- data.frame(fare=mean(titanic$fare), age=mean(titanic$age)) lodds &lt;- predict(model, df) p &lt;- exp(lodds)/(1+exp(lodds)) #get marginal effects p*(1-p)*coef(model)[c(&quot;fare&quot;,&quot;age&quot;)] ## fare age ## 0.003197334 -0.003233789 So, holding age at the mean, a one pound increase in fare from the mean is associated with an increase in the the probability of survival of 0.32%. Holding fare at the mean, a one year increase in age is associated with a decrease in the probability of survival of 0.32%. Coincidentally, the marginal effects of fare and age at the mean are about the same but in opposite directions. For categorical variables, the marginal effect on the probability is simply given by the difference in probability between the indicated and reference category. Lets add gender to our model and estimate its marginal effect on the probability. titanic$female &lt;- as.numeric(titanic$sex==&quot;Female&quot;) model &lt;- glm(survival~fare+age+female, data=titanic, family=binomial(logit)) df &lt;- data.frame(fare=rep(mean(titanic$fare),2), age =rep(mean(titanic$age),2), female=c(0, 1)) lodds &lt;- predict(model, df) p &lt;- exp(lodds)/(1+exp(lodds)) p ## 1 2 ## 0.8025075 0.2841702 diff(p) ## 2 ## -0.5183373 Men had a 52% lower probability of surviving the Titanic than women, holding fare and age at the mean values. How do we calculate the marginal effects for fare and age in this last model. Gender is categorical and therefore we can’t just take the mean. Or can we? You may have noticed that I created a new variable called female coded directly as a 0/1 dummy variable. The mean of this variable is the proportion female on the Titanic. Although it would seem odd to do so, I can use this mean to get a predicted value for the probability mathematically. The result just tells me the expected log-odds/probabilities among a set of passengers where the proportion female equals that for the whole dataset. I can then use this approach to calculate the marginal effects for my quantitative variables. df &lt;- data.frame(fare=mean(titanic$fare), age=mean(titanic$age), female=mean(titanic$female)) lodds &lt;- predict(model, df) p &lt;- exp(lodds)/(1+exp(lodds)) p*(1-p)*coef(model)[c(&quot;fare&quot;,&quot;age&quot;)] ## fare age ## -0.002310890 0.002089346 If you don’t want to go through the hard work of calculating these marginal effects by hand, you are in luck. The mfx library has a command called logitmfx. This command will run a logit model and report back the marginal effects on the probability. library(mfx) logitmfx(survival~fare+age+sex, data=titanic) ## Call: ## logitmfx(formula = survival ~ fare + age + sex, data = titanic) ## ## Marginal Effects: ## dF/dx Std. Err. z P&gt;|z| ## fare -0.00231089 0.00039964 -5.7824 7.365e-09 *** ## age 0.00208935 0.00115590 1.8075 0.07068 . ## sex2 0.51833735 0.02577096 20.1132 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## dF/dx is for discrete change for the following variables: ## ## [1] &quot;sex2&quot; Notice that this function also reports back inferential statistics in addition to the marginal effects themselves. The probit link The logit link is the most common link used for generalized linear models with dichotomous outcomes. However, another link option is the probit link. Formally the probit link is the inverse of the cumulative normal distribution, but its ok if that doesn’t make a lot of sense to you. Just imagine that you had a standard normal distribution with a mean of zero and standard deviation of one. The cumulative normal distribution gives us the density (proportion of the area) of that standard normal distribution for a given value. So imagine you put in a value of one. A value of one would correspond to being one standard deviation above the mean. Figure 101: Standard normal distribution with area to the right of one standard deviation above the mean shaded. Figure 101 shows what that area is in the shaded region. 84% of the area of a standard normal distribution is below that one standard deviation mark. The inverse cumulative normal distribution just reverses which number we are asking about. Instead of feeding in “+1 SD” to get 0.84, I feed in 0.84 to get “+1 SD.” The idea is that we can represent any given probability as an abstract score on a continuous distribution that ranges from negative infinity to infinity. So, just like the logit transformation. the probit transformation stretches the probability across the full number line. To use a probit model, just replace the logit link with the probit link in your glm command: model.probit &lt;- glm((survival==&quot;Survived&quot;)~fare, data=titanic, family=binomial(probit)) summary(model.probit)$coef ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.538440000 0.0452344872 -11.903307 1.137481e-32 ## fare 0.007202863 0.0009081451 7.931401 2.166876e-15 The downside of the probit model is that the results are much more difficult to interpret. The direction of the effect can be determined, but the magnitude is often difficult to gauge. For this reason, the logit model is usually preferred. The two link functions usually produce very similar results. Figure 102 compares the probabilty fit of the two types of models when predicting survival by fare paired. The results are nearly identical. Figure 102: Predicted probabilities of survival on the Titanic by fare paid using a logit and a probit model Assessing model fit For a linear model, we typically assess goodness of fit by the \\(R^2\\) value for the model which gives us the proportion of the variance in the dependent variable that can be accounted for by the independent variables. However, once we move to a logit model, we no longer have an \\(R^2\\) value. This is because we are not actually estimating \\(\\hat{y}_i\\), the predicted values of \\(y_i\\), which we use to calculate residuals and \\(R^2\\). Rather, we are calculating the underlying log-odds and probabilities of success for the data-generating process. However, the maximum likelihood estimation (MLE) procedure used for GLMs gives us another measure of goodness of fit. Remember that MLE selects the model parameters that maximize the likelihood of observing the data that we actually observe. The likelihood associated with the best-fitting model is itself a measure of goodness of fit. However, we don’t typically use the likelihood itself but rather a derived measure called deviance. The deviance \\((G^2)\\) of a generalized linear model is given by: \\[G^2=-2 (logL)\\] Where \\(logL\\) is the log-likelihood of the model. We basically multiply this value by -2. Why we choose -2 is a bit complicated and we won’t delve into it here, besides saying that it allows for some other mathematical properties that just work. Remember that the log-likelihood is the log of a probability. Because probabilities are between zero and one, the log of the likelihood will always be a negative number. The more negative the log-likelihood, the closer the likelihood to zero and therefore the poorer the fit of the best-fitting model. When we multiply this value by a negative value to get the deviance, the directionality reverses. A high deviance indicates worse fit – think of it like a golf score. Lower numbers are better, which intuitively makes sense of something called “deviance.” The glm object in R gives us the deviance directly: model.fare$deviance ## [1] 1654.785 The value of the deviance by itself is usually not that interesting. What we are interested in is the value of this deviance in comparison to other models of the same outcome. The implicit comparison is always to the null model. Luckily, the glm object also gives us the deviance of this null model, so we can directly compare them. model.fare$null.deviance ## [1] 1741.024 model.fare$null.deviance-model.fare$deviance ## [1] 86.23961 Our model using fare as a predictor of survival reduced the deviance from the null model (where everyone had the same probability of survival) by 86.2. Is that a good number? There are several approaches to gauging the magnitude of that value. Likelihood Ratio Test The Likelihood Ratio Test (LRT) is analagous to the F-test for linear models. It can be used to compare any two nested models where the null hypothesis is that all additional independent variables in the more complex model have a slope of zero. The LRT is tested by using the reduction in deviance in the more complex model as the test statistic. If the null hypothesis is true, then this test statistic should come from a chi-squared distribution with degrees of freedom equal to the number of additional variables in the more complex model. The chi-squared distribution has a somewhat similar shape to an F-distribution and won’t delve into the details here. We simply look for the area in the right tail of this chi-squared distribution to calculate the p-value. The pchisq function in R will give us this left-tail area and so we can just subtract this from one to get the right-tail area. Lets try it out by running an LRT of our fare model versus the null model. 1-pchisq(86.2,1) ## [1] 0 The reduction in deviance is so large that our p-value is very close to zero. We reject the null hypothesis in this case. You can also use the anova command to do an LRT but you have to remember to specify test=\"LRT\" as an argument: anova(model.fare, test=&quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: (survival == &quot;Survived&quot;) ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 1308 1741.0 ## fare 1 86.24 1307 1654.8 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Just like the F-test, we can compare any two nested models using the LRT. Lets try a model with passenger class and then one with passenger class and fare. model.pclass &lt;- glm((survival==&quot;Survived&quot;)~pclass, data=titanic, family=binomial(logit)) model.complex &lt;- update(model.pclass, .~.+fare) anova(model.pclass, model.complex, test=&quot;LRT&quot;) ## Analysis of Deviance Table ## ## Model 1: (survival == &quot;Survived&quot;) ~ pclass ## Model 2: (survival == &quot;Survived&quot;) ~ pclass + fare ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 1306 1613.3 ## 2 1305 1602.3 1 10.916 0.0009535 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The model with fare added reduces deviance by an additional 10.9 over the model just with passenger class. The p-value for this improvement is not quite as good as the last one, but is still very low and we would surely reject the null hypothesis. Keep in mind, that the LRT has all the same drawbacks as the F-test. It is based on the logic of hypothesis testing and therefore it will prefer more complex models in larger samples. Pseudo-\\(R^2\\) Pseudo-\\(R^2\\), also called McFadden’s D provides a very simple measure of goodness of fit. It simply calculates the reduction in deviance as a proportion of the null deviance. To calulcate pseudo-\\(R^2\\): \\(D=\\frac{G^2_0-G^2}{G^2}\\)$ where \\(G^2_0\\) is the deviance of the null model. Although not the same as \\(R^2\\), the idea is quite similar. The null deviance is as large as your deviance can get, so to ask how much we reduced the deviance, it makes intuitive sense to place it on a scale of how much deviance there was that could be accounted for in the first place. To calculate the reduction in deviance for our fare model: (model.fare$null.deviance-model.fare$deviance)/model.fare$deviance ## [1] 0.05211531 Using fare to predict the likelihood of survival reduces the deviance from the null model by about 5 percent. BIC for GLMs We can also calculate a BIC for generalized linear models just as we did for linear models. Because we have no \\(R^2\\) value the calculatioon is different, but the interpretations are the same. To calculate BIC for any two models: \\[BIC=(G^2_2-G^2_1)+(p_2-p_1)\\log n\\] Where \\(G^2_1\\) and \\(G^2_2\\) are the deviance for model 1 and model 2, respectively, and \\(p_1\\) and \\(p_2\\) are the number of independent variables in models 1 and 2, respectively. The resulting BIC gives you the relative preference for model 2 over model 1. A negative BIC means that model 2 is preferred to model 1 and the more negative this value, the stronger the preference. Lets use this formula to compare the BIC of our passenger class model to the passenger class and fare model: p1 &lt;- length(coef(model.pclass))-1 p2 &lt;- length(coef(model.complex))-1 (model.complex$deviance-model.pclass$deviance)+(p2-p1)*log(nrow(titanic)) ## [1] -3.738641 Because the BIC is negative but less than -5, we have a moderate preference for the more complex model. Note that we could have gotten this same result by just using the BIC command on each model (which takes BIC relative to the saturated model) and subtracting them: BIC(model.complex)-BIC(model.pclass) ## [1] -3.738641 Using the formula from above, we can also construct a BIC’ value where we compare the model to the null model. We just need to plug in values for the null model as model 1: \\[BIC&#39;=(G^2-G^2_0)+(p)\\log n\\] Unfortunately, R does not have a built-in function for this calculation, but we can create a custom function pretty easily: BIC.null.glm &lt;- function(model) { n &lt;- length(model$resid) p &lt;- length(model$coef)-1 return((model$deviance-model$null.deviance)+p*log(n)) } BIC.null.glm(model.fare) ## [1] -79.06259 BIC.null.glm(model.pclass) ## [1] -113.4114 BIC.null.glm(model.complex) ## [1] -117.1501 By comparing the results, we can see that we strongly prefer all three models to the null model. Of the three models, we get the most negative BIC’ for the model that includes both fare and passenger class. Note that the difference between the passenger class model and the complex model is the same as above with the built-in BIC function. BIC.null.glm(model.complex)-BIC.null.glm(model.pclass) ## [1] -3.738641 Separation Separation can occur in logit models when the categories of the dependent variable separate an independent variable or combination of independent variables into segments. The easiest case to understand would be a two-way table in which all the values of category 1 and category 2 on one variable are identical to the values of another set of categories on another variable. Lets imagine a simple example of a yes/no question by gender of respondent for a simple case of six respondents: y &lt;- factor(c(&quot;yes&quot;,&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;no&quot;,&quot;no&quot;)) x &lt;- factor(c(&quot;female&quot;,&quot;female&quot;,&quot;female&quot;,&quot;male&quot;,&quot;male&quot;,&quot;male&quot;)) table(x,y) ## y ## x no yes ## female 0 3 ## male 3 0 In this table, gender perfectly predicts a yes or no response. This is called perfect separation. If we try to predict yes/no by gender with a logit model, we will get some odd results: model.sep &lt;- glm(y~x, family=binomial) summary(model.sep) ## ## Call: ## glm(formula = y ~ x, family = binomial) ## ## Deviance Residuals: ## 1 2 3 4 5 6 ## 6.547e-06 6.547e-06 6.547e-06 -6.547e-06 -6.547e-06 -6.547e-06 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 24.57 75639.11 0 1 ## xmale -49.13 106969.82 0 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8.3178e+00 on 5 degrees of freedom ## Residual deviance: 2.5720e-10 on 4 degrees of freedom ## AIC: 4 ## ## Number of Fisher Scoring iterations: 23 Notice that the model reports values although it spits out a warning. However, the numbers in the coefficient table are all strange. My coefficient sizes are huge (-49 log-odds ratio), my standard errors are astronimical and my p-value is so close to 1 that it reports as 1. These are all signs of separation. The MLE solution does not exist for this data because the slope on male should be negative infinity! Complete separation can also happen for a quantitative independent variable. x &lt;- c(20,21,22,23,24,25) model.sep &lt;- glm(y~x, family=binomial) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred summary(model.sep) ## ## Call: ## glm(formula = y ~ x, family = binomial) ## ## Deviance Residuals: ## 1 2 3 4 5 6 ## 2.110e-08 2.110e-08 1.052e-05 -1.052e-05 -2.110e-08 -2.110e-08 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1062.76 2594089.60 0 1 ## x -47.23 115264.41 0 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8.3178e+00 on 5 degrees of freedom ## Residual deviance: 2.2152e-10 on 4 degrees of freedom ## AIC: 4 ## ## Number of Fisher Scoring iterations: 25 I get a similar problem here because the quantitative variable can be perfectly separated by the dependent variable at age 22. All observations less than or equal to 22 say yes and all those above 22 say no. Perfect separation is a fairly rare occurence, except in very small datasets. A more common occurence is quasi-complete separation in which the dependent variable partially separates the independent variable. Lets continue with our example from before: y &lt;- factor(c(&quot;yes&quot;,&quot;yes&quot;,&quot;yes&quot;,&quot;no&quot;,&quot;no&quot;,&quot;no&quot;)) x &lt;- factor(c(&quot;female&quot;,&quot;female&quot;,&quot;female&quot;,&quot;male&quot;,&quot;female&quot;,&quot;male&quot;)) table(x,y) ## y ## x no yes ## female 1 3 ## male 2 0 In this case, gender does not perfectly predict the yes/no response. However, I get no cases of men who said yes. When you see cases of zero cells in your two-way table, it indicates that you have sparseness in the data and also quasi-complete separation. The effects of quasi-complete separation will be similar to complete separation. model.qsep &lt;- glm(y~x, family=binomial) summary(model.qsep) ## ## Call: ## glm(formula = y ~ x, family = binomial) ## ## Deviance Residuals: ## 1 2 3 4 5 6 ## 0.75853 0.75853 0.75853 -0.00008 -1.66511 -0.00008 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.099 1.155 0.951 0.341 ## xmale -20.665 7604.236 -0.003 0.998 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8.3178 on 5 degrees of freedom ## Residual deviance: 4.4987 on 4 degrees of freedom ## AIC: 8.4987 ## ## Number of Fisher Scoring iterations: 18 The effects are not quite as exaggerated as for complete separation, but again I get enormous effect sizes with standard errors that are even larger, producing p-values close to one. If you see a result like this in your model, you have a problem of quasi-complete separation. Separation causes problems for model estimation. One way to resolve the issue is to try collapsing categorical variables with lots of categories. If some of these categories are small, then they could lead to sparseness when added to the model. This is particularly true if you start adding interactions between categorical variables. By collapsing some categories you may be able to remove the problem of sparseness. In cases where collapsing the variables is not an option, you can consider alernate estimation techniques. The most common is a penalized likelihood model. These models apply a penalty to the likelihood for very large coefficient sizes and can be used to pull extreme coefficients resulting from separation back closer to a reasonable value. You can implement a penalized likelihood model in R using the logistf package. This package includes a function logistf that can be used to directly estimate a penalized likelihod logit model. library(logistf) logistf(y~x) ## logistf(formula = y ~ x) ## Model fitted by Penalized ML ## Confidence intervals and p-values by Profile Likelihood ## ## coef se(coef) lower 0.95 upper 0.95 Chisq p ## (Intercept) 3.859368 3.790634 0.7979208 12.218743 0 1 ## xmale -3.124038 4.081161 -11.4834130 5.611449 0 1 ## ## Likelihood ratio test=-3.478826 on 1 df, p=1, n=6 "],
["models-for-polytomous-outcomes.html", "Models for Polytomous Outcomes", " Models for Polytomous Outcomes "],
["useful-references.html", "Useful References ", " Useful References "],
["example-datasets.html", "Example Datasets", " Example Datasets We will utilize several different datasets throughout this course to develop concepts and to provide examples. You should become familiar with these datasets. Below, I provide a brief description of each of the datasets we will use. If you are taking my undergraduate course, you will have access to these datasets through RStudio Cloud. Otherwise, you can download a ZIP file of these datasets here. You should unzip this folder to your own computer and place it somewhere you can easily access it (e.g. the desktop). You should use this folder to organize all of your work for the course.The datasets are in a *.RData format. You can simply click on them in your filesystem viewer to load them into R. Crimes The crimes data contain information on crime rates and demographic variables for all fifty US states and the District of Columbia. The crime rates are for the year 2010 and come from the FBI’s Uniform Crime Reports (UCR). The UCR is a program where local law enforcement agencies all report crime statistics to the FBI and these are aggregated into final crime statistics. For our purposes, we are dividing crimes into two main categories of violent and property crime. The demographic characteristics come from the American Community Survey (ACS) between the years 2008 and 2012. The ACS is an annual sample of the US population. To get a large enough sample in each state to calculate correct statistics (with little sampling error), I combine five years of data that are “centered”\" on 2010. Here is a full description of all variables in the dataset that we will use. Violent: violent crimes per 100,000 population within each state. This includes the crimes of murder, rape, robbery, and aggravated assault. By dividing the number of crimes by the population size, we avoid the problem of larger population states having more crimes because of a larger population. This is often called the crime “rate.” Property: property crimes per 100,000 population. This includes the crimes of burglary, larceny, and motor vehicle theft.MedianAge: Median age of a state’s population. PctMale: Percent of a state population that is male. PctLessHS: The percent of the state population over the age of 25 without a high school diploma. MedianIncomeHH: Median household income in a state. This is measured in thousands of dollars (i.e. 35 means $35,000). We are taking the income of each household (meaning all members of that household combined) rather than individual level income. For most purposes, this is thought to be a better measure because consumption and savings are typically organized at the household level. Unemployment: Unemployment rate in the state. The unemployment “rate” is really just a percentage. Its the percentage of individuals who are not working but want to work among all those in the labor force (those who are working or looking for work). Poverty: Poverty rate in the state. The poverty “rate” is also really just a percentage. It is the percent of individuals living below the poverty line. The poverty line is a number developed by the federal government. It was originally developed in the 1960s and is adjusted for inflation every year. Many people critique the poverty line as being too low because it has not kept pace with increases in the consumer price index. Gini: A measure of income inequality in the state. The gini coefficient is a widely used measure of how unequally income is distributed. If gini is zero, then everyone has exactly the same income. If gini is 100, then one person makes all the money and everyone else zero. The higher the gini coefficient, the more income inequality exists. Movies The movie data contain information about 2,612 movies produced between 2001 and 2013. The data come from the Open Movie Database, which itself contains data from the Internet Movie Database and Rotten Tomatoes. To simplify our analyses, I have limited the analysis to movies that played in the US and received 10 or more reviews. I have also excluded all shorts, documentaries, foreign language films, and movies that received an NC-17 rating or were unrated. Here are the variables we have for each movie: Year: The calendar year of the film’s release. Rating: The movie’s maturity rating (G, PG, PG-13, R). Runtime: The length of the movie in minutes. Oscars: The number of Oscar awards that the movie received. This includes Oscars that go to individual actors (leading and supporting), as well as more general awards (best screenplay, editing, cinematography, etc.), and best picture overall. TomatoMeter: The tomato meter is the percent of reviews that are judged to be positive by Rotten Tomatoes staff. This metric goes from 0 to 100 percent. Note that movies are spread out pretty evenly across the range of this variable, something we call a “uniform” distribution. Note, that this method makes no distinction between how positive a positive review was or how negative a negative review was, so its perfectly possible for two movies with the same Tomato Meter to be viewed very differently by reviewers. TomatoRating: The tomato rating is a combination of all reviews where the review used some kind of numeric rating (e.g. 3 out of 4 stars, 7 out of 10). Rotten Tomatoes “normalizes” these scores so that they are all recorded on the same basis. The scale of this normalized score goes from 1 to 10. Unlike the Tomato Meter, this scale should be capable of distinguishing how strongly positive or negative the review was. BoxOffice: The box office returns for the movie in millions of US dollars. Genre: The genre of the film. This is a tricky variable to create. In actuality, movies could be listed as multiple genres in the original dataset, with twenty different genres to choose from. For example, “No Country for Old Men” is listed in the genres of crime, drama, and thriller while “Lord of the Rings: Return of the King” is listed as action, adventure, and fantasy. This is probably the best way to treat genres, but for our purposes it adds a lot of complexity. Therefore, I have recoded movies into a single “best” genre based on a decision rule where certain genres trump all others on an ordered basis. For example, comedy trumps romance, so romantic comedies will always show up in this dataset as comedies. The ordering of this system is Animation &gt; Family &gt; Musical &gt; Horror &gt; SciFi/Fantasy &gt; Comedy &gt; Romance &gt; Action &gt; Thriller &gt; Mystery &gt; Drama &gt; All Others. For the most part, this system works well, but you may notice some odd discrepancies for a few movies. Politics This data comes from the 2016 American National Election Study (ANES). The ANES is a survey of the American electorate that is conducted every two years. The study collects information on a variety of political attitudes and voting behaviors. For our purposes, we are going to primarily look at respondent’s vote for president and attitudes on three issues: (1) birthright citizenship, (2) gay marriage, and (3) global warming. The variables we will look at are: brcitizen: Respondents were asked whether they would support a proposal to change the US Constitution to remove birthright citizenship (citizenship automatically granted to individuals born in the US regardless of their parent’s citizenship status). Respondents could either favor, oppose, or neither favor or oppose. gaymarriage: Respondents were asked for their position on gay marriage and were given the choices of “no legal recognition”, “civil union (but no marriage)”, “support gay marriage.” globalwarm: A question on whether the respondent believes that anthropogenic global warming is happening. I constructed this variable from two separate questions. The first question asks whether respondents think that global warming has been happening with the options being that it “probably has” or “probably has not.” The second question asks whether respondents thought that global warming was caused by human activity (either entirely or partially). I combine these into a single dichotomous variable where individuals either think the earth is warming from human activity or that it is not warming from human activity, where the latter category includes people who think it isn’t warming at all and people who think it is warming but not because of human activity. party: The political party with which the respondent identifies. This does not necessarily mean that a respondent is officially registered with a given party. relig: The respondent’s religion. This category is based on the combination of people’s statement about the kind of services they typically attend along with several non-exclusive yes/no questions about their religion (e.g. evangelical, Pentecostal, agnostic, atheist). age: The age of the respondent. gender: The respondent’s self-reported gender, recorded as “Male”,“Female”, or “Other.” race: the racial identification of the respondent. Respondents could write in multiple races, but to keep it simple, we will combine the small number of individuals who reported multiple races with those who listed “Other” as their race. educ: The education of the respondent. This is recorded as an ordinal variable. The “Some college” response indicates individuals who have attended college (including 2-year programs) but have not earned a BA. income: The family income of the respondent in 1000s of dollars. Respondents did not give actual dollar amounts here but rather indicated which bracket of income (e.g. $20,000-30,000) they fell within. For the purposes of our class, I randomly select an actual value within this bracket for each respondent. workstatus: The work status of the respondent. Respondents could either be working, unemployed, or out of the labor force. The last category refers to people who are not employed and not currently looking for work, whereas unemployed indicates a person who is not employed an is currently looking for work. military: Whether the respondent has ever served or is currently serving in the US military. Popularity This data comes from the National Longitudinal Study of Adolescent to Adult Health (Add Health), conducted by the Carolina Population Center at UNC-Chapel Hill and supported by a grant from the National Institute of Child Health and Human Development. The first wave of the study which we are using surveyed adolescents between 7th and 12th grade in school in the 1994-95 school year. One of the particularly valuable features of the Add Health survey is that many respondents were in the “saturation sample” which sampled all students at 16 schools. In this saturation sample, students were asked about who were their friends and sexual partners, which allows researchers to construct network maps of adolescent social systems. We will use this saturation sample to look at a various basic measure of that network that estimates students’ popularity. This measure, which is called “in degree” in the network analysis literature, measures the number of times a student was nominated as a friend by other students in the school. We will treat it as a simple proxy measure of a student’s popularity. We can then look at what other student characteristics were positively or negatively associated with a student’s popularity. Here is a full description of all variables in the dataset that we will use. indegree: The number of friend nominations received by other students at the same school. This is the measure of popularity that we will use. race: A six-category nominal variable indicating the race that the student best thought described them when asked to choose a single race: white, black, Latino, Asian, American Indian, other. sex: Add Health reports this as a student’s “biological” sex. Students were only reported as male or female.grade: current grade of the student as a quantitative variable. psuedoGPA: Students were asked for the most recent letter grade in four course types: math, language arts, science, and math. This variable was constructed by calculating GPA from those responses. honorsociety: A true/false variable for whether a student was in honor society or not. alcoholuse: A true/false variable that is true if the student reported drinking at least once or twice a month in the last twelve months. smoker: A true/false variable that is true if student smoked more than 5 cigarettes in the past 30 days. bandchoir: a true/false variable that is true if the student was in band or choir. academicclub: a true/false variable that was true if the student was in an academically-oriented club such as math club, book club, etc. nsports: The number of different school sports a student reported participating in. Students who reported more than six sports were top-coded at the value of six. parentinc: Parent’s household income measured in $1000’s of dollars. Sex The sex data come from a special supplemental questionnaire that was added to the General Social Survey (GSS) in 2004. The GSS is a survey of attitudes that is conducted every two years by the National Opinion Research Council (NORC). In the 2004 supplement, respondents were asked questions about their sexual behavior. We will be looking specifically at respondents reported frequency of sexual activity and its relationship to demographic characteristics such as age, education, and marital status. Here are the variables we will look at: sexf: A quantitative variable indicating the frequency of sexual activity as the number of sexual encounters per year. The sequal frequency response was coded as an ordinal scale variable in which respondents were given a set of options from less to more sexual activity in the previous year. For our purposes, I have recoded the ordinal sexfreq variable into a quantitative variable by giving everyone the midpoint number of sexual acts per year based upon their answer. For example, individuals who said (2 or 3 times a month) were given a value of \\(2.5*12=30\\). This will allow us to use sexual frequency as a dependent variable in regression models. age: The age of the respondent. The GSS only surveys adults aged 18 years and older. gender: The gender of the respondent. educ: Years of education for the respondent. marital: Marital status of the respondent: Never married, married, divorced, widowed, separated. relig: Religious affiliation of the respondent. Protestants have been divided into “mainline” and “fundamentalist” based on a coding of specific denominations used by the GSS. Titanic The titanic data contain information on all 1,309 passengers aboard the Titanic. The data do not include information about the crew. The data primarily come from the online database, Encyclopedia Titanica. Here are the variables we will look at: survival: Did the passenger survive? sex: The reported sex of the passenger. age: The age of the passenger. This variable is reported in whole numbers for those over one year old and as a decimal (based on months of age) for infants under a year of age. agegroup: A categorical variable indicating whether the person was an adult or a child. I have constructed this variable from the age variable. The cutoff for adults is sixteen years of age. pclass: There were three passenger classes: First, second, and third (also known as steerage). To give some pop culture references, Rose was first class, and Jack was third class. Most of the passengers were in third class. fare: The fare paid for the ticket, measured in British pounds. family: The number of family members traveling with the passenger. These family members can either be parents, spouses, siblings, or children. Wages This data has information on the hourly wages of US workers in 2018. The data here are extracted from Current Population Survey data via IPUMS. I used the earning data from the outgoing rotation groups (ORG) for each month of the CPS. Each household in the CPS is is part of a rolling panel in which they are in for four months, out for eight months, and back in for four months. In the fourth and eight month of inclusion they are given additional questions as part of the outgoing rotation group. The hourly wage of salaried workers is assessed by a question on hours worked in a typical week and earnings in the prior week. I limited the data only to those individuals between the ages of 18 and 65 in order to capture the age range of the typical worker. The dataset contains the following variables: wages: The hourly wage for the respondent. For workers who report being paid hourly, this value is based on a direct question that asked for respondents’ hourly wages. For individuals in salaried positions, this value was derived by dividing the earnings from the previous week by the hours worked in the previous week. Anyone who reported a wage of less than one dollar is removed. Any wage higher than $99.99 is top-coded as $99.99. age: age of the respondent in years. gender: Male or Female. race: The respondent’s racial identification recoded from two separate questions on race and hispanicity into the following categories: White, Black, Latino, Asian, Indigenous, and Other/Multiple races. The indigenous category includes American Indians, Pacific Islanders, and Alaska Natives. marstat: The respondent’s current marital status: never married, married, divorced or separated, and widowed. education: The respondent’s highest educational attainment: no high school diploma, high school diploma, associate’s degree, bachelor’s degree, graduate degree. The last category includes master’s degrees, professional degrees, and doctoral degrees. occup: The broad occupational category of the respondent. In the actual CPS data, there are hundreds of different occupations listed. For our purposes, I have simplified this into a broader (and smaller) set of occupational categories that we will use for the analysis. Here are the categories of the occupational variable, along with some examples of specific occupations: Managers: Human resources Managers, Operations Managers Business/Finance Specialist: Claims Adjusters, Compliance Officers, Accountants, Tax Preparers STEM: Computer Programmers, Civil Engineers, Biological Scientists Doctors: Dentists, Surgeons, Optometrists Legal: Lawyers, Judges, Paralegals Education: Preschool and Kindergarten Teachers, Librarians Arts, Design, and Media: Artists, Dancers and Choreographers, Writers and Authors Other Healthcare: Registered Nurses, Physical Therapists, Dental Hygienists Social Services: Clergy, Social Workers Service: Waiters and Waitresses, Barbers, Bartenders Sales: Cashier, Telemarketer Administrative Support: Bank Tellers, Data Entry Keyers, Receptionist Manual: Carpenters, Logging Workers, Mining Machine Operators, Small Engine Mechanic nchild: Number of own children living in the household with the respondent. foreign_born: A variable indicating whether the respondent is foreign born or not. Recorded as “Yes” or “No”. earn_type: This variable indicates whether the respondent reported being paid hourly wages or by salary. earningwt: A technical weighting variable for use with any CPS analysis of earnings. "],
["common-r-commands.html", "Common R Commands", " Common R Commands Below is a list of common commands that we use for the undergraduate class, along with some examples. You can view a help file in RStudio for each command by searching for the command name in the help tab in the lower right panel. You can also just type the name of the command preceded by a “?” into the console. For example, if you wanted to understand how barplot works, type: ?barplot The list here does not contain information about making plots in R. That information is in the Plotting Cookbook appendix. Univariate Statistics mean Calculate the mean of a quantitative variable. Remember that this command will not work for categorical variables. mean(earnings$wages) ## [1] 24.27601 median Calculate the median of a quantitative variable. Remember that this command will not work for categorical variables. median(earnings$wages) ## [1] 19.21667 sd Calculate the standard deviation of a quantitative variable. Remember that this command will not work for categorical variables. sd(earnings$wages) ## [1] 16.23676 IQR Calculate the interquartile range of a quantitative variable. Remember that this command will not work for categorical variables. IQR(earnings$wages) ## [1] 17 quantile Calculate percentiles of a distribution. Remember that this command will not work for categorical variables. By default, the quantile command will return the quartiles (0,25,50,75,100 percentiles). If you want different percentiles, you will have to specify the probs argument. quantile(earnings$wages) ## 0% 25% 50% 75% 100% ## 1.00000 13.00000 19.21667 30.00000 99.99000 #get the 10th and 90th percentile instead quantile(earnings$wages, probs = c(0.1,0.9)) ## 10% 90% ## 10.000 47.596 table Calculate the absolute frequencies of the categories a categorical variable. table(movies$Genre) ## ## Action Animation Comedy Drama Family ## 207 139 781 332 155 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 220 103 39 138 258 ## Thriller ## 181 prop.table Calculate the proportions (i.e. relative frequencies) of the categories of a categorical variable. This command must be run on the output from a table command. You can do that in one command by nesting the table command inside the prop.table command. prop.table(table(movies$Genre)) ## ## Action Animation Comedy Drama Family ## 0.08108108 0.05444575 0.30591461 0.13004309 0.06071289 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 0.08617313 0.04034469 0.01527615 0.05405405 0.10105758 ## Thriller ## 0.07089698 summary Provide a summary of a variable, either categorical or quantitative. summary(earnings$wages) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 13.00 19.22 24.28 30.00 99.99 summary(movies$Genre) ## Action Animation Comedy Drama Family ## 207 139 781 332 155 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 220 103 39 138 258 ## Thriller ## 181 Bivariate Statistics table Can be used to create a two-way table, although further work needs to be done to extract useful information from the two-way table. table(movies$Genre, movies$Rating) ## ## G PG PG-13 R ## Action 0 3 98 106 ## Animation 37 92 6 4 ## Comedy 1 71 352 357 ## Drama 2 36 116 178 ## Family 16 131 8 0 ## Horror 0 1 60 159 ## Musical/Music 0 7 57 39 ## Mystery 0 0 7 32 ## Romance 0 12 64 62 ## SciFi/Fantasy 0 10 186 62 ## Thriller 0 0 37 144 prop.table Calculate the conditional distributions from a two-way table. The first argument here must be a two-way table output from the table command. It is very important that you also add a second argument that indicated the way you want the conditional distributions. 1 will give you distributions conditional on the row variable and 2 will give you distributions conditional on the column variable. ## ## G PG PG-13 R ## Action 0.000000000 0.014492754 0.473429952 0.512077295 ## Animation 0.266187050 0.661870504 0.043165468 0.028776978 ## Comedy 0.001280410 0.090909091 0.450704225 0.457106274 ## Drama 0.006024096 0.108433735 0.349397590 0.536144578 ## Family 0.103225806 0.845161290 0.051612903 0.000000000 ## Horror 0.000000000 0.004545455 0.272727273 0.722727273 ## Musical/Music 0.000000000 0.067961165 0.553398058 0.378640777 ## Mystery 0.000000000 0.000000000 0.179487179 0.820512821 ## Romance 0.000000000 0.086956522 0.463768116 0.449275362 ## SciFi/Fantasy 0.000000000 0.038759690 0.720930233 0.240310078 ## Thriller 0.000000000 0.000000000 0.204419890 0.795580110 tapply Calculate a statistic (e.g. mean, median, sd, IQR) for a quantitative variable across the categories of a categorical variable. The first argument should be the quantitative variable. The second argument should be the categorical variable. The third argument should be the name of the command that will calculate the desired statistic. tapply(movies$Runtime, movies$Rating, mean) ## G PG PG-13 R ## 90.80357 99.71901 108.02321 105.25547 tapply(movies$Runtime, movies$Rating, median) ## G PG PG-13 R ## 90 96 105 102 tapply(movies$Runtime, movies$Rating, sd) ## G PG PG-13 R ## 14.63796 13.95487 17.58490 16.07108 cor Calculate the correlation coefficient between two quantitative variables. cor(crimes$Violent, crimes$Gini) ## [1] 0.5454737 Statistical Inference nrow Return the number of observations in a dataset. nrow(crimes) ## [1] 51 qt Calculate the t-value needed for a confidence interval. For a 95% confidence interval, the first argument should always be 0.975. The second argument should be the appropriate degrees of freedom for the statistic and dataset. qt(0.975, nrow(politics)-1) ## [1] 1.960524 pt Calculate the p-value for a hypothesis test. The first argument should always be the negative version of the t-statistic and the second argument should be the appropriate degrees of freedom for the statistic and dataset. 2*pt(-2.1, nrow(politics)-1) ## [1] 0.03578782 OLS Regression Models lm Run an OLS regression model. The first argument should always be a formula of the form dependent~independent1+independent2+.... To simplify the writing of variable names, it is often useful to specify a second argument data that identifies that dataset being used. Then you don’t have to include dataset_name$ in the formula. **Remember to always put the dependent (y) variable on the left hand side of the equation. #simple model with one independent variable model_simple &lt;- lm(wages~age, data=earnings) #same simple model but recenter age on 45 years of age model_recenter &lt;- lm(wages~I(age-45), data=earnings) #a model with multiple independent variables, both quantitative and qualitative model_multiple &lt;- lm(wages~I(age-45)+education+race+gender+nchild, data=earnings) #a model like the previous but also with interaction between gender and nchild model_interaction &lt;- lm(wages~I(age-45)+education+race+gender*nchild, data=earnings) Once a model object is created, information can be extracted with either the coef command which just reports the slopes and intercept, or a full summary command which gives more information. coef(model_interaction) ## (Intercept) I(age - 45) educationHS Diploma ## 17.3568021 0.2242916 4.5382688 ## educationAA Degree educationBachelors Degree educationGraduate Degree ## 7.4288321 16.2657784 23.0187910 ## raceBlack raceLatino raceAsian ## -3.4176245 -2.1133582 0.5641751 ## raceIndigenous raceOther/Multiple genderFemale ## -1.5198248 -0.4331997 -4.3777137 ## nchild genderFemale:nchild ## 1.2629571 -0.7490706 summary(model_interaction) ## ## Call: ## lm(formula = wages ~ I(age - 45) + education + race + gender * ## nchild, data = earnings) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.638 -7.779 -2.198 4.568 90.578 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.356802 0.159721 108.669 &lt; 2e-16 *** ## I(age - 45) 0.224292 0.002858 78.471 &lt; 2e-16 *** ## educationHS Diploma 4.538269 0.154451 29.383 &lt; 2e-16 *** ## educationAA Degree 7.428832 0.181143 41.011 &lt; 2e-16 *** ## educationBachelors Degree 16.265778 0.164396 98.943 &lt; 2e-16 *** ## educationGraduate Degree 23.018791 0.178161 129.202 &lt; 2e-16 *** ## raceBlack -3.417625 0.123798 -27.607 &lt; 2e-16 *** ## raceLatino -2.113358 0.109491 -19.302 &lt; 2e-16 *** ## raceAsian 0.564175 0.157602 3.580 0.000344 *** ## raceIndigenous -1.519825 0.321284 -4.730 2.24e-06 *** ## raceOther/Multiple -0.433200 0.303134 -1.429 0.152987 ## genderFemale -4.377714 0.090203 -48.532 &lt; 2e-16 *** ## nchild 1.262957 0.043476 29.049 &lt; 2e-16 *** ## genderFemale:nchild -0.749071 0.063268 -11.840 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 13.74 on 145633 degrees of freedom ## Multiple R-squared: 0.284, Adjusted R-squared: 0.2839 ## F-statistic: 4443 on 13 and 145633 DF, p-value: &lt; 2.2e-16 Utility functions round Used for rounding the results of numbers to a given number of decimal places. By default, it will round to whole numbers, but you can specify the number of decimal places in the second argument. 100*round(prop.table(table(movies$Genre)), 2) ## ## Action Animation Comedy Drama Family ## 8 5 31 13 6 ## Horror Musical/Music Mystery Romance SciFi/Fantasy ## 9 4 2 5 10 ## Thriller ## 7 sort Sort a vector of numbers from smallest to largest (default), or largest to smallest (with additional argument decreasing=TRUE). sort(100*round(prop.table(table(movies$Genre)), 2), decreasing = TRUE) ## ## Comedy Drama SciFi/Fantasy Horror Action ## 31 13 10 9 8 ## Thriller Family Animation Romance Musical/Music ## 7 6 5 5 4 ## Mystery ## 2 sort(100*round(prop.table(table(movies$Genre)), 2)) ## ## Mystery Musical/Music Animation Romance Family ## 2 4 5 5 6 ## Thriller Action Horror SciFi/Fantasy Drama ## 7 8 9 10 13 ## Comedy ## 31 "],
["plotting-cookbook.html", "Plotting Cookbook", " Plotting Cookbook This appendix will provide ggplot example R code and output for of all the graphs that we might use this term. For further information, I highly recommend Kieran Healy’s Data Visualization book and Hadley Wikham’s ggplot2 book. All the examples provided will use the standard example datasets that we have been working with throughout the term. Barplots Oddly, barplots are one of the trickiest graphs in ggplotif you want proportions rather than absolute frequencies for each category. To make proportions, I need to specify y=..prop.. in the aesthetics, and then use group=1 to ensure that all categories are part of a single group and thus have proportions that sum up to one. In the code below, I am also using the scale_y_continuous command and the scales::percent command to label the tickmarks on the y-axis as percents rather than proportions. ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+ geom_bar()+ scale_y_continuous(labels=scales::percent)+ labs(x=&quot;passenger class&quot;, y=NULL, title=&quot;Distribution of passenger class on Titanic&quot;)+ theme_bw() Figure 103: A barplot The result of this code is shown in Figure 103. Because the tickmark labels are self-explanatory I use a value of NULL for the y-axis label. Histograms Histograms are a snap in ggplot. The binwidth argument will allow you to easily resize bin widths without having to provide every break as base plot does. ggplot(movies, aes(x=Runtime))+ geom_histogram(binwidth = 5, col=&quot;black&quot;)+ labs(x=&quot;movie runtime (minutes)&quot;, title=&quot;Histogram of movie runtime&quot;)+ theme_bw() Figure 104: A basic histogram Figure 104 shows the basic histogram from the code above. By default, ggplot will not place a border around bars, but I like to make the border a slightly different color (e.g. col=\"black\") to provide better visual definition. You can also calculate density instead of count, if you prefer, by adding the y=..density.. option to the aesthetics. Density is the proportion of cases in a bin, divided by bin width. If you plot a histogram with density, you can also overlay this figure with a smoothed line approximating the distribution called a kernel density. These results are shown in Figure 105. ggplot(movies, aes(x=Runtime, y=..density..))+ geom_histogram(binwidth = 5, col=&quot;black&quot;)+ geom_density(col=&quot;red&quot;, fill=&quot;grey&quot;, alpha=0.5)+ labs(x=&quot;movie runtime (minutes)&quot;, title=&quot;Histogram of movie runtime&quot;)+ theme_bw() Figure 105: A histogram with kernel density smoother Boxplots The only tricky thing to a boxplot in ggplot is to remember that the aesthetic is y not x. Its not required, but setting x=\"\" in the aesthetics and then setting the x label to NULL will also create a cleaner display with no x-axis. I also apply a geom_boxplot only argument here that colors outliers a different color. ggplot(movies, aes(x=&quot;&quot;, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime&quot;)+ theme_bw() Figure 106: A basic boxplot You can also try a geom_violin instead which basically gives you a mirrored kernel density plot. For a single variable, use x=1 because geom_violin requires an x aesthetic. Below, I show how you can extend this into a comparative violin plot. ggplot(movies, aes(x=1, y=Runtime))+ geom_violin(fill=&quot;grey&quot;)+ scale_x_continuous(labels=NULL)+ labs(x=NULL, y=&quot;movie runtime (minutes)&quot;, title=&quot;Violin plot of movie runtime&quot;)+ theme_bw() Figure 107: A violin plot Comparative Barplots Comparative barplots are definitely one of the most difficult cases to graph properly in ggplot because grouping causes all kinds of issues with calculating proportions correctly. The best approach that I have found is to use faceting to separate out the barplots of one variable by the categories of the other variable. The basic code is then identical to that for a single barplot above with the addition of a facet_wrap command to identify the second categorical variable to use for faceting. ggplot(titanic, aes(x=survival, y=..prop.., group=1))+ geom_bar()+ facet_wrap(~pclass)+ scale_y_continuous(labels = scales::percent)+ labs(x=NULL, y=NULL, title=&quot;Survival on the Titanic by class&quot;)+ theme_bw() Figure 108: A comparative barplot made by faceting Figure 108 shows the result. In this case, since survival and death are mirror images, we could convey the same information, but with much less ink by a simple dotplot of the percent surviving by passenger class. However, this would require some preparatory work to prepare the data in the way we want to display it. For the undergraduate class, you are not required to understand how to prepare this data. tab &lt;- prop.table(table(titanic$survival, titanic$pclass),2) tab &lt;- subset(as.data.frame.table(tab), Var1==&quot;Survived&quot;, select=c(&quot;Var2&quot;,&quot;Freq&quot;)) colnames(tab) &lt;- c(&quot;pclass&quot;,&quot;prop_surv&quot;) ggplot(tab, aes(x=pclass, y=prop_surv))+ geom_point()+ scale_y_continuous(labels=scales::percent, limits=c(0,0.8))+ labs(x=NULL, y=NULL, title=&quot;Percent surviving Titanic by class&quot;)+ coord_flip()+ theme_bw() Figure 109: A simple dotplot Figure 109 shows the resulting dotplot. If that is a little too spartan for you, you could try a lollipop graph, shown in Figure 110. The geom_lollipop is in the ggalt library. ggplot(tab, aes(x=pclass, y=prop_surv))+ geom_lollipop()+ scale_y_continuous(labels=scales::percent, limits=c(0,0.7))+ labs(x=NULL, y=NULL, title=&quot;Percent surviving Titanic by class&quot;)+ coord_flip()+ theme_bw() Figure 110: A lollipop plot Comparative Boxplots To construct comparative boxplots, we just need to add an x to the aesthetic of a boxplot. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, title=&quot;Boxplot of movie runtime by genre&quot;)+ theme_bw() Figure 111: A basic comparative boxplot The mediocre results are shown in Figure 111. The labels on the x-axis often run into one another, so coord_flip is a good option to avoid that. In addition, I can also use the varwidth argument in geom_boxplot to scale the width of boxplots relative to the number of observations. ggplot(movies, aes(x=Genre, y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;, varwidth=TRUE)+ labs(y=&quot;movie runtime (minutes)&quot;, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 112: Flipping coordinates for better display My improved comparative boxplot is shown in Figure 112. However, Its also often useful to re-order the categories by the mean or median of the quantitative variable. We can do this with the reorder command: ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_boxplot(fill=&quot;grey&quot;, outlier.color = &quot;red&quot;, varwidth=TRUE)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 113: Re-order categories by median The results are shown in Figure 113. In this case, we used the reorder command to reorder the categories of Genre by the median value of Runtime. We could also try a comparative violin plot as shown in Figure 114. ggplot(movies, aes(x=reorder(Genre, Runtime, median), y=Runtime))+ geom_violin(fill=&quot;grey&quot;)+ labs(y=&quot;movie runtime (minutes)&quot;, x=NULL, title=&quot;Boxplot of movie runtime by genre&quot;)+ coord_flip()+ theme_bw() Figure 114: A comparative violin plot Scatterplots The geom_point command gives us the the geom we want for scatterplots. We can also use geom_smooth to graph a variety of lines to our data. Setting an alpha value can help with overplotting as well by providing semi-transparency. ggplot(politics, aes(x=age, y=income))+ geom_point(alpha=0.5)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 115: Basic Scatterplot The results are shown in Figure 115. As more points are plotted in the same position, the overall plot darkens. However, there is still a lot of overplotting because age is only recorded in whole numbers. In cases like this, we can use geom_jitter rather than geom_point to randomly perturb each value pair a little bit, as shown in Figure 14. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 116: Jittering points to deal with overplotting It is often useful to apply some model to the data to draw a best-fitting line through the points. The geom_smooth function will do this for us and we can use the method argument to specify what kind of model we want to fit. You an apply non-parametric smoothers like LOESS as well as an OLS regression line. Lets first try a LOESS smoother. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;loess&quot;)+ scale_y_continuous(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 117: Plotting loess smoother to data Figure 117 shows us the smoothed line. It looks like an inverted U-shaped distribution. Family income is also highly right skewed. Lets try scale_y_log10 to put income on a log-scale (advanced). ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;loess&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 118: Changing scale of y-axis to logarithmic Figure 118 shows the scatterplot with an exponential scale for the y-axis. Note how the values go from $1K to $10K to $100K at even intervals. Now lets try switch the smoothing to a linear model, by simply changing the method in geom_smooth: ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;lm&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 119: Fitting an OLS regression line Figure 119 shows the OLS line predicting income. Its pretty flat which to some extent hides the underlying curvilinear relationship. In practice, I would probably want to apply a quadratic term on age to any regression model. Finally, we can add add in an aesthetic for a third categorical variable by applying color to the plots. In this case, I will color the plots by highest degree received. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.5, aes(color=educ))+ geom_smooth(method=&quot;lm&quot;)+ scale_color_brewer(palette = &quot;YlOrRd&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;, color=&quot;Highest degree&quot;)+ theme_bw() Figure 120: Adding color aesthetic Figure 120 shows the result. Because I defined the color aesthetic in geom_jitter rather than the base ggplot command, I still only get one smoothed OLS regression line for the whole dataset. If I wanted separate lines by educational level, I could add the color aesthetic to ggplot instead. ggplot(politics, aes(x=age, y=income, color=educ))+ geom_jitter(alpha=0.5)+ geom_smooth(method=&quot;lm&quot;)+ scale_color_brewer(palette = &quot;YlOrRd&quot;)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;, color=&quot;Highest degree&quot;)+ theme_bw() Figure 121: Separate OLS lines by color aesthetic Figure 121 shows the same plot but now with separate lines for each educational level. We are basically plotting interaction terms here between age and educational level. Instead of coloration, I could have used faceting to try to separate out differences by education. Figure 122 shows the results using this approach. For this graph, I use two separate geom_smooth commands to get the LOESS and linear fit for each panel. ggplot(politics, aes(x=age, y=income))+ geom_jitter(alpha=0.3)+ geom_smooth(method=&quot;lm&quot;)+ geom_smooth(method=&quot;loess&quot;, color=&quot;red&quot;)+ facet_wrap(~educ)+ scale_y_log10(labels=scales::dollar)+ labs(x=&quot;age&quot;, y=&quot;family income (in thousands USD)&quot;, title=&quot;Scatterplot between age and family income&quot;, caption=&quot;ANES 2016&quot;)+ theme_bw() Figure 122: facet by education instead of color If you have a small number of cases, it might make sense to label your values. You can label values with geom_text but you can often run into problems with labels being truncated or overlapping. ggplot(crimes, aes(x=Unemployment, y=Property))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text(aes(label=State), size=2)+ labs(x=&quot;Unemployment rate&quot;, y=&quot;Property crimes per 100,000 population&quot;, title=&quot;Scatterplot of unemployment and property crime&quot;)+ theme_bw() Figure 123: labeling points can look really bad Figure 123 shows labeling with the default geom_text approach. You can fiddle with a variety of arguments here that let you offset the labels but it rarely produces clean labels. The library ggrepel has a function geom_text_repel that will work harder to place labels in a readable position: ggplot(crimes, aes(x=Unemployment, y=Property))+ geom_point(alpha=0.7)+ geom_smooth(method=&quot;lm&quot;, se=FALSE)+ geom_text_repel(aes(label=State), size=2)+ labs(x=&quot;Unemployment rate&quot;, y=&quot;Property crimes per 100,000 population&quot;, title=&quot;Scatterplot of unemployment rate and property crime rate&quot;)+ theme_bw() Figure 124: repel works better for labeling points, but still can get too busy "],
["r-stat-lab.html", "R Stat Lab ", " R Stat Lab "],
["using-scripts.html", "Using Scripts", " Using Scripts One of the primary advantages of using statistical software like R or Stata is the ability to perform your analysis in a script. A script is a text file of commands that can be run to reproduce a data cleaning exercise, an analysis, a simulation, etc. It is a good practice to always do your statistical work using scripts. The advantage of scripting is that you will have an easily reproducible record of exactly what you did. Lets say that I did a thorough and time-consuming statistical analysis in Excel or (god forbid) SPSS using pull-down menus and buttons and I found The Best Result Ever. I quickly put together a paper and submit it to a flagship journal. My R&amp;R comes back several months later and the reviewers say that my results look great, but they want to see if the results hold up if I limit my sample in some way. Now I have to (a) try to remember exactly what I did months ago, and (b) re-run that entire time-consuming analysis from scratch. That is horribly inefficient and is likely to lead to inconsistencies between the two analyses. Imagine that instead of cowboying my analysis, I had written everything into an R or Stata script. Now, I only have to add a line of code at the top that subsets my sample and processes the script. This will take a few minutes at most. Getting Started with Scripts Lets put together a simple script in R. In RStudio, you can go to File &gt; New File &gt; R Script to get a new script up and running. This will open a text pane in the upper left corner of RStudio. This is your R script. Lets start by writing a simple “Hello World” script. Type the following command into your script: cat(&quot;Hello World!&quot;) 2+2 You can run this script a couple of different ways: You could just copy and paste both lines of code to the console in the lower left. This is quick and dirty but is typically not the most efficient way to run code from your script. You could run your entire script by clicking the “Source” button in the upper right corner of the script pane. Note that with this button, you won’t get the output, unless you use the drop-down arrow to choose “Source with Echo”. You could run a single line of your script where the cursor is located. You can do this by either hitting the “Run” button or by clicking Ctrl+Enter on Windows/OSX or Command+Enter on OSX. When you do execute the code this way, your cursor will automatically move to the next line of code, so you can execute your way through the code simply by clicking Ctrl+Enter repeatedly. If you have saved the script to a file, you can also type in the source command in the console to source the script. Lets run this script using the “Source with Echo” button. Here is what the output looks like in my console: &gt; source(&#39;~/.active-rstudio-document&#39;, echo=TRUE) &gt; cat(&quot;Hello World&quot;) Hello World &gt; 2+2 [1] 4 Now, lets save this script as helloworld.R (“.R” is the expected suffix for R scripts) and source it from the console: source(&quot;helloworld.R&quot;, echo=TRUE) ## ## &gt; cat(&quot;Hello World&quot;) ## Hello World ## &gt; 2 + 2 ## [1] 4 Now lets try a slightly more useful script. In this script, I am going to use the politics dataset to do the following: re-code the gay marriage support variable into support for gay marriage vs. all else, and the religion variable as evangelical vs. all else Create a two-way table (crosstab) of these two new variables. Calculate the odds ratio between the two new variables. I don’t expect you to know how all of this code works yet. I just want to show you an example of a script that actually does something interesting. politics$supportgmar &lt;- politics$gaymarriage==&quot;Support gay marriage&quot; politics$evangelical &lt;- politics$relig==&quot;Evangelical Protestant&quot; tab &lt;- table(politics$supportgmar, politics$evangelical) tab prop.table(tab, 2) OR &lt;- tab[1,1]*tab[2,2]/(tab[1,2]*tab[2,1]) OR If I run this script, I will get: source(&quot;politics.R&quot;, echo=TRUE) ## ## &gt; politics$supportgmar &lt;- politics$gaymarriage == &quot;Support gay marriage&quot; ## ## &gt; politics$evangelical &lt;- politics$relig == &quot;Evangelical Protestant&quot; ## ## &gt; tab &lt;- table(politics$supportgmar, politics$evangelical) ## ## &gt; tab ## ## FALSE TRUE ## FALSE 1103 662 ## TRUE 2218 255 ## ## &gt; prop.table(tab, 2) ## ## FALSE TRUE ## FALSE 0.3321289 0.7219193 ## TRUE 0.6678711 0.2780807 ## ## &gt; OR &lt;- tab[1, 1] * tab[2, 2]/(tab[1, 2] * tab[2, 1]) ## ## &gt; OR ## [1] 0.1915562 About 49% of non-evangelicals supported gay marriage while only 24% of evangelicals supported it. That works out to an odds ratio of 0.33, meaning that the odds of gay marriage support were about a third as high for evangelicals as for non-evangelicals. Lets say I then wanted to change this script to broaden the definition of support for gay marriage to include civil unions. I could then just change the line of code creating the supportgmar variable to: politics$supportgmar &lt;- politics$gaymarriage==&quot;Support gay marriage&quot; | politics$gaymarriage==&quot;Civil unions&quot; Not Everything Goes Into Your Script You don’t need to put every command into your script. The script should provide a narrative of your analysis (the part that you want to be easily reproducible), not a log of every single command you ran. Sometimes you may try out some exploratory commands or may just need to get some basic information about a variable. For example, in the script above, I first had to remember what the names were for the categories of the gaymarriage variable. To figure this out, I typed: table(politics$gaymarriage) ## ## No legal recognition Civil unions Support gay marriage ## 773 992 2473 From there I could see that the category I wanted was called “Support gay marriage” and I could use that in my script. However, there was no need to put this command into my script because it wasn’t producing anything that I needed to know or that was necessary for later commands in the script. Commenting for Sanity There is one big thing missing from the scripts listed above: comments! Comments are crucial for good script writing. Comments are lines in your script that will not be processed by R (or Stata). In R, you can create single line comments by using the pound sign (#). Anything after the pound sign will be ignored by R until a new line. You should use these comments to explain what the code is doing. You can also use them to help visually separate the script into sections for easier readability. Comments help you remember what you were doing when you come back to a project you haven’t worked on for weeks or months. They are also useful for other people who might end up reading your code (co-authors, advisers, etc). Increasingly, academics are being asked to do more “open” research where we make our code available. As you write code, its useful to think of it as something that will eventually be seen by other people and thus needs to be well documented. Here is the script from above, but now with some helpful commenting: ################################################################# # Aaron Gullickson # Program to analyze the differences in support for gay marriage # between evangelical christians and all those of other religious # affiliations ################################################################# #### DATA ORGANIZATION #### #load the politics dataset. Note that this command will not work if #you have the dataset loaded into a different directory. load(&quot;politics.RData&quot;) #dichotomize both the support for gay marriage and the religious variable politics$supportgmar &lt;- politics$gaymarriage==&quot;Support gay marriage&quot; politics$evangelical &lt;- politics$relig==&quot;Evangelical Protestant&quot; #### ANALYSIS #### #create a crosstab tab &lt;- table(politics$supportgmar, politics$evangelical) tab #Distribution of support conditional on religion prop.table(tab, 2) #Calculate the odds ratio of support OR &lt;- tab[1,1]*tab[2,2]/(tab[1,2]*tab[2,1]) OR Even if you don’t understand what the code here is doing yet, you can at least get a sense of what it is supposed to be doing. Notice that I use multiple pound signs to draw attention to the header and larger sections. For a script this small, the division between DATA ORGANIZATION and ANALYSIS is probably overkill, but for larger scripts, this sectioning can be very useful in helping to easily distinguish different components of the analysis. One nice feature of RStudio is that if you surround your comments with at least four pound signs on either side, then the outline of your script will show these sections and you can navigate to them. You can also go to Code &gt; Insert Section to add a section with a somewhat different look. Don’t Overcomment While commenting is necessary for good script writing, more commenting is not always better. The key issue is that you need to think of commenting as documentation of your code. If you change the code, you also need to change the documentation. If you change the code, but not the documentation, then you will actually make your code more confusing to other readers. Only write documentation that you will actively maintain. The most common error that novices make is to use comments to describe the results of their code. This is very bad practice, because as you make changes to your data and methods, these results are likely to change and if you don’t update your comments, there will be a lack of alignment between your real results and your documentation of the results. Later in the term, we will learn a better way to separate out the reporting of results in a “lab notebook” fashion, from comments in scripts. "],
["object-types.html", "Object Types", " Object Types R is an object-oriented programming language. This means that within R, you can save a variety of objects to memory. These objects will show up in the upper right panel in the environment tab in RStudio when you create them. A variety of functions can be applied to objects and in many cases, the same function may produce different results when applied to different kinds of objects. We will work most commonly with the data.frame object, but it is useful to know some other basic object types to understand how R works. Objects can have different types and R will often handle them differently depending on their type. However, type is defined in two different ways. Each object has a mode and a class. The class of an object is typically what determines how it is handled by other functions. The mode is most important in terms of the “atomic” or “basic” modes of R objects. These are the basic-building blocks of everything else. Atomic Modes The three most important atomic modes for our purposes are: numeric: records a numeric value. character: records a set of characters. This is often called a “string” in computer science parlance. logical: records either a TRUE or FALSE value. In computer science parlance, this is called a “boolean.” There is also a fourth type complex for things like imaginary numbers, but we won’t really need to worry about that. Lets try assigning values to an object of each mode: a &lt;- 3 b &lt;- &quot;bob said&quot; c &lt;- TRUE a ## [1] 3 mode(a) ## [1] &quot;numeric&quot; class(a) ## [1] &quot;numeric&quot; b ## [1] &quot;bob said&quot; mode(b) ## [1] &quot;character&quot; class(b) ## [1] &quot;character&quot; c ## [1] TRUE mode(c) ## [1] &quot;logical&quot; class(c) ## [1] &quot;logical&quot; note that for these simple objects the mode is the same as the class. These objects will now show up in my upper right panel. However, programming with these simple objects with one value is not very useful. What we usually really want is an aggregation of many of these values. There are a variety of ways we can do this. Vectors and Matrices Vectors If you want to put a bunch of values of the same mode together (don’t call it a “list” because that means something else, see below), you can do that with a vector. You can create a vector with the concatenation function c. Lets do that below: name &lt;- c(&quot;Bob&quot;,&quot;Juan&quot;,&quot;Maria&quot;,&quot;Jane&quot;,&quot;Howie&quot;) age &lt;- c(15,25,19,12,21) ate_breakfast &lt;- c(TRUE,FALSE,TRUE,TRUE,FALSE) name ## [1] &quot;Bob&quot; &quot;Juan&quot; &quot;Maria&quot; &quot;Jane&quot; &quot;Howie&quot; age ## [1] 15 25 19 12 21 ate_breakfast ## [1] TRUE FALSE TRUE TRUE FALSE mode(name) ## [1] &quot;character&quot; class(name) ## [1] &quot;character&quot; mode(age) ## [1] &quot;numeric&quot; class(age) ## [1] &quot;numeric&quot; mode(ate_breakfast) ## [1] &quot;logical&quot; class(ate_breakfast) ## [1] &quot;logical&quot; Note that the mode and class of each vector is given by the mode of the values that makes up the vector. As you can see, vectors are basically equivalent to variables for the purpose of data analysis. We can even calculate values like the mean for numeric and logical vectors: mean(age) ## [1] 18.4 mean(ate_breakfast) ## [1] 0.6 How can you calculate a mean for a logical vector? R automatically converts logical values to numeric values where TRUE=1 and FALSE=0 when it seems like a numeric value is needed. Therefore, the mean is tellling us that 60% of respondents ate breakfast. You can also force vectors (and some other objects) into a different basic type: as.character(age) ## [1] &quot;15&quot; &quot;25&quot; &quot;19&quot; &quot;12&quot; &quot;21&quot; as.numeric(ate_breakfast) ## [1] 1 0 1 1 0 as.numeric(name) ## Warning: NAs introduced by coercion ## [1] NA NA NA NA NA Note that this didn’t work so well when converting a character string to a number and I ended up with a set of missing values (NA). Matrices A matrix is just an extention of a vector, but two dimensional instead of one dimensional. We can use the matrix command to turn a vector into a matrix, by speficying the number of rows and columns. x &lt;- matrix(c(4,5,3,9,7,8), 3, 2) x ## [,1] [,2] ## [1,] 4 9 ## [2,] 5 7 ## [3,] 3 8 mode(x) ## [1] &quot;numeric&quot; class(x) ## [1] &quot;matrix&quot; I created a matrix of numeric values with three rows and two columns. Notice that when putting all of these values, R tries to fill up each column before moving on to the next column. Its also worth noting that the mode and class of my matrix are no longer the same. This means that I can specify specific functions that will apply to the class of matrix. The mode tells me what kind of values I have within the matrix. I can also create a matrix by binding together vectors into different rows (rbind) or columns (cbind). a &lt;- c(4,5,3) b &lt;- c(9,7,8) cbind(a,b) ## a b ## [1,] 4 9 ## [2,] 5 7 ## [3,] 3 8 rbind(a,b) ## [,1] [,2] [,3] ## a 4 5 3 ## b 9 7 8 This might seem like a good way to create a full dataset from the variables I created above, but there is a problem: cbind(name, age, ate_breakfast) ## name age ate_breakfast ## [1,] &quot;Bob&quot; &quot;15&quot; &quot;TRUE&quot; ## [2,] &quot;Juan&quot; &quot;25&quot; &quot;FALSE&quot; ## [3,] &quot;Maria&quot; &quot;19&quot; &quot;TRUE&quot; ## [4,] &quot;Jane&quot; &quot;12&quot; &quot;TRUE&quot; ## [5,] &quot;Howie&quot; &quot;21&quot; &quot;FALSE&quot; A matrix has to have values of the same atomic mode. In most cases, if there is any character vector in the binding, then everything will get converted to characters. We will see a better way to create a dataset (spoiler: the data.frame object) below. Note there is an extension to the matrix object called the array which generalizes it to n-dimensions rather than two. However, we will not make much use of that in this class. Indexing Vectors and Matrices What if I want to know a specific value in my vector or matrix. Lets say I want to know the name of the fourth person in my name vector. You can easily get this value by indexing the vector or matrix with square brackets. In my case: name[4] ## [1] &quot;Jane&quot; Because a vector only has one dimension, I only need one index. In the case of matrices, you will need two numbers, separated by a comma. If you want to get an entire row or column, you can leave one of the indices blank, but you still need the comma. x ## [,1] [,2] ## [1,] 4 9 ## [2,] 5 7 ## [3,] 3 8 #value in 2nd row, 1st column x[2,1] ## [1] 5 #2nd row x[2,] ## [1] 5 7 #1st column x[,1] ## [1] 4 5 3 #1st and 2nd row x[c(1,2),] ## [,1] [,2] ## [1,] 4 9 ## [2,] 5 7 Factors You will note that we don’t yet have a representation for categorical variables. Lets say for example that I wanted to include highest degree received for my respondents from above. I could create this as a character variable: high_degree &lt;- c(&quot;Less than HS&quot;, &quot;College&quot;, &quot;HS Diploma&quot;, &quot;HS Diploma&quot;, &quot;College&quot;) summary(high_degree) ## Length Class Mode ## 5 character character However, this is not very satisfying because there is very little that can be done with categorical variables. As you see the summary command failed to produce anything useful. What I want to do is turn this into a factor. A factor in R is the standard object for coding categorical variables. Each value is actually recorded with a mode of “numeric” but the object also contains a set of labels that provide the meaning of each level. Almost all functions in R will know how to handle these factor objects correcly. To create a factor object, I can just apply the factor function to my vector: high_degree_fctr &lt;- factor(high_degree) levels(high_degree_fctr) ## [1] &quot;College&quot; &quot;HS Diploma&quot; &quot;Less than HS&quot; summary(high_degree_fctr) ## College HS Diploma Less than HS ## 2 2 1 mode(high_degree_fctr) ## [1] &quot;numeric&quot; class(high_degree_fctr) ## [1] &quot;factor&quot; Now the summary command gives me a table of frequencies for each category. Re-ordering categories in factors The only problem with my factor is that this is an ordinal variable and the categories are backwards with “College” first and “Less than HS” last. This is because R sorts alphabetically by default. In order to ensure a specific order to the categories in the factor, I will need to specify the levels argument in the factor function and explicitly write out the order I want: high_degree_fctr &lt;- factor(high_degree, levels=c(&quot;Less than HS&quot;,&quot;HS Diploma&quot;,&quot;College&quot;)) levels(high_degree_fctr) ## [1] &quot;Less than HS&quot; &quot;HS Diploma&quot; &quot;College&quot; summary(high_degree_fctr) ## Less than HS HS Diploma College ## 1 2 2 Another option is to use the relevel function on a factor to just change the very first level of a factor to something else: levels(high_degree_fctr) ## [1] &quot;Less than HS&quot; &quot;HS Diploma&quot; &quot;College&quot; levels(relevel(high_degree_fctr,&quot;HS Diploma&quot;)) ## [1] &quot;HS Diploma&quot; &quot;Less than HS&quot; &quot;College&quot; Logical Values and Boolean Statements One of the most important features of all computer programming languages is the ability to create statements that will either evaluate to a “boolean” value of TRUE or FALSE (a “logical” value in R parlance). These kinds of statements are called boolean statements. The following basic operators will allow you to make boolean statements in R. Operator Meaning == equal to != not equal to &lt; less than &gt; greater than &gt;= less than or equal &lt;= greater than or equal Note that the “equal to” syntax is a double-equals. This is because the single equals is used for assignment of values to objects. As a simple example, lets say that I wanted to identify all respondents from my data above that were 18 years of age or older: age&gt;=18 ## [1] FALSE TRUE TRUE FALSE TRUE You can use factor variables in boolean statements of equality as well, but you need to use the character string variables. Lets say I want to identify all respondents with a college degree: high_degree==&quot;College&quot; ## [1] FALSE TRUE FALSE FALSE TRUE A very important feature of boolean statements is the ability to string together multiple boolean statements with a &amp; (AND) or | (OR) in order to make compound statement. Lets say I wanted to identify all respondents who had either a high school diploma or a college degree: high_degree==&quot;College&quot; | high_degree==&quot;HS Diploma&quot; ## [1] FALSE TRUE TRUE TRUE TRUE Lets say I want to find all respondents who are between the ages of [20,25): age&gt;=20 &amp; age&lt;25 ## [1] FALSE FALSE FALSE FALSE TRUE You can also use parenthesis to ensure that your compound boolean statements are interpreted in the correct order. (age&gt;=20 &amp; age&lt;25) &amp; (high_degree==&quot;College&quot; | high_degree==&quot;HS Diploma&quot;) ## [1] FALSE FALSE FALSE FALSE TRUE Another useful option is the ability to put a ! sign in front of a boolean variable to indicate “not”. Lets say I wanted to find all respondents who had NOT eaten breakfast: !ate_breakfast ## [1] FALSE TRUE FALSE FALSE TRUE Missing Values Missing values are a common feature of most real-world data. They can exist for a variety of reasons, but item non-response (i.e. respondent declined to answer a specific question) is one of the most common reasons. In R, missing values are represented with the NA value. missing values can exist for any of the modes we have discussed. Lets insert a missing value into the age vector that we have been using: age[4] &lt;- NA age ## [1] 15 25 19 NA 21 Watch what happens now when we try to calculate the mean of age: mean(age) ## [1] NA The mean is missing! This is the default behavior for many functions in R. If the values you feed in have missing values, R will return a missing value. R wants to be sure that you explicitly decide how to treat missing values. In Soc 513, we will learn about other ways of dealing with missing values, but our approach this term will be to simply remove observations that have missing values (i.e. casewise deletion) and then calculate the appropriate statistics. In many of the functions in R, this can be accomplished by setting the argument na.rm to TRUE: mean(age, na.rm=TRUE) ## [1] 20 Another useful function to know is the is.na function. If you feed in a vector of values, this function will return a logical vector that evaluates to TRUE if a value is missing. is.na(age) ## [1] FALSE FALSE FALSE TRUE FALSE Combining this with the ! from the previous section, we can easily create a function that tells us which observations have non-missing values: !is.na(age) ## [1] TRUE TRUE TRUE FALSE TRUE Lists Lists are one of the most flexible types of standard objects. Lists are just collections of different objects and the objects can be of different types and dimensions. You can even put lists into lists and end up with lists of lists. Lets put our four variables into a list: my_list &lt;- list(name, age, ate_breakfast, high_degree_fctr) my_list ## [[1]] ## [1] &quot;Bob&quot; &quot;Juan&quot; &quot;Maria&quot; &quot;Jane&quot; &quot;Howie&quot; ## ## [[2]] ## [1] 15 25 19 NA 21 ## ## [[3]] ## [1] TRUE FALSE TRUE TRUE FALSE ## ## [[4]] ## [1] Less than HS College HS Diploma HS Diploma College ## Levels: Less than HS HS Diploma College mode(my_list) ## [1] &quot;list&quot; class(my_list) ## [1] &quot;list&quot; In this case, each item in the list is a vector of the same length but they don’t need to be. Accessing elements of the list You will notice a lot of brackets in the list output above. To access an object at a specific index of the list, I need to use double square brackets. Lets say, I wanted to access the third object (ate_breakfast): my_list[[3]] ## [1] TRUE FALSE TRUE TRUE FALSE If I want to access a specific element of that vector, I can follow up that double bracket indexing with single indexing: my_list[[3]][4] ## [1] TRUE My fourth respondent did eat breakfast. Good to know. There is another way to access objects within the list but to do this, I need to provide a name for each object in the list. I can do this within the initial list command by using a name=value syntax for each object: my_list &lt;- list(name=name, age=age, ate_breakfast=ate_breakfast, high_degree=high_degree_fctr) Now, I can call up any object by its name with the basic syntax list_name$object_name. Lets do that for age: my_list$age ## [1] 15 25 19 NA 21 mean(my_list$age, na.rm=TRUE) ## [1] 20 You will notice in RStudio that when you type the “$”, it brings up a list of all the names you could want. You can select the one you want and tab to complete. Thanks, RStudio! The lapply command is awesome Lets say that I just want to get a summary of each object in my list: summary(my_list) ## Length Class Mode ## name 5 -none- character ## age 5 -none- numeric ## ate_breakfast 5 -none- logical ## high_degree 5 factor numeric Well, that was not super helpful. R is giving me a summary of the list itself rather than a summary of each object in the list. What if I want to apply the same function to every object in the list? This is exactly what the lapply command does. Feed in a list and a function (even a custom function) and it will sequentally apply that function to each object in the list: lapply(my_list, summary) ## $name ## Length Class Mode ## 5 character character ## ## $age ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 15 18 20 20 22 25 1 ## ## $ate_breakfast ## Mode FALSE TRUE ## logical 2 3 ## ## $high_degree ## Less than HS HS Diploma College ## 1 2 2 Thats much better. It won’t be immediately obvious, but lapply turns out to be a very powerful and helpful tool. We will learn more about it later this term. Data Frames The list was a nice way to organize my data, but it wasn’t ideal because it didn’t represent data in the two-dimensional observations-on-the-row and variables-on-the-columns way we expect most datasets to be typical organized. This is where the data.frame object comes in. This is the object that we will mostly work directly with in this class and the one you are most likely to work with in real projects. The data.frame object is basically a special form of a list in which each object in the list is required to be a vector of the same length, but not necessarily the same mode and class. The results can be displayed like a matrix and the same kinds of options for indexing that are available for matrices can be used on data.frames. Lets put the variables into a data.frame: my_data &lt;- data.frame(name, age, ate_breakfast, high_degree=high_degree_fctr) my_data ## name age ate_breakfast high_degree ## 1 Bob 15 TRUE Less than HS ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 4 Jane NA TRUE HS Diploma ## 5 Howie 21 FALSE College Now that looks like a dataset. Note that by default it just used the name of the object as the column name. However, I specifically changed this behavior for high_degree. We can run a summary on the whole dataset now: summary(my_data) ## name age ate_breakfast high_degree ## Bob :1 Min. :15 Mode :logical Less than HS:1 ## Howie:1 1st Qu.:18 FALSE:2 HS Diploma :2 ## Jane :1 Median :20 TRUE :3 College :2 ## Juan :1 Mean :20 ## Maria:1 3rd Qu.:22 ## Max. :25 ## NA&#39;s :1 It is now treating the name variable like a factor. By default, R will convert all character variables into factors when including them in a data.frame. I can also access any specific variable with the same “$” syntax as for lists: mean(my_data$age, na.rm=TRUE) ## [1] 20 I can also use the same indexing as for matrices to retrieve particular values: my_data[3,2] ## [1] 19 Renaming Variables in a data.frame Vectors, matrices, lists, and data.frames can all have names associated with their elements. You have already seen an example of specifying names in the creation of the list and data.frame objects. But you can also change names of elements after creation. We will focus here specifically on the case of data frames, but much of this is generalizable to other objects as well. the rownames and colnames command can be used to both retrieve and set the row and column names, respectively for a data frame. The colnames command is generally more useful because we typically care more about variable names than observation names. colnames(my_data) ## [1] &quot;name&quot; &quot;age&quot; &quot;ate_breakfast&quot; &quot;high_degree&quot; Lets say I decided that I wanted to have all my variable names capitalized. I can easily change this by just assigning a new character vector to this colnames command: colnames(my_data) &lt;- c(&quot;Name&quot;,&quot;Age&quot;,&quot;Ate_breakfast&quot;,&quot;High_degree&quot;) my_data ## Name Age Ate_breakfast High_degree ## 1 Bob 15 TRUE Less than HS ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 4 Jane NA TRUE HS Diploma ## 5 Howie 21 FALSE College I can also use indexing of the colnames to just change specific variable names. Lets say I decided that “Ate_breakfast” was too long: colnames(my_data)[3] &lt;- &quot;Breakfast&quot; my_data ## Name Age Breakfast High_degree ## 1 Bob 15 TRUE Less than HS ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 4 Jane NA TRUE HS Diploma ## 5 Howie 21 FALSE College Subsetting Data Frames One of the most common tasks of organizing data is slicing and dicing datat into smaller subsets. There are two cases that are common. Subsetting Observations You may want only a subset of observations by some logical characteristics (e.g. only women, people born after 1975, people who grew up in Lake Geneva WI, member-states of the OECD). One nice feature of R is that you can use the same indexing of rows as I discussed above but instead of specific index numbers, you can provide a boolean statement and only observations that evaluate to TRUE will be kept. Lets say that you want limit the dataset we have been using to only those who completed college: my_data[my_data$High_degree==&quot;College&quot;,] ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 5 Howie 21 FALSE College One “gotcha” with this approach is missing values in your boolean statement. Lets try to subset the data to only those respondents 18 years and older: my_data[my_data$Age&gt;=18,] ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## NA &lt;NA&gt; NA NA &lt;NA&gt; ## 5 Howie 21 FALSE College Notice we get one full row of NA values. This was because of the one missing value on age. To avoid this, we first need to tell R to only keep cases that do not have missing values on age: my_data[!is.na(my_data$Age) &amp; my_data$Age&gt;=18,] ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 5 Howie 21 FALSE College Now it works. However, we can also use the subset command to do the same thing and the subset command will be more robust to this missing value problem. The first argument to the subset command is the data frame you want to subset and the second is a boolean statement about the variables in that data frame. When this boolean statement evaluates to true for an observation it will be kept in the subset. subset(my_data, Age&gt;=18) ## Name Age Breakfast High_degree ## 2 Juan 25 FALSE College ## 3 Maria 19 TRUE HS Diploma ## 5 Howie 21 FALSE College Notice that I did not have to use the full my_data$Age specification in my boolean statement, because the subset command knows that this variable must be in the dataset that I fed in. I also did not have to deal with the whole business of missing values. In general, the subset command produces cleaner and more readable code than bracketing and indexing. Subsetting by variables Sometimes you want to drop variables. This may be because you have used those variables to construct another variable and you don’t need them any longer or maybe your data source just contains a lot more variables than you need. I am a big proponent of dropping irrelevant variables. It keeps your dataset cleaner and easier to read for others. You can use indexing of columns in your data frame to choose variables that you want to keep. Lets say that I only wanted to keep age and highest degree in the dataset that we have been looking at: my_data[,c(&quot;Age&quot;,&quot;High_degree&quot;)] ## Age High_degree ## 1 15 Less than HS ## 2 25 College ## 3 19 HS Diploma ## 4 NA HS Diploma ## 5 21 College You can also use the negative sign in front of a number index to remove it rather than keep it. my_data[,c(-1,-3)] ## Age High_degree ## 1 15 Less than HS ## 2 25 College ## 3 19 HS Diploma ## 4 NA HS Diploma ## 5 21 College You can also use the select argument in the subset command to keep only certain variables. subset(my_data, select=c(&quot;Age&quot;,&quot;High_degree&quot;)) ## Age High_degree ## 1 15 Less than HS ## 2 25 College ## 3 19 HS Diploma ## 4 NA HS Diploma ## 5 21 College You can even use the subset command to simultaneously subset observations and drop variables. Lets say that I wanted to use my Name variable as row names for my dataset and I also want to drop any cases that are missing on age: rownames(my_data) &lt;- my_data$Name my_data2 &lt;- subset(my_data, !is.na(Age), select=c(&quot;Age&quot;,&quot;Breakfast&quot;,&quot;High_degree&quot;)) my_data2 ## Age Breakfast High_degree ## Bob 15 TRUE Less than HS ## Juan 25 FALSE College ## Maria 19 TRUE HS Diploma ## Howie 21 FALSE College "],
["pretty-pictures.html", "Pretty Pictures", " Pretty Pictures One of the best features of R is the ability to make beautiful graphs. R gives the user an incredible amount of control over exactly how your graphs appear. If you know enough, you can virtually make any graph that you can imagine in your head. This flexibility comes at a cost, however. The curve in learning graphics is quite substantial, because of the number of parameters that can be controlled. Nonetheless, if you intend to do quantitative data analysis, then its well worth the effort. In my opinion, figures are almost always a better way to express a result than a table. If the figure is well-designed it will make your results much more intelligible to a broader audience because you have literally helped them to visualize it. In this lab, I am going to show you two techniques for plotting in R. First, I am going to show you how to plot things in R using the basic plotting functions, sometimes called “base plot.” Base plotting in R gives you full control over pretty much everything you could want to stick on a canvas, but sometimes it gives you too much control and you can end up fiddling around a lot to get things to look just how you want them to. So, I am also going to show you how to plot in R using the ggplot2 library. ggplot has become so popular as a method of plotting that many people prefer it to base plot. The “gg” in ggplot refers to teh “graphics of grammar.” The basic idea of ggplot is that rather than control everything about your plot, you give R a “grammar” of what data you want to graph, how you want to graph, and how you want to tie aesthetics to the different pieces of data. In future versions of this course, I plan to transition many of the graphs we use to ggplot although most slides are still written in base plot. There are two different ways you should use pictures in your work. The first method is to use figures to visualize what you data looks like. This is what we have been learning already in class. Histograms, barplots, boxplots, and scatterplots are all examples of trying to visualize your data. Most work on how to make figures is devoted to the topic of visualizing your data like this. However, there is a second way to use pictures, which I think is actually equally important and under-utilized. You can use figures to vizualize your findings. The difference here might not be immediately obvious, partly because we haven’t talked about “models” yet, which is the primary way that results are expressed in quantitative work. Let me give you an example from my own work that may help to communicate what I mean. Here is a figure from an AJS article that shows how to think about the results of a model I ran that tries to predict the number of lynchings in a county by the inequality in occupation between individuals classified as black and mulatto and the inequality between individuals classified as white and mulatto or black (i.e. “African ancestry”). This is a heat (or topological) map of risk where dark areas indicate greater risk (of lynching). The point was to show that the effect of each variable produces opposite effects depending on the level of the other variable. I am not describing data here, but rather the results of a model of lynching risk that is based on the underlying county-level data. Since we don’t know much about these types of models yet, we will focus for now on using graphs to describe data. The data we will look at come from one of my current projects. In this project, my colleague (Nicole Marwell) and I have data on social service contracts awarded by the City of New York to non-profit organizations working within the city. The data I am using here are created by aggregating all the money allocated to a particlar health area (a bureaucractic neighborhood boundary in NYC) between the years of 2009 and 2010. We combine that data with data on the poverty rate for each health areas and also divide by the population in a health area to get an estimate of the social service funding per capita. Here is a peak at what the data look like: head(nyc) ## health_area amtcapita poverty unemployed income borough popn ## 1 10110 29.168592 22.77850 11.324410 40287.02 Manhattan 27983.43 ## 2 10120 109.451055 22.62417 10.179587 43266.45 Manhattan 20235.00 ## 3 10210 216.306532 30.08883 11.616439 29015.87 Manhattan 28688.00 ## 4 10221 29.148974 27.16760 17.279031 33987.68 Manhattan 27161.08 ## 5 10222 2.422527 12.10421 8.580756 66350.84 Manhattan 13372.73 ## 6 10300 506.985063 27.39747 13.606836 32591.55 Manhattan 16035.99 ## lincome poverty.z unemployed.z lincome.z deprivation.summated ## 1 10.60378 0.1623016 0.65034173 0.2704751 0.3902136 ## 2 10.67513 0.1496155 0.34143111 0.1183988 0.2195641 ## 3 10.27560 0.7632204 0.72914069 0.9699954 0.8871098 ## 4 10.43375 0.5230918 2.25709249 0.6328925 1.2296246 ## 5 11.10271 -0.7151376 -0.08998557 -0.7929740 -0.5757443 ## 6 10.39181 0.5419873 1.26621463 0.7222968 0.9116593 We have the numeric code of the health area, the amount of funding per capita, the poverty rate as a percentage of the population, a numeric code for borough (Manhattan, Brooklyn, Bronx, Queens, Staten Island), and the population size. You can download this data in the files section of Canvas. Base Plot For this example, we are going to use a scatterplot to look at the association between funding per capita and poverty rates. Here is the ultimate figure that we want to end up with: There is a lot going on in this figure. Lets identify a few of the nifty features: dots are color-coded by borough the scale of the y-axis is logarithmic which means that an increase of one unit is actually a multiplicative increase of 10 (sort of like the richter scale). The size of dots is scaled to the population size of the health area. There is a best-fitting line drawn through the points. It looks positive. the y-axis has a dollar-sign and commas to make the numbers easier to read and the x-axis has a % marker to make clear the units. there are two different legends drawn on the outside margins of the figure. Thats a lot of stuff. Lets start from the basics and build our way up to this final model. Lets start by just running the basic plot command for a scatterplot: plot(nyc$poverty, nyc$amtcapita) Ok, thats a lot different. Lets deal with the biggest substantive problem. The amount per capita is so heavily right-skewed that it is difficult to see the relationship because almost all of the data points are “squished” down at the bottom of the y-axis. This can be resolved by using whats called a logarithmic scale such that each tick mark indicates not an additive unit increase on the y-axis but a multiplicative increase. We can do this easily by specifying “y” as an option to the log argument in plot: plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;) Ok, that looks better. This graphical approach is related to the idea of tranformations that we will discuss later in the term. Now we can better see that there is a positive effect. However, the numbers on the y-axis tick-marks look horrible. Instead of using the default tickmarks, I can specify my own axis. A quick check using summary reveals that the maximum amount per capita is $12,290. If I am going up by multiples of ten, the next tick mark above this would be 100,000 (from 10,000, to 100,000). So, I am going to do a few things in the next command. First, I am going to use yaxt=\"n\" to tell R not to draw the default y-axis. Then I am going to use ylim=c(0.1,100000) to specify the upper and lower limits on my y-axis. plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000)) Notice that there is now no y-axis tickmarks. I can define those myself with the axis command. plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=1) The 2 argument tells R to draw the axis for the y-axis and the at argument allows me to specify where I want the tick marks. The las argument specifies that I want my tickmark labels to be perpendicular to the axis, for easier reading. There are still a couple of problems with this axis. First, the label for the y-axis is now overlapping with the tickmark labels. Second, the tickmark labels are in an ugly exponential notation. These can both be fixed. First, I can fix the tickmark labels. There are a variety of functions such as paste, format, and formatC which will allow you to turn numbers into pretty well-formatted character strings. In this case, I am going to use those functions to write out the full number with a comma at the thousands place and put a dollar sign in front: plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) I am not going to go over the details of these commands here, but you can see how the tickmark labels are much easier to read now. I still have the problem of the overlapping label for the y-axis and now, the left margin is also not quite large enough for the top label which is being cut-off. To adjust the margin, let me introduce the par command. The par command can be called before the plot command to set up the parameters of the plot. If you look at the help file for par you will see that there are a huge number of parameters that we can adjust. In our case, we want to adjust the mar argument which is a vector of four numbers defining the size of the bottom, left, top, and right margins, respectively. The default for these is c(5,4,4,2). I will increase the left margin slightly and reduce the right margin since we won’t use it. In addition, I am going to remove the default y label by specifying ylab=\"\". par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;amount per capita&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) The final step is to define my own y-axis label. I can do this with the title command. If you look at the help file for the title command, you will see that in addition to telling R which title you want to add, you can specify the number of lines away from the graph for the title, which allows us to control its placement. After experimenting around a bit, I discovered that line=5 looked nice. par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) The y-axis now looks pretty good. I was able to override the default y-axis with the yaxt=\"n\" and ylab=\"\" arguments to plot and then use the axis and title commands to customize tick marks and labels. Now, I will do the same to the x-axis: par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) In this case, I didn’t have to define a separate title, because the default x-axis label fits fine. I also don’t need the las command because the default has the correct alignment. Now that I have my axes well-labeled. I can focus on the actual plot. The default “dots” for scatterplots in R are prety ugly, but there are lots of options for better dots. You can specify the shape and style of the dot with the pch argument in plot. If you use ?points, the help file will give you a list of the numeric codes that correspond to different kinds of dots. I usually use pch=21 because it will give me a circle that has a separate border and fill color. The border color can be specified by the col argument an the fill color can be specified by the bg argument. I can also use the cex option to define the size of the dots (relative to the default of 1). I will use this now to create circles of half the average size with a red fill and a black border. model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=&quot;red&quot;, col=&quot;black&quot;, cex=0.5) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) This is now pretty good, but the final touch here is to vary the size of the dots by the size of the health area and the fill color by the borough of the health area. R is very flexible about these types of arguments. If I give a vector of numbers for the size or a vector of colornames for the fill color, R will assume that those colors correspond to the individual dots and will allow for variation in the size and color. For example, lets just feed in population size divided by 10,000 to cex. par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=&quot;red&quot;, col=&quot;black&quot;, cex=nyc$popn/10000) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) Well, that sort of worked but some of those dots are way too big and some are way too small. The variation in size between health areas is so large that any kind of linear scaling of the population size is going to result in this problem. There are a variety of potential solutions to this, but our earlier use of logarithmic scales suggests an easy one. Logging the population values will allow for differences in size but at a diminishing scale difference. After experimentation, I decided that dividing population size by 3000 and logging with a base of 5 produced good size variation. par(mar=c(5,6,1,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=&quot;red&quot;, col=&quot;black&quot;, cex=log(nyc$popn/3000,5)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) The final step is to color-code the dots. I could put in a vector of any five colors. However, it is important to think about accessibility for color-blind individuals and whether a color combination will show up well in print. There are many online resources for this sort of thing. I like ColorBrewer. Here I have specified five classes with a diverging scheme that are colorblind and print friendly. I wiould have selected qualitative scale but there are no color-blind options in that category for five classes. ColorBrewer gives me some options with hexadecimal color codes which I can feed into R. I first create a vector of the five color names and then I use the borough index to assign them in my plot command. color_choices &lt;- c(&quot;#ca0020&quot;,&quot;#f4a582&quot;,&quot;#f7f7f7&quot;,&quot;#92c5de&quot;,&quot;#0571b0&quot;) par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5)) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) We are almost there but I also want to add a best-fitting line. Normally, I could do this with the abline command as discussed in the Canvas section on the OLS regression line. However, that won’t work in this case because of the the logarithmic scale on the y-axis. Instead, I can create a sequence of poverty rate values and then based on a model, I can calculate the predicted amount per capita (note that you don’t know how to do this yet, so just hang tight). I can then feed those x and y values into a lines command to draw a line on my plot. model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) x &lt;- 0:60 y &lt;- exp(model$coef[1])*exp(model$coef[2])^x par(mar=c(5,6,3,1)) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5), bty=&quot;n&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) lines(x,y, lwd=3, col=&quot;grey20&quot;) I don’t want you to worry to much about the model part. The important feature I want to highlight here is that lines is one of a number of commands that include points, text, and mtext that you can use to later add other stuff to a plot that you have already made. In this case, I have added a straight line. The lwd argument defines the width of the line and the col argument defines the color of the line. I snuck in one other argument here that made a noticeable change. The argument bty defines how the border is drawn around the overall plot area. By setting this to “n”, I removed the border altogether, which I think gives it a cleaner look. The last step is to add some legends. Legends can be tricky. The first thing you have to figure out is where to place the legend. In my case, I would rather have the legend in the margin than in the main plot area, but R won’t do this by default. In order to do that, I need to specify an xpd=TRUE argument in the par command to allow writing output to the margins and not just the main plot area. In the legend command itself, I need to make a label for each component of the legend and then I need to specify how each component is identified. Lets start with the legend for boroughs. model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) x &lt;- 0:60 y &lt;- exp(model$coef[1])*exp(model$coef[2])^x par(mar=c(5,6,3,1), xpd=TRUE) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5), bty=&quot;n&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) lines(x,y, lwd=3, col=&quot;grey20&quot;) lg &lt;- legend(0, 100000, legend=c(&quot;Manhattan&quot;,&quot;Bronx&quot;,&quot;Brooklyn&quot;,&quot;Queens&quot;,&quot;Staten Island&quot;), pch=21, col=&quot;black&quot;, pt.bg=color_choices, ncol=5, cex=0.45, pt.cex=1.2, yjust=5) The first two arguments to legend give the x and y placement. The legend argument to legend (I know, its weird) gives the labels for the legend components. The pch argument tells the legend that I am using points and what their shape is. The col argument gives the border color for these points and the pt.bg argument gives their fill color. The ncol tells the legend to use five separate columns rather than a vertical alignment all in one column. The cex and pt.cex arguments indicate the size of the overall legend and the size of the dots, respectively. The yjust argument allows me to fudge the placement to get it just right. Notice, that I saved the output of legend to an object that I called lg. This is a very useful feature of plots. I want to draw my next legend for health area size next to this first legend, but I have no idea exactly how big the first legend will be, so its hard to know at what value of x to start it. I could guess a number here, but that might also change if I rescale the figure manually. However, if I look at the lg object, the information I am looking for is returned there: lg ## $rect ## $rect$w ## [1] 33.53141 ## ## $rect$h ## [1] 0.486 ## ## $rect$left ## [1] 0 ## ## $rect$top ## [1] 3.056 ## ## ## $text ## $text$x ## [1] 1.562143 8.190318 14.818492 21.446667 28.074842 ## ## $text$y ## [1] 2.813 2.813 2.813 2.813 2.813 In this specific case, I am looking for lg$rect$w which gives the width of the first legend. I can use that to set up the placement of my second legend: model &lt;- lm(I(log(nyc$amtcapita))~nyc$poverty) x &lt;- 0:60 y &lt;- exp(model$coef[1])*exp(model$coef[2])^x par(mar=c(5,6,3,1), xpd=TRUE) plot(nyc$poverty, nyc$amtcapita, log=&quot;y&quot;, yaxt=&quot;n&quot;, ylim=c(0.1,100000), ylab=&quot;&quot;, xaxt=&quot;n&quot;, xlim=c(0,60), xlab=&quot;Poverty rate&quot;, pch=21, bg=color_choices[nyc$borough], col=&quot;black&quot;, cex=log(nyc$popn/3000,5), bty=&quot;n&quot;) axis(2,at=c(0.1,1,10,100,1000,10000,100000), las=2, labels=paste(&quot;$&quot;,formatC(c(0.1,1,10,100,1000,10000,100000), format=&quot;d&quot;, big.mark=&quot;,&quot;), sep=&quot;&quot;)) title(ylab=&quot;amount per capita&quot;, line=5) axis(1,at=seq(from=0,to=60,by=10),labels=paste(seq(from=0,to=60,by=10),&quot;%&quot;,sep=&quot;&quot;)) lines(x,y, lwd=3, col=&quot;grey20&quot;) lg &lt;- legend(0, 100000, legend=c(&quot;Manhattan&quot;,&quot;Bronx&quot;,&quot;Brooklyn&quot;,&quot;Queens&quot;,&quot;Staten Island&quot;), pch=21, col=&quot;black&quot;, pt.bg=color_choices, ncol=5, cex=0.45, pt.cex=1.2, yjust=-0.7) legend(lg$rect$w*1.05, 100000, legend=c(&quot;5000 people&quot;,&quot;50,000 people&quot;,&quot;100,000 people&quot;), pch=21, col=&quot;black&quot;,pt.bg=&quot;grey&quot;,ncol=3, pt.cex=log(c(5000,50000,100000)/3000, 5), yjust=-0.7, cex=0.45) And there is the final product. Keep in mind that some of the final touches here are fairly complex. I am not expecting you to be able to produce graphs of this complexity tomorrow. The goal was to show you the richness and depth of graphing in R and to give you some reference points for beginning to build your own beautiful graphs. The plot function is one of the most basic functions for creating plots and once you get the basics down you can create a wide variety of two-dimensional plots. However, there are a variety of other functions that will draw more specific plots. We have already seen examples of pie, barplot, hist, and boxplot. Most of the options for customization that are available for plot are also available for these other functions. For example, in the code here I use the text function to plot the actual percentage values at the top of my bars for a barplot of movie maturity rating and to create lines for the y-axis at 10% intervals. percent &lt;- round(100*table(movies$Rating)/sum(table(movies$Rating)),1) b &lt;- barplot(percent, las=1, ylab=&quot;Percent&quot;, ylim=c(0,50), col=&quot;salmon&quot;) text(b[,1], percent+2, paste(percent, &quot;%&quot;, sep=&quot;&quot;)) abline(h=seq(from=10,to=50,by=10), lwd=0.5, col=&quot;grey80&quot;, lty=2) Another useful command that we haven’t learned yet is matplot which is short for matrix plotting. The matplot function will plot the values of one dimension of the matrix across the indices of the other dimension. This allows you to plot, for example, trend lines separately by different categories. Lets try it out by plotting the time trend in movie runtime separately by maturity rating. The first step in doing this is to calculate the mean of movie runtime by year and maturity rating using the tapply command. tab &lt;- tapply(movies$Runtime, movies[,c(&quot;Year&quot;,&quot;Rating&quot;)], mean) tab ## Rating ## Year G PG PG-13 R ## 2001 89.20000 101.43750 105.1667 107.1744 ## 2002 92.66667 97.42308 107.0139 104.3974 ## 2003 75.40000 96.08333 109.7143 108.1692 ## 2004 93.00000 99.65714 106.2027 105.8769 ## 2005 84.83333 100.76471 109.6024 107.8659 ## 2006 93.85714 99.30233 107.2118 109.6282 ## 2007 99.66667 99.37931 104.7324 107.8900 ## 2008 91.44444 101.50000 105.8750 106.4935 ## 2009 99.33333 98.24242 107.8481 104.1071 ## 2010 103.00000 101.79412 109.4559 102.1495 ## 2011 89.33333 100.84000 109.3297 101.5098 ## 2012 88.00000 99.87500 109.4400 103.1043 ## 2013 104.00000 100.81250 112.7971 103.4904 Now, I can feed this matrix into matplot to see the trend across time. In this case, I am going to leave off NC-17 and Unrated because the small number of movies here makes these measures very noisy. matplot(2001:2013, tab[,1:4], type=&quot;b&quot;, xlab=&quot;year&quot;, ylab=&quot;mean movie runtime&quot;, las=1) One interesting trend is that PG-13 movies have become longer than R movies, mostly because the runtime of R movies has gotten progressively smaller since 2006. G movies are also getting slight longer over time, but its highly variable from year to year. I could have also created this plot with the basic plot command and some lines commands, like so: plot(-1,-1, xlab=&quot;year&quot;, ylab=&quot;mean movie runtime&quot;, las=1, xlim=c(2001,2013), ylim=c(70,120), bty=&quot;n&quot;) lines(2001:2013, tab[,1], lwd=2, col=&quot;green&quot;) lines(2001:2013, tab[,2], lwd=2, col=&quot;blue&quot;) lines(2001:2013, tab[,3], lwd=2, col=&quot;purple&quot;) lines(2001:2013, tab[,4], lwd=2, col=&quot;red&quot;) legend(2001, 120, legend=c(&quot;G&quot;,&quot;PG&quot;,&quot;PG-13&quot;,&quot;R&quot;), lty=1, lwd=2, col=c(&quot;green&quot;,&quot;blue&quot;,&quot;purple&quot;,&quot;red&quot;), ncol=4, cex=0.7) The plot command here basically creates an empty canvas because I give a single point coordinate (-1,-1) that is outside the range of my xlim and ylim values. I can then use the lines command to write specific lines onto this blank canvas. ggplot Now, I want to show you how to plot that same scatterplot of non-profit funding in NYC using ggplot. Ggplot builds up a graph from layers. The first and most essential component is the function ggplot where I indicate the data I am using and the aesthetics that I want to be carried through to all of the other layers: ggplot(nyc, aes(x=poverty, y=amtcapita)) This command does not actually plot anything yet. It just sets up the basic structure of my plot by identifying the dataset and that I will use poverty as my x variable and amtcapita as my y variable. I then can add layers to this basic command using the “+” sign. For example, if I wanted to create a scatterplot by plotting points: ggplot(nyc, aes(x=poverty, y=amtcapita))+ geom_point() Now, I have a basic (and very ugly) scatterplot that is similar to what I started with in base plot. I can now add a variety of layers to that start to make a better graph. There are three types of layers I can add: geoms - these are a variety of geometric patterns such as points, bars, lines, etc. The coords that define the coordinate system used to plot the values. We typically won’t futz around with this much because everything is drawn on a Cartesian coordinate system, but it can be useful for maps and some other things. scales - these indicate how I want the scales of my various aesthetics to work. This can include the scaling of my x and y variables, but also things like color gradiations. labels - I can identify labels and themes to use. For example, let me use the scale_y_log10() function to re-scale my y-axis to a logarithmic basis: ggplot(nyc, aes(x=poverty, y=amtcapita))+ geom_point()+ scale_y_log10(labels=dollar) That looks better. Notice that I also gave the scale_y_log10 command an argument of labels. This argument identifies the specific labels I want to use for the tick mark. In this case, I am supplying a function from the scales library that turns raw numbers into formatted dollar amounts. From here I can add a variety of layers and aesthetics to enrich my graph. Let me first add aesthetics for color and size. I will also tranform borough into a proper factor variable so it displays more nicelyin the legend. ggplot(nyc, aes(x=poverty, y=amtcapita, size=popn, color=borough))+ geom_point(alpha=0.7)+ scale_y_log10(labels=dollar)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ theme_bw() By just adding in the borough and pop size as aesthetics, the graph was quickly adjusted. I didn’t have to fiddle around with exact sizing of the dots. Ggplot handled those details. Note that I also added a scale for the color using one of ggplot’s pre-defined palettes. I also used the argument of alpha=0.7 to add some transparency to points, which helps me deal with issues of overplotting. Finally, I used theme_bw() at the bottom to change to a black and white theme. I also want to add a line for the best-fitting OLS regressin line. The geom_smooth function will allow me to do this, although I will have to specify the method: ggplot(nyc, aes(x=poverty, y=amtcapita))+ geom_point(alpha=0.7, aes(color=borough, size=popn))+ geom_smooth(method=&quot;lm&quot;, color=&quot;black&quot;, se=FALSE)+ scale_y_log10(labels=dollar)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ theme_bw() I made a couple of important changes here that are quite subtle but important. First, I moves the aesthetics for color and size out of the ggplot command and put them into the geom_point. This is because I don’t want those aesthetics to apply to all geoms. I only want them to apply to geom_point. If I had left them in they would have affected the geom_smooth and we would have had five separate lines for each borough. I also added color=\"black\" to the geom_smooth command. Note that this is not part of an aesthetic call (e.g. aes()). It is not considered an aesthetic because we are just asking for the line to be a single color. Try surrounding that command in an aes and see what happens. I also used the se=FALSE. If I don’t do this then the line above will be surrounded by a confidence band, which may be good or bad. For our purposes, I did not want to clutter the graph. We are now pretty close to being complete, but I still need to label all of my axes and provide a title. I also want better labeling for the two legends. This can all be done with the labs command which we append to the entire plot: rm(percent) ggplot(nyc, aes(x=poverty/100, y=amtcapita))+ geom_point(alpha=0.7, aes(color=borough, size=popn))+ scale_x_continuous(label=percent)+ geom_smooth(method=&quot;lm&quot;, color=&quot;black&quot;, se=FALSE)+ scale_y_log10(labels=dollar)+ scale_color_brewer(palette=&quot;Dark2&quot;)+ theme_bw()+ theme(legend.position=&quot;right&quot;)+ labs(x=&quot;poverty rate&quot;, y=&quot;amount per capita&quot;, title=&quot;Non-profit funding to NYC health area by poverty rate&quot;, caption=&quot;Data from NYC, 2009-2010&quot;, color=&quot;Borough&quot;, size=&quot;Population&quot;) I did a couple of other things here as well. I added a scale_x_continuous so I could label the x tick mark labels as percents. I also added another theme command that would allow me to change the placement of the legends. I want to keep it on the right, but could have chosen “left”, “right”, “top”, or “bottom.” We now have a very nice looking graph. Ggplot can be a little overwhelming at first, but it has quite a few advantages over base plot. It is designed so that we have to fidget around less with things like the size of our labels, the exact placement of our legends, and the margins of our table. All of that just works internally, and we can focus on the “grammar of graphics”, i.e. the logic structure of what we are trying to say with our graph. Lets do one more example to show how flexible ggplot is. Lets look at the distribution of popularity by race in the Add Health data. Because we have one categorical and one quantitative variable, we want comparative boxplots. Here is our basic set up: ggplot(addhealth, aes(x=race, y=indegree))+ geom_boxplot()+ theme_bw() That works pretty well, However, its often better to display these boxplots horizontally so that we don’t have to worry about category labels overlapping. We can do that with ggplot with the coord_flip command (an example of a coordinate layer): ggplot(addhealth, aes(x=race, y=indegree))+ geom_boxplot()+ coord_flip()+ theme_bw() This is already pretty good. Notice that I don’t have to worry about specifying margins to make sure my category labels fit. Ggplot does that for me. I just need to apply labels and maybe a bit of tint to my boxplots. ggplot(addhealth, aes(x=race, y=indegree))+ geom_boxplot(fill=&quot;grey70&quot;)+ coord_flip()+ theme_bw()+ labs(x=NULL, y=&quot;Number of friend nominations received&quot;, title=&quot;Comparative boxplots of friend nominations by race&quot;, caption=&quot;Add Health data, Wave 1&quot;) Note that the labels for x and y refer to the logic decision of which value is x and y as defined in the aesthetics not the actual placement, which was reversed due to the coord_flip. Note also that I used NULL for the x label because the category labels and title are self-explanatory. There are also some more advanced geoms that do something similar to a boxplot. A popular one is the geom_violin which plots a mirror image of the density distribution. With ggplot, its as simple as swapping out my boxplot with the violin: ggplot(addhealth, aes(x=race, y=indegree))+ geom_violin(fill=&quot;grey70&quot;)+ coord_flip()+ theme_bw()+ labs(x=NULL, y=&quot;Number of friend nominations received&quot;, title=&quot;Comparative boxplots of friend nominations by race&quot;, caption=&quot;Add Health data, Wave 1&quot;) I encourage you to explore the online ggplot documentation. I would also highly recommend Kieran Healy’s new book, Data Visualization which uses ggplot extensively if you want to learn more about using ggplot most effectively. "],
["using-git.html", "Using Git", " Using Git Git is an example of a version control system. Version control systems allow you to track changes to files within a directory and to distribute changes to files to other collaborators. They also allow you to revert changes and to merge any changes made by different collaborators. Version control systems have been used by developers for decades to manage collaborative projects and to keep track of changes. For academics, version control systems have a lot to offer. This is particularly true for academics doing quantitative work, because much of the daily work flow of quantitative work resembles that of developers. Briefly here are some of the benefits of implementing version control for your projects. Research log: All changes to your project need to be “committed” to a repository with a brief description of your changes. You can look over the history of these commits to get a record of all the things you have done on your project. Nothing is lost: All changes that you committed to your repository are recoverable. If you realize that the code you changed isn’t working right, you can easily revert back to a prior commit or find the text/code that was changed and bring it back in piecemeal. Keep files tidy: The typical project folder for most academics will contain multiple versions of the same document, perhaps entitled something like “paper-1.docx”, “paper-v2.1_06272017.docx”, “paper_conference_revision.docx”, “paper_FINAL.docx”, “paper_FINAL_REALLY_THIS_TIME.docx”, and so on. Similarly, scripts are often littered throughout the directory without a clear indication of what scripts will do what and which ones are actually essential to the project. This is a mess. With version control, all of that messiness can go away. Because all changes are tracked by the version control system, there is no need to hang on to older versions of files. Collaborate intelligently: Because changes can be shared among multiple users, version control systems make collaboration much simpler. I don’t have to hunt through my emails to find a version of the file that my co-author sent me. I don’t have to remember to send out my changes to everyone and hope they also didn’t make changes. I just “push” my changes directly and “pull” in the changes of others. Back up your system: If you push your changes to another repository (highly encouraged), then you effectively back up your project. It also makes it easy to work on the same project across multiple computers because you can push and pull changes between the computers. Plain Text is Better Version control systems work best when your files are written in simple plain text rather than as binary files. Version control systems can get inside of a plain text file and document changes line to line. Version control can only tell that a binary file was changed, without information about the changes within the binary file. This makes is much easier to log, document, and merge changes in plain text files than in binary files. All of the scripts that we typically write are plain text files. A lot of the datasets that we use (but not all) are in plain text CSV or fixed-width format. Most academics, however, are more familiar with writing their papers in a WYSIWIG (What You See Is What You Get) document processing format like Microsoft Word, which saves files as a binary. To truly get the most out of a version control system, you might want to consider writing documents using a logical mark up language like R Markdown or LaTeX. We will discuss both of these options later in the term. In the meantime, I would recommend taking a look at Kieran Healy’s The Plain Person’s Guide to Plain Text Social Science which gives an easy introduction to getting outside the “office” approach to science. Git Git is a version control system designed by the legendary open-source programmer Linus Torvalds. It has become the de facto standard for version control in recent years. It is open-source, lightweight, free to use, and runs on all major platforms. Git should not be confused with a commercial service like GitHub. GitHub provides users with remote git repositories for sharing and collaborating on projects, but it is not git itself. Git at its most basic is a command-line program. All of the basic commands that I will discuss below can be run by opening up a console/terminal on your operating system and typing them in. That won’t be our primary way of accessing git, but I want you to know that when RStudio runs git, it is just performing the same commands that you could do directly from the command line. At the end of this document, I discuss the basic commands you can run from the command line with git. Here is a basic diagram of a git workflow. We will build up our understanding of this diagram in parts. Git Workflow Git installation You can install git on your system by downloading the appropriate files here and installing them on your computer. On most computers, this will not give you a graphical client to open, but it will install git “under the hood” of your computer and make it accessible to RStudio. In order to use git properly, you will need to tell it your name and email address. This information will be attached to any commit you make. You can do this from the command line, but since that is intimidating to most people, we can also do it from within RStudio. In RStudio, you will need to install the usethis package: install.packages(&quot;usethis&quot;) You can then run the use_git_config command to specify your user name and email: use_git_config(user.name = &quot;Your Name Here&quot;, user.email = &quot;myemail@uoregon.edu&quot;) Ideally, your email should be the same one that you provided as your primary email to GitHub. You should only have to setup this configuration once. You should now be ready to go. Setting up a repository The basic idea of git is that your project directory is stored in a repository. This repository keeps a record of all changes made to the project. The repository lives in the same place as your project directory, but it is not the same as the project directory. You can set up a new git repository from within RStudio using the “File &gt; New Project” menu option. From the dialog window that opens, select “New Directory” and then “New Project.” On the “Create New Project” window, be sure to check “Create a git repository” and then give the directory a name and location. You can see from the screenshot below that I am creating a new test_project on my desktop. Setting up new project with Git You will now see a couple of files in your new directory. The .gitignore file contains a list of files that you want git to ignore (i.e. not track changes). The Rproj file keeps track of this directory as a project in RStudio. You can double-click this file from your file browser to open up the project in RStudio. Git Project View Most importantly, you will notice a new “Git” tab in the upper right panel. This is where the magic happens. This tab is already listing two files in the workspace as different from the repository. Next we will learn how to commit these changed files to the repository. Committing changes to your local repository Any time you make a change to a file in your project directory, that change will be noted by git. However, the change will not be automatically added to your repository. Changes made to your files are changes in your workspace. To get those changes to show up in the repository, you have to follow a two-step process: add: This will add any changed files to the “staging area” but will not put them in the repository yet. commit: This will commit all changes from the files in the “staging area” to the repository. Typically, you will also include a text message here that gives some indication of what the changes do. Now, lets make some substantive changes to the project. I am going to do two things. I am going to add in the movies RData file and a script that does some basic analysis: ####################################################### # movie_analysis.R # Aaron Gullickson # Read in movie data and perform some basic analysis ####################################################### #read in movie data load(&quot;movies.RData&quot;) summary(movies) ggplot(movies, aes(x=runtime, y=BoxOffice))+ geom_point(alpha=0.2)+ labs(x=&quot;runtime in minutes&quot;, y=&quot;box office returns in millions of USD&quot;, title=&quot;Scatterplot of runtime by box office returns&quot;) Once I add these things to my directory, I will see that the git tab in RStudio is showing me changed files for both the movies.RData file and the movie_analysis.R script. I can click the “Staged” button to add these files to my staging area and then hit the “Commit” button. Changed files in Git The “Commit” button will bring up another window which will allow me to review changes and add a commit message. Review changes dialog Once I am satisfied with these changes and I have added a commit message, I can hit the “Commit” button to commit these changes to the repository. You can then close out of this dialog and you will see that there are no pending changes in the git repository. But wait! There is an error in my script. I forgot to capitalize the “Runtime” variable. If I make this correction to my script, I will see that movie_analysis.R is now showing up as modified. If you go to the review changes dialog, you will see that the single line that was changed is indicated in the diff view. Git diff view So the basic workflow here for a project is quite simple. At logical intervals, you should stage and commit changes that you have made to your repository. You will then have a complete log of all changes made to the project from its beginning. You can access this log with the little clock icon in the git tab in RStudio. Working with remotes Keeping track of your own changes in a repository is only half the fun. The real power of git is in the ability to share changes with other repositories. One of the key features of git is that it allows for peer-to-peer connections between repositories. Often, collaborators will set up one repository as the central repository (on GitHub, for example). Each collaborator can clone this repository to their own computer. After changes are committed to the local repository (the “master” repository), those changes can then be pushed back to the central repository (the “origin” repository). If I want to get changes that have been pushed by other collaborators, I can pull those changes from the central repository to my own local repository. As an example, I am going to clone one of my own public repositories and then show you how to make changes and push those changes back to the GitHub repository. Cloning the Repository The first step is to clone the GitHub repository. You will notice a “Clone or Download” button in the upper right of all GitHub repositories. If I click on that, it will give me a link to clone via https (I can also clone by ssh, but that is more complicated). GitHub clone button I copy that link and then go back to the “File &gt; New Project” option in RStudio. This time, I will select the “Version Control” option and then “Git” option. I will then paste my https link into the “Repository URL” line. This will give me an automatic project directory name. I just have to select where to put this directory and then “Create Project.” Create Project in RStudio from GitHub Git will clone the repository and RStudio will open up this directory as a project. Notice that it already wants to add a couple of files, because I did not have this repository set up as an RStudio project yet. Git project from GitHub I am going to make one simple change to the analysis.R file in this directory. I am going to add the following simple comment right below the file header: #This is an example comment for Sociology 512 I then commit those changes: Its important to understand clearly what I have and have not done at this point. I have made a change to my local workspace and committed that change to my local repository. But nobody else in the world knows about this change yet because I have not made any changes to the remote repository from GitHub. The Git tab in RStudio provides me an important reminder of this fact: Git reminder It is telling me that my repository is one commit ahead of the origin repository on GitHub. Let me pause for an important note on nomenclature. The master repository is always your own local repository. The origin repository is the place from which you cloned your local repository. You can change this name and you can link to multiple remote repositories but we won’t get that complex at the moment. What we want to do is push the changes in our repository to the remote repository on GitHub. We can do this as easy as falling off a log by clicking the big green button in the Git tab. The first time you do this, you will be asked for your GitHub password. I give the password, and git pushes my changes. If you now look at this file on the GitHub repository, you will see the new comment. What if I want to get any changes that might have been made by someone else to the GitHub repository? I can get those changes easily enough by pulling from the GitHub repository. RStudio makes this easy with the big blue button pointing down. Sometimes you may need to merge in changes from someone else when you push and pull. This can usually be handled gracefully, but occasionally you may run into conflicts if you both changed the same line of code. In those cases, git will leave both versions of the code in the document and you will have to make a decision about how to integrate them. If you are working on some big changes with collaborators then a better solution is to create a separate branch and then use the pull request system in GitHub to help manage conflicts, but that is beyond what we will learn for the basics. A basic git workflow I know that this may initially seem like a lot to juggle but the basic workflow of this process is really simple once you get used to it. Here is how you should think about integrating git into your workflow when you sit down to work on a project: run a pull command to catch any changes made by other collaborators since the last time you worked on the project. Do your work (writing code, downloading data, writing a paper, etc.) When you have reached a logical place to do so, commit all of the changes from your project to your local repository. You can do this multiple times in a work session if you are working on different parts of the project. Remember that its best to separate out commits by logically distinct changes rather than something like “A bunch of changes for the day.” When you are done for the day push all the changes back to the central repository. Now your colleagues will have access to your changes and you have a backup. Github Private Repositories Its important to keep in mind the distinction between git and GitHub. GitHub is just a service that provides free remote git repositories and a variety of additional bells and whistles like wiki’s and issue tracking (mostly associated with software development, but also useful for academics). There are other git repository service providers out there like GitLab and BitBucket. You can even run remote git repositories off of your own server, as I do. Free accounts on GitHub do not give you any access to private repositories, which means that any repository you push to GitHub is viewable by the world. However, as students, you can apply for the GitHub Student Developer Pack. This pack provides access to quite a list of software that you probably don’t need, but importantly it also gives you private repositories for free. If you plan to use git in your research workflow, its well worth it to apply. Note that the repositories you work with for our class will be private because they are associated with the GitHub Classroom that I have created. One important note about GitHub repositories: the maximum size for a single file is 100MB. This can create problems because git will allow you to commit a file larger than this size but you will not be able to push it up to GitHub, which means you will have to remove it from your commit history to push. If you are writing a script over 100MB, then you are doing it wrong. However, datasets can sometimes be larger than 100MB. R can usually read datasets that have been compressed (e.g. .zip, .gz, .bz2 extension) so keeping your datasets zipped is a good practice to avoid this problem. For really big datasets, you may need to either break up the dataset or host it somewhere else and read it remotely. Git from the command line We will use the git tab interface in RStudio for all of our gitting needs this term, but its useful to know the actual command line interface as well for potential problem shooting. For this section, I will use the same “test_project” I described above as an example. You should be able to access a command line interface with the “Terminal” tab in the main window of RStudio. This will probably show up by default in MacOSX. If not, you should be able to bring up a new one with Tools &gt; Terminal &gt; New Terminal. It may not show up in Windows, but you can also get a command line interface in Windows using the “git-bash” application that came with Git. The terminal will look very similar to the command line interface of R, but this command line is actually interfacing directly with the command line of your computer, not R. Like R, it expects the commands to be executed in the correct working directory. You can change working directories with the cd command (short for “change directory”). Once in the correct directory, you can run git commands. All git commands are two words with the first word being “git”. The first and most useful git command is git status. git status On branch master No commits yet Untracked files: (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) .gitignore test_project.Rproj nothing added to commit but untracked files present (use &quot;git add&quot; to track) This command tells me the current state of my repository. It shows here that I have two untracked files that have not been committed to the repository. Committing these files is a two-step process. First I need to use the git add command followed by the file names (and paths if in sub-directories) to the staging area: git add .gitignore test_project.Rproj git status On branch master No commits yet Changes to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage) new file: .gitignore new file: test_project.Rproj The status command now shows the files are staged and ready to be committed. I can now commit them with the git commit command. I will use the “-m” option to specify a log message. git commit -m &quot;initial commit of project files&quot; git status On branch master nothing to commit, working tree clean Git is now telling me that everything looks good. I have no other uncommitted changes in my repository. If I make new files or edit existing files, I can then repeat this two step process of adding and committing files. I can also use git log to show the history of all my commits: git log commit 35a0940b190dab64a08a5cad5025eb045ac7d353 (HEAD -&gt; master) Author: Aaron Gullickson &lt;aarong@uoregon.edu&gt; Date: Mon Jan 27 10:17:40 2020 -0800 initial commit of project files In this case its showing me the only commit that I made, when I made it and the commit message that I included. It also gives a unique ID for the commit. I can also push and pull my changes but first I need a remote repository. You can set up a new empty repository on GitHub through the GitHub web interface. GitHub will then nicely give you information on how to get your local repository up the remote one: Creating project from GitHub The first thing we have to do is create a new “origin” remote repository. We can do this with the git remote command: git remote add origin https://github.com/AaronGullickson/test_project.git Note that I could have called this anything I wanted, but “origin” is the general practice. Now that I have the remote repository’s address set up, I can use the git push command to upload the contents of my local repository to GitHub: git push -u origin master Enumerating objects: 4, done. Counting objects: 100% (4/4), done. Delta compression using up to 12 threads Compressing objects: 100% (3/3), done. Writing objects: 100% (4/4), 457 bytes | 457.00 KiB/s, done. Total 4 (delta 0), reused 0 (delta 0) To https://github.com/AaronGullickson/test_project.git * [new branch] master -&gt; master Branch &#39;master&#39; set up to track remote branch &#39;master&#39; from &#39;origin&#39;. It looks like everything worked. The first time I do this I need to specify the names of the repositories involved (“origin master”), but I also include the “-u” command which will tell git to treat the origin remote repository as my default “upstream” repository in the future. In the future, I will then just need to type “git push” to do the same operation. If someone else were to make changes to the remote repository, I could then use the git pull command to integrate them into my local repository. "],
["reading-and-writing-data.html", "Reading and Writing Data", " Reading and Writing Data Students often get hung up at the start of a quantitative research project with the simple task of getting their data loaded into the statistical software package they are using. This is frequently a problem because data are distributed in inconsistent and often confusing ways by the agencies that release them. Knowing how to work with raw data in multiple formats is an important skill in being able to quickly get up and running with the more important parts of your analysis. Data Formats Data typically come to us in one of two general formats: (1) plain text, or (2) binary. R also has tools for accessing data that is loaded into some kind of database format (e.g. SQL, MS Access) and R also has tools for “scraping” your own data from online sources. However, the vast majority of data comes in either text or binary format and so that is what we will focus on in this class. Base R comes with several helpful methods for reading in plain text data such as read.csv and read.fwf. For a long time, binary data from a variety of sources could be read in using the foreign library. However, some more recent packages that are part of the tidyverse have been developed that provide much more easy and efficient ways to read in a variety of plain text and binary formats. I will use those libraries for all of the examples here. These libraries are: readr for reading in plain-text rectangular data. haven for reading in binary data files from SAS, Stata, and SPSS. readxl for reading in Excel files. All three of these packages read data into an object called a tibble. Tibbles are the tidyverse upgrade to the venerable data.frame. In general, they operate just like data.frames with some nice additional features, so you shouldn’t really need to worry about it. In a pinch, a tibble can be recast as a data.frame with the as.data.frame command: df &lt;- as.data.frame(df) Plain text files Plain text files (also known as ASCII files), in contrast to binary files, are easily readable across any platform without specialized (and often proprietary) software. When you write a document in WordPad in Windows or TextEdit in OSX, you are writing a plain text file. When you write a document in Microsoft Word, you are writing a binary file. Plain text files are easily transportable across a variety of different program formats, usually take up less memory, and are better for tracking changes in version control systems. If you want to share data with others, plain text is the best format to use because it is accessible regardless of statistical software or computer platform. While plain text has the advantage of accessibility and portability, it has the disadvantage of lacking any ability to add meta-characteristics to your data. Lets say that you coded an ordinal variable for highest degree earned in your data using a factor in R. When you output this data as plain-text, the variable names will simply show up as character strings with no information about proper ordering. When re-loaded into R or another statistical software package, the ordering of the variables that you so carefully specified will not be respected and they will simply be ordered in some default manner (e.g. alphabetically in R). When organizations release their data as plain text, they typically code all variables as numeric and provide a codebook that can be used to convert things like categorical variables and missing value codes. This is a good practice for data distribution, but not terribly helpful when we want to save the analytical data that we worked so hard to organize for our own project. Data in plain text files usually comes in one of two formats: comma-separated values (CSV) files or fixed-width files. In both cases, one line of text corresponds to an observation, or a row of data. The difference between the two formats is how to distinguish the values for different variables within a line of text (i.e. the columns). Working with CSV files In a comma-separated values format, the columns in the data are separated by commas. Here is an example with a very small CSV data file named “data.csv”. name,location,race,gender,yrsed &quot;Bobbi&quot;,&quot;Eugene,OR&quot;,&quot;Black&quot;,&quot;Female&quot;,12 &quot;Jim&quot;,&quot;Seattle,WA&quot;,&quot;White&quot;,&quot;Male&quot;,11 &quot;Mary&quot;,&quot;Oakland,CA&quot;,&quot;White&quot;,&quot;Female&quot;,16 There are a couple of things to note here about the format. First, the top line is the “header” line that gives the names of the variables. Its important for your statistical program to know whether the data file has a header line or not in order to properly process the file. Second, notice that character strings representing category labels are surrounded by quotes. This is good practice because character strings may sometimes include commas within them, as the location variable does in this case, and the program will treat that as a delimiter if not surrounded by quotation marks. We can read this data into R with the read_csv command in the readr library: library(readr) mydata &lt;- read_csv(&quot;resources/data.csv&quot;) mydata ## # A tibble: 3 x 5 ## name location race gender yrsed ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bobbi Eugene,OR Black Female 12 ## 2 Jim Seattle,WA White Male 11 ## 3 Mary Oakland,CA White Female 16 Notice that The read_csv command tried to guess what type of variable each column should be. It correctly guessed that yrsed was a quantitative variable. You can also give more explicity information about column types with ther col_types command. The read_csv command also assumed that the first line was a header row. If that were not true, I could change that with the col_names=FALSE argument. The use of the comma as the delimiter between columns is pretty standard today, but you will occasionally find other delimiters used. The next most common delimiter is the tab (“ in R speak). The readr package provides a handy function called read_tsv but we can also use the read_delim function to do the same thing and this will also show how easy it is to”roll your own\" read function. Here is the same data as above, but this time separated by tabs and named “data_tab.txt”: name location race gender yrsed &quot;Bobbi&quot; &quot;Eugene,OR&quot; &quot;Black&quot; &quot;Female&quot; 12 &quot;Jim&quot; &quot;Seattle,WA&quot; &quot;White&quot; &quot;Male&quot; 11 &quot;Mary&quot; &quot;Oakland,CA&quot; &quot;White&quot; &quot;Female&quot; 16 I can read it in using the with the read_delim function: mydata &lt;- read_delim(&quot;resources/data_tab.txt&quot;, delim=&quot;\\t&quot;) mydata ## # A tibble: 3 x 5 ## name location race gender yrsed ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bobbi Eugene,OR Black Female 12 ## 2 Jim Seattle,WA White Male 11 ## 3 Mary Oakland,CA White Female 16 All I had to do was specify what character served as the delimiter. In actuality, read_csv and read_tsv are just convenienct functions taht both call read_delim. If you access the help file for read_delim, you will see that there are many different arguments for dealing with specific problems that might arise in your dataset. Lets say for example that I had a data file saved as “data_messy.csv” that looked like this: *Some data that I collected *I cant remember when name of person,location of person,racial category,gender of person,years of education name,location,race,gender,yrsed &quot;Bobbi&quot;,&quot;Eugene,OR&quot;,&quot;Black&quot;,&quot;Female&quot;,12 &quot;Jim&quot;,&quot;Seattle,WA&quot;,&quot;na&quot;,&quot;Male&quot;,11 &quot;Mary&quot;,&quot;Oakland,CA&quot;,&quot;White&quot;,&quot;Female&quot;,16 There are several complications here. First, there are a couple of comment lines at the top where the comment symbol is \"*\" that I don’t want to get processed. I could handle this with either the skip argument to skip a certain number of rows before reading the data or the comment argument to define what lines to skip by what character they start with. Second, there is a line above the proper headers with a description of each variable. I could use the skip option here again to skip this line. Finally, the lower-case “na” won’t be recognized by default as a missing value by R, but the option na will allow me to specify additional character strings like “na” that should be interpreted as missing values. All together, I use the command: mydata &lt;- read_delim(&quot;resources/data_messy.csv&quot;, delim=&quot;,&quot;, comment=&quot;*&quot;, skip=1, na=&quot;na&quot;) mydata ## # A tibble: 4 x 5 ## `name of person` `location of pe… `racial categor… `gender of pers… ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 name location race gender ## 2 Bobbi Eugene,OR Black Female ## 3 Jim Seattle,WA &lt;NA&gt; Male ## 4 Mary Oakland,CA White Female ## # … with 1 more variable: `years of education` &lt;chr&gt; Note that because I used the comment argument I only specify skip=1 . Working with fixed-width text files The second form that data in text format can take is “fixed-width” format where the specific length of each variable in terms of the number of characters is specified. For example, here is the same dataset in fixed-width format, saved as “data_fw.txt”: BobbiEugene,OR BlackFemale12 Jim Seattle,WAWhiteMale 11 Mary Oakland,CAWhiteFemale16 Notice that the actual starting location of each variable is the same within each row. If you count the characters up, you will see that the first variable has a width of 5 characters, the second variable has a width of 10 characters, and so on. Note also that this file does not contain headers which is pretty typical of fixed width files. We can use the read_fwf command by feeding in this data, by explicitly specifying the starting and endind positions of each variable: mydata &lt;- read_fwf(&quot;resources/data_fw.txt&quot;, col_positions = fwf_positions(start=c(1, 6,16,21,27), end =c(5,15,20,26,28), col_names=c(&quot;name&quot;,&quot;location&quot;,&quot;race&quot;, &quot;gender&quot;,&quot;yrsed&quot;))) mydata ## # A tibble: 3 x 5 ## name location race gender yrsed ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bobbi Eugene,OR Black Female 12 ## 2 Jim Seattle,WA White Male 11 ## 3 Mary Oakland,CA White Female 16 I have to do a little more work than I did with CSV files. First I need to define a col_positions argument and feed in the results from the function fwf_positions. This fwf_positions function needs to have a vector of starting and ending positions for each variable. I also use the col_names argument in this function to assign names to each of my variables. Now I want to show you an example of fixed width data that is more realistic. The data I will use here were created from an extract of the 2018 American Community Survey data downloaded from IPUMS. The data come zipped up as a “G-zip” file with suffix “gz” which is a standard method of compressing data files. You can download the file here. You can also view the codebook that IPUMS provides which shows where the starting and ending positions are for each variable. Here is the relevant part of that codebook: File Type: rectangular Case Selection: No Variable Columns Len 2018 YEAR H 1-4 4 X SAMPLE H 5-10 6 X SERIAL H 11-18 8 X CBSERIAL H 19-31 13 X HHWT H 32-41 10 X CLUSTER H 42-54 13 X STATEFIP H 55-56 2 X STRATA H 57-68 12 X GQ H 69 1 X PERNUM P 70-73 4 X PERWT P 74-83 10 X SEX P 84 1 X AGE P 85-87 3 X YRMARR P 88-91 4 X EDUC P 92-93 2 X EDUCD P 94-96 3 X I don’t need all of these variables. Some of them are just defaults that come with every IPUMS extract like HHWT, CLUSTER, and STRATA. For my purposes, lets say I only want the variables of STATEFIP, YEAR, SEX, AGE, YRMARR, and EDUCD. Using read_fwf, I can just skip the indices of variables I don’t need. Another nice feature of the readr package is that all of the functions can read directly directly from zipped data, which saves me room on my computer and my git repository (GitHub has a limit of 100MB for files). Here is my command to read in the data: acs &lt;- read_fwf(&quot;resources/usa_00084.dat.gz&quot;, col_positions = fwf_positions(start = c(1,55,84,85,88,94), end = c(4,56,84,87,91,96), col_names = c(&quot;year&quot;,&quot;statefip&quot;,&quot;sex&quot;, &quot;age&quot;,&quot;yrmarr&quot;,&quot;educd&quot;)), col_types = cols(.default = &quot;i&quot;), progress = FALSE) acs ## # A tibble: 3,214,539 x 6 ## year statefip sex age yrmarr educd ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2018 1 2 19 0 65 ## 2 2018 1 2 18 0 65 ## 3 2018 1 1 53 0 64 ## 4 2018 1 1 28 0 71 ## 5 2018 1 2 25 0 30 ## 6 2018 1 2 30 0 63 ## 7 2018 1 2 66 1992 71 ## 8 2018 1 1 38 0 30 ## 9 2018 1 1 41 0 63 ## 10 2018 1 2 18 0 65 ## # … with 3,214,529 more rows I am also using the col_types argument to force read_fwf to read in all variables as numeric integers because this is how all the data come from IPUMS. I also set progress=FALSE for the output here, but I can ghange that to the progress=TRUE option to get a progresss bar as my data is read in that lets me know how long I can expect to wait. Data in binary format As a result of initiatives to make science more open, data is increasingly becoming available in simple text format, which improves its portability and accessibility. However, there are still many cases where data is available in a binary format that is readable only by a specific statistical software program. For example, the quick download page for the General Social Survey provides the comprehensive GSS data files but only in Stata and SPSS formats. Thats fine if you have purchased that software, but you are out of luck if you have not. This approach is Bad For Science, but you will still run into it quite a bit. Luckily, R can usually still read those files. For aeons upon aeons, the preferred packages for this was foreign, but the tidyverse has changed the game. The haven package can easily read in data from Stata, SAS, and SPSS. The readxl package can easily read in data from an Excel file. To test these libraries out, I have loaded the data exampleI have been using into Stata and saved it as a binary stata dataset (*.dta). I can use the read_dta function in haven to read in this data. library(haven) mydata &lt;- read_dta(&quot;resources/data.dta&quot;) mydata ## # A tibble: 3 x 5 ## name location gender yrsed race ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl+lbl&gt; ## 1 Bobbi Eugene,OR Female 12 1 [Black] ## 2 Jim Seattle,WA Male 11 2 [White] ## 3 Mary Oakland,CA Female 16 2 [White] Everything looks good except that because I didn’t encode location and gender in Stata as categorical variables, those variables show up in R as character strings rather than factor variables. If I wanted to turn my either variable into a proper factor (categorical) variable, I can use the factor command: mydata$gender &lt;- factor(mydata$gender) summary(mydata$gender) ## Female Male ## 2 1 Saving data R has its own binary format for keeping track of data. You can save any object or set of objects to your filesystem with the save command. This will save a file in a binary *.RData format. These objects can then be loaded back into R with the load command: save(mydata, file=&quot;resources/data.RData&quot;) load(&quot;resources/data.RData&quot;) I recommend saving your own analytical data as RData files because these files are light-weight and contain all of the added meta-information that you have created for categorical variables and such. However, for sharing data more widely I recommend that you provide data in plain text format. The readr has a variety of write_* functions including write_csv, write_delim, and write_tsv for outputing data as delimited plain text. For example, I could write my data to a csv with: write_csv(mydata, path=&quot;resources/mydata_R.csv&quot;) name,location,gender,yrsed,race Bobbi,&quot;Eugene,OR&quot;,Female,12,1 Jim,&quot;Seattle,WA&quot;,Male,11,2 Mary,&quot;Oakland,CA&quot;,Female,16,2 The haven package also has several write_* functions for outputing data in other binary formats. Here are the most common: write_dta for a Stata data file write_sav for a SPSS data file write_xpt for a SAS transport file and write_sas for a SAS data file "],
["cleaning-data.html", "Cleaning Data", " Cleaning Data Cleaning data is a major component of any quantitative project. Rarely will data come to you in exactly the form that you want for your research question. In this section, we will focus on three of the most important components of data cleaning: (1) assigning and handling missing values, (2) recoding variables, and (3) aggregating data. For this analysis, we will use the same data extract from the American Community Survey that we used for the reading and writing data lab. I downloaded this extract from IPUMS. You can download the file here. You can also view the codebook that IPUMS provides The data is a sample of individuals in the US. In the read_fwf command below, I am only going to read in a few variables that we will use for our data cleaning examples. acs &lt;- read_fwf(&quot;resources/usa_00084.dat.gz&quot;, col_positions = fwf_positions(start = c(1,55,84,85,88,94), end = c(4,56,84,87,91,96), col_names = c(&quot;year&quot;,&quot;statefip&quot;,&quot;sex&quot;, &quot;age&quot;,&quot;yrmarr&quot;,&quot;educd&quot;)), col_types = cols(.default = &quot;i&quot;), progress = FALSE) acs ## # A tibble: 3,214,539 x 6 ## year statefip sex age yrmarr educd ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2018 1 2 19 0 65 ## 2 2018 1 2 18 0 65 ## 3 2018 1 1 53 0 64 ## 4 2018 1 1 28 0 71 ## 5 2018 1 2 25 0 30 ## 6 2018 1 2 30 0 63 ## 7 2018 1 2 66 1992 71 ## 8 2018 1 1 38 0 30 ## 9 2018 1 1 41 0 63 ## 10 2018 1 2 18 0 65 ## # … with 3,214,529 more rows All of the variables shown here use integer values to record data, even in cases where the variable is categorical in nature. This is a common approach with many datasets, because it makes datasets smaller and more portable. Part of the cleaning process will be to use information in the codebook to convert from integer values to proper categorical variables. The Most Important Rule: Check yourself before you wreck yourself The number one rule of cleaning and processing data is to always check your code to make sure it is producing what you think it should be producing. In the examples below, I run several diagnostic checks to ensure that what I am producing looks correct. There are a variety of ways you can check your code, but its important to take your time here and be extra scrupulous. If you don’t properly clean your data, then everything you produce in the later analysis will be questionable at best. You don’t want this to happen to you. Assigning missing values One of the first things you should check in your data is how missing values are coded. In many cases, missing values will be coded with a numeric value rather than a proper missing value. If you don’t correct this, then all of your statistics will be incorrect. Codebooks that are provided with the data will normally identify the codes used for missing values. Usually, missing values are coded with numeric values that are impossible or well outside the range of the data, so that they can easily be identified. For example, the missing value for the year of marriage variable yrmarr in our dataset is zero. Since yrmarr is the calendar year of marriage, zero is a non-sensical value. However, 1,332,999 individuals have a value of zero. These are all individuals who should be NA on yrmarr because they have never been married. If you leave the zeros in, then you will drastically change any calculation you make with this variable. Bracket and Boolean Approach In R, we can easily code in missing values by identifying a subset of the variable with a boolean statement and then assigning the special NA value to that subset. I refer to this approach to replacing values in a variabl as the bracket-and-boolean approach. summary(acs$yrmarr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0 0 1971 1166 1997 2018 acs$yrmarr[acs$yrmarr==0] &lt;- NA summary(acs$yrmarr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1937 1979 1993 1992 2006 2018 1332999 The mean and median change dramatically once I code in all of the missing values because I had a lot of never-married individuals without a proper value for yrmarr who were being reported as zero. Ifelse approach The bracket-and-boolean approach is one of the standard ways to re-code certain values for a variable in R, but another approach that I prefer for its greater flexibility is the ifelse command. An ifelse command takes three arguments. The first argument is a boolean statement. The second argument is what value should be returned if the first argument evaluates to TRUE and the third argument is what value should be returned if the first argument evaluates to FALSE. Here is how I can use the ifelse command in this example: acs$yrmarr &lt;- ifelse(acs$yrmarr==0, NA, acs$yrmarr) When acs$yrmarr==0 is TRUE, the ifelse command returns NA and when acs$yrmarr==0 is false, the ifelse command returns the original value of yrmarr. This command accomplishes the same thing as the bracket-and-boolean approach above. Where the ifelse command really shines is when you can string together multiple “cascading” ifelse commands as I will demonstrate below. Recoding Recoding variables is an integral part of preparing your data for analysis. Rarely will all the variables come in exactly the form that you want for the reserch question at hand. In many cases, you will want to collapse and/or combine categorical variables and transform quantitative variables. It will also frequently be useful to use sensitivity analysis to try out muliple approaches to coding your variables to see how sensitive your results are to coding decisions. When you recode variables, it is usually a good idea to generate a new version of the variable with a different name. This allows you to check the new variable against the old variable to ensure that your code is working correctly. Coding Values for Categorical Variables As I noted above, all of the data from IPUMS come to us in numeric form. For example, lets run a table command on the sex variable. table(acs$sex) ## ## 1 2 ## 1574618 1639921 I have two unique values here: 1 and 2. How do these values compare to the categories that I expected? We can look at the codebook to see how the numeric values map onto the categories we want. Here is the relevant excerpt from the codebook: SEX Sex 1 Male 2 Female I can accomplish this re-code using either the bracket-and-boolean approach or the ifelse approach. I am going to show the ifelse approach here first, because I can demonstrate the use of cascading ifelse statements. In both cases, I will assign my results to a new variable called gender. acs$gender &lt;- ifelse(acs$sex==1, &quot;Male&quot;, ifelse(acs$sex==2, &quot;Female&quot;, NA)) What exactly is going on here? I have a second ifelse statement embedded inside the first statement for the final argument of that statement. What this approach does is create a cascade of ifelse statements that should pick up all of the possible cases. Anytime the boolean evaluates to TRUE, then I will assign the given value and pop out. Otherwise, I will continue on to the next statement. In simple terms, here is what the process looks like: Is the value a 1? If yes, then assign a value of “Male” to the gender variable. If no, then continue to (2). Is the value a 2? If so then assign a value of “Female” to the gender variable. If no, then move to (3). We have exhausted all possible logical cases. If you are still here, then assign a missing value. Note that since I don’t actually have any cases other than a 1 or a 2, I could have done this with a single ifelse statement. However, by always making the final category an NA I ensure that pick up any missing values for cases that I don’t explicitly assign. This is good practice and helps us to avoid mistakes. Since gender is a categorical variable, I really should make it a factor variable, but the command above did not turn it into a factor variable, but rather a character string variable. To “factor it up” I need to run the factor command and specify my categories: acs$gender &lt;- factor(acs$gender, levels=c(&quot;Male&quot;,&quot;Female&quot;)) If I really want to be efficient, I could have wrapped my cascading ifelse statements in a factor command and done it all at once. acs$gender &lt;- factor(ifelse(acs$sex==1, &quot;Male&quot;, ifelse(acs$sex==2, &quot;Female&quot;, NA)), levels=c(&quot;Male&quot;,&quot;Female&quot;)) Now we need to check ourselves before we wreck ourselves. The most straightforward way to do this is to run a table command on the original and new variable to make sure all of the observations are being coded correctly. When I run this table command, I want to include the argument exclude=NULL so that missing value codes show up in the table. table(acs$sex, acs$gender, exclude=NULL) ## ## Male Female ## 1 1574618 0 ## 2 0 1639921 Everything looks good. All the 1s are “Male” and all the 2s are “Female.” What if I wanted to do this the bracket-and-boolean way? Well, typically the first step is to set up a new variable that is all missing values and then plug in the values one by one. Here is what it would look like: acs$gender &lt;- NA acs$gender[acs$sex==1] &lt;- &quot;Male&quot; acs$gender[acs$sex==2] &lt;- &quot;Female&quot; acs$gender &lt;- factor(acs$gender, levels=c(&quot;Male&quot;,&quot;Female&quot;)) table(acs$sex, acs$gender, exclude=NULL) ## ## Male Female ## 1 1574618 0 ## 2 0 1639921 Collapsing Categorical Variables Collapsing categorical variables means reducing the number of categories to a smaller set of categories, by lumping some categories together. As an example, take a look at the full set of categories for the detailed educational attainment category educd: 000 N/A or no schooling 001 N/A 002 No schooling completed 010 Nursery school to grade 4 011 Nursery school, preschool 012 Kindergarten 013 Grade 1, 2, 3, or 4 014 Grade 1 015 Grade 2 016 Grade 3 017 Grade 4 020 Grade 5, 6, 7, or 8 021 Grade 5 or 6 022 Grade 5 023 Grade 6 024 Grade 7 or 8 025 Grade 7 026 Grade 8 030 Grade 9 040 Grade 10 050 Grade 11 060 Grade 12 061 12th grade, no diploma 062 High school graduate or GED 063 Regular high school diploma 064 GED or alternative credential 065 Some college, but less than 1 year 070 1 year of college 071 1 or more years of college credit, no degree 080 2 years of college 081 Associate&#39;s degree, type not specified 082 Associate&#39;s degree, occupational program 083 Associate&#39;s degree, academic program 090 3 years of college 100 4 years of college 101 Bachelor&#39;s degree 110 5+ years of college 111 6 years of college (6+ in 1960-1970) 112 7 years of college 113 8+ years of college 114 Master&#39;s degree 115 Professional degree beyond a bachelor&#39;s degree 116 Doctoral degree 999 Missing There are a a lot of categories here. Its unlikely that this fine level of detail will be helpful in any analyses that we will perform. Lets also take a look at the case counts for these categories: table(acs$educd) ## ## 1 2 11 12 14 15 16 17 22 23 25 ## 95147 97571 45463 37691 35302 38104 41690 41318 44435 56815 45653 ## 26 30 40 50 61 63 64 65 71 81 101 ## 66566 69213 76684 85612 52431 602113 103161 190344 381192 213684 486531 ## 114 115 116 ## 217891 53833 36095 A lot of categories are not even showing up in our data. The reason for these zero categories is that the set of categories was created to capture all of the ways that educational attainment was recorded across censuses since 1850, but many of the codes are not used in the particular 2018 data that we have. Many scholars use a parsimonious four-category coding of educational attainment: less than high school diploma, high school diploma, some college including associate’s degree, four-year college degree or more. We can collapse our full set of categories into this four-category system fairly easily using cascading ifelse statements: acs$edattain &lt;- factor(ifelse(acs$educd&gt;=2 &amp; acs$educd&lt;=61, &quot;Less than HS&quot;, ifelse(acs$educd&gt;=62 &amp; acs$educd&lt;=64, &quot;HS Diploma&quot;, ifelse(acs$educd&gt;=65 &amp; acs$educd&lt;=100, &quot;Some College&quot;, ifelse(acs$educd&gt;=101 &amp; acs$educd&lt;=116, &quot;College&quot;, NA)))), levels=c(&quot;Less than HS&quot;,&quot;HS Diploma&quot;,&quot;Some College&quot;,&quot;College&quot;)) table(acs$educd, acs$edattain, exclude=NULL) ## ## Less than HS HS Diploma Some College College &lt;NA&gt; ## 1 0 0 0 0 95147 ## 2 97571 0 0 0 0 ## 11 45463 0 0 0 0 ## 12 37691 0 0 0 0 ## 14 35302 0 0 0 0 ## 15 38104 0 0 0 0 ## 16 41690 0 0 0 0 ## 17 41318 0 0 0 0 ## 22 44435 0 0 0 0 ## 23 56815 0 0 0 0 ## 25 45653 0 0 0 0 ## 26 66566 0 0 0 0 ## 30 69213 0 0 0 0 ## 40 76684 0 0 0 0 ## 50 85612 0 0 0 0 ## 61 52431 0 0 0 0 ## 63 0 602113 0 0 0 ## 64 0 103161 0 0 0 ## 65 0 0 190344 0 0 ## 71 0 0 381192 0 0 ## 81 0 0 213684 0 0 ## 101 0 0 0 486531 0 ## 114 0 0 0 217891 0 ## 115 0 0 0 53833 0 ## 116 0 0 0 36095 0 Notice how I am pairing a &gt;= with a &lt;= in an AND boolean statement in order to define a range of values. The statement x&gt;=A &amp; x&lt;=B will return true for all values of x between A and B. Looking at the table, its clear that all of the numeric values are showing up in the right category and all the missing values coded at 1 (these are primarily youths) are coded as missing values correctly. In some cases, recoding may involve incorporating information on multiple variables in the construction of one new variable. The process is no different from the cases discussed above except for requiring possibly more complex boolean statements. Transforming Quantitative Variables Quantitative variables can also be recoded in R and this is typically much easier as you just need to apply the right mathematical equation to it. Lets say for example that I want to create a new variable from yrmarr that is the age of the respondent for the current marriage for all currently married respondents. I can get this information simply by subtracting the yrmarr variable from the year variable to get the duration of marriage and then subtracting this value from the respondent’s dage.. acs$age_marr &lt;- acs$age - (acs$year-acs$yrmarr) summary(acs$age_marr) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 14.0 22.0 26.0 29.5 34.0 95.0 1332999 Notice that this coding only works correctly because I already assigned missing values to the yrmarr variable above. Creating Interval Categories From a Quantitative Variable In some cases, I may want to turn a quantitative variable into a categorical variable that measures intervals of data. For example, I might want to turn age into ten-year age groups (e.g. 20-29, 30-39). I can do this sort of coding using the same bracket-and-boolean or cascading ifelse appoaches outlined above. However, R also provides a built-in function called cut that makes this pretty easy to do: acs$age_group &lt;- cut(acs$age, breaks=seq(from=0, to=100, by=10), right=FALSE) summary(acs$age_group) ## [0,10) [10,20) [20,30) [30,40) [40,50) [50,60) [60,70) [70,80) ## 338958 403401 378691 388612 380621 450316 443250 279682 ## [80,90) [90,100) ## 120200 30808 The most important argument to the cut function is the breakpoints for each interval. Here I have used the seq function to define a sequence of numbers from 0 to 100 at intervals of 10. I also specify right=FALSE so that the intervals are inclusive of the lower value and exclusive of the upper value. The age_group variable is now a proper factor variable and can be used like any other factor variable. After Cleaning You Still Need to Tidy Because you are adding new variables as you recode, the size of your dataset will often grow as you clean, organize, and re-code. This can quickly lead to a lot of clutter and this clutter can have consequences. When you have multiple variables with similar names, it is easy to mistakenly use the wrong one. To avoid confusion, it is always best to limit your final analytical dataset to just the variables that you want. The easiest way to do this in R is to put a subset command at the bottom of your organizational script and use the select argument to add variables you want to keep to it as you clean them. So, for example my command here might look like: acs &lt;- subset(acs, select=c(&quot;statefip&quot;,&quot;gender&quot;,&quot;edattain&quot;,&quot;age_marr&quot;,&quot;age&quot;)) acs ## # A tibble: 3,214,539 x 5 ## statefip gender edattain age_marr age ## &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;int&gt; ## 1 1 Female Some College NA 19 ## 2 1 Female Some College NA 18 ## 3 1 Male HS Diploma NA 53 ## 4 1 Male Some College NA 28 ## 5 1 Female Less than HS NA 25 ## 6 1 Female HS Diploma NA 30 ## 7 1 Female Some College 40 66 ## 8 1 Male Less than HS NA 38 ## 9 1 Male HS Diploma NA 41 ## 10 1 Female Some College NA 18 ## # … with 3,214,529 more rows People often keep a bunch of cluttered variables in their data frame out of a misguided fear that they will lose them if they are dropped from the current dataset. But that is the wrong way to think about your data. If you discover that you actually want a variable you dropped, just add it to the subset command and re-source your cleaning data script. In most cases this will take minutes to process at most. Thats a reasonable price to pay for the clarity of a small analytical dataset. Aggregating Data It is not uncommon to want to aggregate data at one unit of analysis up to a higher unit of analysis. For example, the individual-level ACS data that we have contains a variable that identifies the state of residence for each respondent (statefip). What if I wanted to aggregate my individual level data up to the state-level? As a simple example, lets say that I wanted to calculate the mean age of marriage of married individuals for each state. The aggregate command is the easiest way to perform such tasks and it has the flexibility to do aggregations on multiple variables or multiple indices at the same time. There are two different syntax forms for aggregate but we will focus on the formula syntax. This syntax works like agg_var~by_var1+by_var2 where the agg_var is the variable that you want to aggregate on and by_var1 and by_var2 are the variables that you want to aggregate by. Here is a simple example of getting the mean for marriage duration by state. temp &lt;- aggregate(age_marr~statefip, data=acs, mean, na.rm=TRUE) head(temp) ## statefip age_marr ## 1 1 29.27548 ## 2 2 29.92585 ## 3 4 30.36038 ## 4 5 29.39313 ## 5 6 30.02206 ## 6 8 29.72513 The results here are output as a data.frame aggregated to the level of states. The dataset contains an id for the state and the mean of age_marr for that state. The example above only aggregated one variable, but we can also get the mean of multiple variables at once by using the cbind command within the formula: temp &lt;- aggregate(cbind(age_marr,age)~statefip, data=acs, mean, na.rm=TRUE) head(temp) ## statefip age_marr age ## 1 1 29.27548 56.28057 ## 2 2 29.92585 53.06636 ## 3 4 30.36038 56.75826 ## 4 5 29.39313 55.65785 ## 5 6 30.02206 55.22376 ## 6 8 29.72513 54.17558 Now I get two variables for each state, mean age of marriage and mean age. We can also aggregate across multiple dimensions at the same time. Lets get mean age of marriage again, but this time by state and educational category. temp &lt;- aggregate(age_marr~edattain+statefip, data=acs, mean, na.rm=TRUE) head(temp) ## edattain statefip age_marr ## 1 Less than HS 1 29.63777 ## 2 HS Diploma 1 29.20512 ## 3 Some College 1 29.28409 ## 4 College 1 29.18011 ## 5 Less than HS 2 28.22311 ## 6 HS Diploma 2 28.90491 Now I get four means for each state, one for each educational attainment category. Its worth thinking a little about the structure of the output here. This data structure is a little different than what we are used to. I don’t have a single line for each observation (state) giving me the age of marriage as four separate variables. Instead I have four separate lines for each state. This data format is called the long format and we are not used to working with it. We will learn more in our next lab about how we can manipulate this format to get what we want. What if we want to aggregate the proportion of respondents in each category of a categorical variable by state? The syntax here is a little messier, but we can do this by creating booleans for each category and then calculating means: temp &lt;- aggregate(cbind(prop_lhs=edattain==&quot;Less than HS&quot;, prop_hs_grad=edattain==&quot;HS Diploma&quot;, prop_scollege=edattain==&quot;Some College&quot;, prop_college_grad=edattain==&quot;College&quot;)~statefip, data=acs, mean, na.rm=TRUE) head(temp) ## statefip prop_lhs prop_hs_grad prop_scollege prop_college_grad ## 1 1 0.2784240 0.2580242 0.2653052 0.1982465 ## 2 2 0.2907537 0.2587413 0.2665113 0.1839938 ## 3 4 0.2812910 0.2038612 0.2814826 0.2333653 ## 4 5 0.2969432 0.2821164 0.2503300 0.1706103 ## 5 6 0.2980686 0.1712014 0.2549658 0.2757642 ## 6 8 0.2389480 0.1763708 0.2509729 0.3337083 Its also important to note that I have been calculating the mean in all of these examples, but I could also easily substitute a different function. For example, I could use sum on the above codes to get counts of individuals by education: temp &lt;- aggregate(cbind(n_lhs=edattain==&quot;Less than HS&quot;, n_hs_grad=edattain==&quot;HS Diploma&quot;, n_scollege=edattain==&quot;Some College&quot;, n_college_grad=edattain==&quot;College&quot;)~statefip, data=acs, sum, na.rm=TRUE) head(temp) ## statefip n_lhs n_hs_grad n_scollege n_college_grad ## 1 1 12925 11978 12316 9203 ## 2 2 1871 1665 1715 1184 ## 3 4 19087 13833 19100 15835 ## 4 5 8772 8334 7395 5040 ## 5 6 109649 62979 93793 101444 ## 6 8 12956 9563 13608 18094 "],
["reshaping-and-merging-data.html", "Reshaping and Merging Data", " Reshaping and Merging Data In this lab, I want to cover two important components of data wrangling, reshaping data from one format to one another and merging two diffent datasets together. As an example for this section, we are going to use some data from The World Bank Development Indicators and the International Labour Organization. Both of these sites have nice designs that allow users to filter an extract of their choosing by country, year, and variable type. They also allow download of the data in a variety of formats including CSV. The labor data provides information on unemployment rates by age and sex and I have limited the years to 2014 or later. I then pulled an extract of the world bank data from 2014 on all countries for the variables of GDP per capita, life expectancy at birth, and population size. You can download the World Bank extract here and the ILO data here.2 Ultimately, I want to create a final dataset which combined these sources. The final dataset should have an observation for each country with the variables of GDP per capita, life expectancy at birth, population size, and unemployment rates. In order to get there though, I am going to have to deal with some data issues. Reshaping All of the data that we have looked at so far have been in what is called a wide format. Each row is an observation is a column. This is usually the easiest way for us to work with data, but sometimes data can come in a long format instead. In the long format, each row gives the value for a certain variable for a certain observation. The observation and variable are identified by id columns and the values themselves are a third column. To give a simple example, here is a very small dataset in wide form: name age height Bob 25 71 Susie 37 65 Linda 28 68 Here is that same dataset in long form: name variable value Bob age 25 Bob height 71 Susie age 37 Susie height 65 Linda age 28 Linda height 68 Notice that in the long format each person shows up once for each variable from the wide dataset. In this case, the “name” variable is my id variable that identified unique observations. Its possible to have more than one id variable in the long format. For example, I might also have a year variable which identifies an observation-year case. Lets take a look at the world bank data that I have and you will see another example of the long format. library(readr) worldbank &lt;- read_csv(&quot;resources/worldbank.csv&quot;) #better variable names colnames(worldbank) &lt;- c(&quot;country_name&quot;,&quot;country_code&quot;,&quot;series_name&quot;,&quot;series_code&quot;, &quot;time&quot;,&quot;time_code&quot;,&quot;value&quot;) head(worldbank) ## # A tibble: 6 x 7 ## country_name country_code series_name series_code time time_code value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan AFG GDP per capita … NY.GDP.PCAP… 2014 YR2014 6.34e2 ## 2 Afghanistan AFG Life expectancy… SP.DYN.LE00… 2014 YR2014 6.04e1 ## 3 Afghanistan AFG Population, tot… SP.POP.TOTL 2014 YR2014 3.16e7 ## 4 Albania ALB GDP per capita … NY.GDP.PCAP… 2014 YR2014 4.57e3 ## 5 Albania ALB Life expectancy… SP.DYN.LE00… 2014 YR2014 7.78e1 ## 6 Albania ALB Population, tot… SP.POP.TOTL 2014 YR2014 2.89e6 You can see that this data is in long format with the actual value for each of my variables given in the “Value” colum. The “Country Code” variable identifies the country and the “Series Code” variable identifies the specific variable. I also have a “Time Code” but in my case that is not particularly relevant since I only downloaded 2014 data. This is the standard format of the World Bank’s World Development Indicators data. In order to create my analytical dataset I want to reshape this data from long to wide. Given what we know about data.frames in R, it is actually possible to manipulate this dataset with our existing skills to get it in the right format, but this is not the best approach because it opens up to significant errors. We could for example, use a subset command to just grab each variable’s value and save those two different vectors. We could then use a data.frame command or a cbind command to comnbine those together into a new data.frame. However, we would be assuming that the ordering and existence of countries was consistent across all of the vectors that we are combining - and this assumption could prove dangerous if it turned out that in one vector country values were reversed or some were missing. We would then have incorrect values for some countries. A better approach would be to save subsets that include the country code and then use the merge command that we will learn below to merge them together into a single dataset by country code. This will ensure that the values are correct for countries, but it still involves quite a bit of code. An even better solution is to use one of the functions in R that can do this reshaping for us. Base R comes with a reshape command but we will not be using that command here. Instead we are going to use the dcast function from the reshape2 library. You will need to install the reshape2 library to gain access to it. The dcast function in the reshape2 library is one of the “cast” functions that will turn a dataset in long format to a dataset in wide format. There is an analagous function called melt to for from wide to long format (melting and casting, get it?). To use dcast we need to feed in three arguments: the name of the data.frame object in long form A formula that defines the id variable(s) and the variable that identifies distinct variables from the wide format. A values.var variable that identifies which variable contains the actual values. In my case, it should look like: library(reshape2) worldbank_wide &lt;- dcast(worldbank, country_code + country_name + time ~ series_code, value.var = &quot;value&quot;) head(worldbank_wide) ## country_code country_name time NY.GDP.PCAP.CD SP.DYN.LE00.IN ## 1 ABW Aruba 2014 NA 75.45110 ## 2 ADO Andorra 2014 NA NA ## 3 AFG Afghanistan 2014 633.9479 60.37446 ## 4 AGO Angola 2014 5232.7623 52.26688 ## 5 ALB Albania 2014 4568.5688 77.83046 ## 6 ARE United Arab Emirates 2014 44238.5998 77.36817 ## SP.POP.TOTL ## 1 103441 ## 2 72786 ## 3 31627506 ## 4 24227524 ## 5 2893654 ## 6 9086139 Just like that we have data in wide format. Notice how the formula works here. My id variable(s) go to the left of the tilde. In this case, I didn’t really need to specify “time” because I only have one year, but I wanted to show you how this can work with multiple id variables. I also didn’t need both “country_code” and “country_name” but by doing it this way, I will get both variables in my wide data format, which can be useful if I need to look up which country is which code. The series_code that identifies variable names for the wide dataset goes to the right of the tilde. My only problem now is that the variable names are ugly, but I can fix that up pretty easily: colnames(worldbank_wide) &lt;- c(&quot;country_code&quot;,&quot;country_name&quot;,&quot;year&quot;, &quot;gdp_cap&quot;,&quot;life_exp&quot;,&quot;pop&quot;) head(worldbank_wide) ## country_code country_name year gdp_cap life_exp pop ## 1 ABW Aruba 2014 NA 75.45110 103441 ## 2 ADO Andorra 2014 NA NA 72786 ## 3 AFG Afghanistan 2014 633.9479 60.37446 31627506 ## 4 AGO Angola 2014 5232.7623 52.26688 24227524 ## 5 ALB Albania 2014 4568.5688 77.83046 2893654 ## 6 ARE United Arab Emirates 2014 44238.5998 77.36817 9086139 Now lets take a look at the ILO data and see what needs to be done with it. Note that I needed to specify a few column types that were giving readr parsing problems, although we don’t really care about those variables anyway. I also recast the tibble to a standard data.frame for the head command so you can see all of the variables. ilo &lt;- read_csv(&quot;resources/ilostat.csv&quot;, col_types=cols(obs_status=col_character(), obs_status.label=col_character())) head(as.data.frame(ilo)) ## collection collection.label ref_area ref_area.label source ## 1 YI Yearly indicators ALB Albania BA:480 ## 2 YI Yearly indicators ALB Albania BA:480 ## 3 YI Yearly indicators ALB Albania BA:480 ## 4 YI Yearly indicators ALB Albania BA:480 ## 5 YI Yearly indicators ALB Albania BA:480 ## 6 YI Yearly indicators ALB Albania BA:480 ## source.label indicator indicator.label ## 1 Labour force survey UNE_DEAP_SEX_AGE_RT Unemployment rate by sex and age (%) ## 2 Labour force survey UNE_DEAP_SEX_AGE_RT Unemployment rate by sex and age (%) ## 3 Labour force survey UNE_DEAP_SEX_AGE_RT Unemployment rate by sex and age (%) ## 4 Labour force survey UNE_DEAP_SEX_AGE_RT Unemployment rate by sex and age (%) ## 5 Labour force survey UNE_DEAP_SEX_AGE_RT Unemployment rate by sex and age (%) ## 6 Labour force survey UNE_DEAP_SEX_AGE_RT Unemployment rate by sex and age (%) ## sex sex.label classif1 classif1.label classif2 ## 1 SEX_T Sex: Total AGE_10YRBANDS_TOTAL 10-year age bands: Total NA ## 2 SEX_T Sex: Total AGE_10YRBANDS_Y15-24 10-year age bands: 15-24 NA ## 3 SEX_M Sex: Male AGE_10YRBANDS_TOTAL 10-year age bands: Total NA ## 4 SEX_M Sex: Male AGE_10YRBANDS_Y15-24 10-year age bands: 15-24 NA ## 5 SEX_F Sex: Female AGE_10YRBANDS_TOTAL 10-year age bands: Total NA ## 6 SEX_F Sex: Female AGE_10YRBANDS_Y15-24 10-year age bands: 15-24 NA ## classif2.label classif3 classif3.label classif4 classif4.label classif5 ## 1 NA NA NA NA NA NA ## 2 NA NA NA NA NA NA ## 3 NA NA NA NA NA NA ## 4 NA NA NA NA NA NA ## 5 NA NA NA NA NA NA ## 6 NA NA NA NA NA NA ## classif5.label time obs_value obs_status obs_status.label note_classif ## 1 NA 2014 17.49 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 NA 2014 39.05 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 NA 2014 19.16 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 4 NA 2014 42.55 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 NA 2014 15.21 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 NA 2014 32.55 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## note_classif.label note_indicator note_indicator.label notes_source ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; S3:5_T3:104 ## 2 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; S3:5_T3:104 ## 3 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; S3:5_T3:104 ## 4 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; S3:5_T3:104 ## 5 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; S3:5_T3:104 ## 6 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; S3:5_T3:104 ## notes_source.label ## 1 Data reference period: Annual or annual average | Age coverage - maximum age: 74 years old ## 2 Data reference period: Annual or annual average | Age coverage - maximum age: 74 years old ## 3 Data reference period: Annual or annual average | Age coverage - maximum age: 74 years old ## 4 Data reference period: Annual or annual average | Age coverage - maximum age: 74 years old ## 5 Data reference period: Annual or annual average | Age coverage - maximum age: 74 years old ## 6 Data reference period: Annual or annual average | Age coverage - maximum age: 74 years old This data is also in a long format but there is a lot going on here. ILO is breaking down unemployment rates by sex and age within each country over time, with detailed notes on data quality and sources. This is all interesting data, but I don’t need all of it, so let me use a subset command to pull out just what I need which is the total unemployment rate across both sexes and all ages in 2014. ilo &lt;- subset(ilo, sex==&quot;SEX_T&quot; &amp; classif1==&quot;AGE_AGGREGATE_TOTAL&quot; &amp; time==2014, select=c(&quot;ref_area&quot;, &quot;ref_area.label&quot;,&quot;source&quot;,&quot;source.label&quot;, &quot;time&quot;,&quot;obs_value&quot;,&quot;note_classif&quot;,&quot;note_classif.label&quot;, &quot;notes_source&quot;,&quot;notes_source.label&quot;)) head(ilo) ## # A tibble: 6 x 10 ## ref_area ref_area.label source source.label time obs_value note_classif ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 ALB Albania BA:480 Labour forc… 2014 17.5 &lt;NA&gt; ## 2 ARG Argentina BA:153 Labour forc… 2014 7.03 &lt;NA&gt; ## 3 ARG Argentina BA:150 Labour forc… 2014 7.27 &lt;NA&gt; ## 4 ARM Armenia BA:867 Labour forc… 2014 17.6 &lt;NA&gt; ## 5 AUS Australia BA:518 Labour forc… 2014 6.07 &lt;NA&gt; ## 6 AUT Austria BA:536 Labour forc… 2014 5.62 &lt;NA&gt; ## # … with 3 more variables: note_classif.label &lt;chr&gt;, notes_source &lt;chr&gt;, ## # notes_source.label &lt;chr&gt; You will notice right away that we have multiple entries for at least one country (Argentina). We can check this more formally by running a table command on ref_area.label or ref_area. temp &lt;- table(ilo$ref_area.label) temp[temp&gt;1] ## ## Argentina Belarus Brazil China Kazakhstan Singapore ## 2 2 3 2 2 2 A few cases have more than one entry. Lets check out Brazil more closely as it has three entries: head(as.data.frame(subset(ilo, ref_area.label==&quot;Brazil&quot;))) ## ref_area ref_area.label source source.label time obs_value ## 1 BRA Brazil BA:3047 Labour force survey 2014 4.85 ## 2 BRA Brazil BA:356 Labour force survey 2014 6.91 ## 3 BRA Brazil BA:6355 Labour force survey 2014 6.81 ## note_classif note_classif.label notes_source ## 1 &lt;NA&gt; &lt;NA&gt; S4:33_T2:239 ## 2 &lt;NA&gt; &lt;NA&gt; S3:20_T2:239 ## 3 C6:1059 Nonstandard age group: Including age 14 T2:83 ## notes_source.label ## 1 Geographical coverage: Main cities or metropolitan areas | Age coverage - minimum age: 10 years old ## 2 Data reference period: September | Age coverage - minimum age: 10 years old ## 3 Age coverage - minimum age: 14 years old It seems that there are multiple sources for each unemployment rate. The notes provide some indication of how these different numbers were developed. There is no easy programmatic way to make a decision about which source to choose. If I were doing a real research project, I would carefully study the documentation here and reach a decision about each country separately. For our purposes, it seems like the broadest scope measure is generally in the first source listed for a country, so I will just take the first row for countries with multiple rows. I can do this easily in R using the duplicated command to remove repeat country entries. The duplicated command will return a TRUE for values in a vector that duplicate earlier entries. I will also restrict this data to the variable that I actually care about, along with the country names and codes. ilo &lt;- subset(ilo, !duplicated(ref_area), select=c(&quot;ref_area&quot;,&quot;ref_area.label&quot;,&quot;obs_value&quot;)) head(ilo) ## # A tibble: 6 x 3 ## ref_area ref_area.label obs_value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 ALB Albania 17.5 ## 2 ARG Argentina 7.03 ## 3 ARM Armenia 17.6 ## 4 AUS Australia 6.07 ## 5 AUT Austria 5.62 ## 6 AZE Azerbaijan 4.91 table(ilo$ref_area) ## ## ALB ARG ARM AUS AUT AZE BEL BGR BIH BLR BMU BOL BRA BRB BTN CAN CHE CHL CHN COL ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## CRI CUB CUW CYM CYP CZE DEU DNK DOM DZA ECU EGY ESP EST ETH FIN FRA GBR GEO GRC ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## GTM GUM HKG HRV HUN IDN IMN IND IRL IRN ISL ISR ITA JAM JEY JOR JPN KAZ KGZ KHM ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## KOR KOS LKA LTU LUX LVA MAC MAR MDA MDV MEX MKD MLT MNE MNG MOZ MUS MYS NLD NOR ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## NPL NZL PAK PAN PER PHL POL PRI PRT PRY PSE QAT ROU RUS SAU SGP SMR SRB SUR SVK ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## SVN SWE THA TTO TUR TWN TZA UKR URY USA VEN VNM WSM ZAF ZWE ## 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Now, everything looks good. This dataset is still technically in a long format, but since I only have one variable (total unemployment rate), the difference between a long and a wide format is now just in the variable names. Changing the variable names gives me the format I want: colnames(ilo) &lt;- c(&quot;country_code&quot;,&quot;country_name&quot;,&quot;unemployment&quot;) head(ilo) ## # A tibble: 6 x 3 ## country_code country_name unemployment ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 ALB Albania 17.5 ## 2 ARG Argentina 7.03 ## 3 ARM Armenia 17.6 ## 4 AUS Australia 6.07 ## 5 AUT Austria 5.62 ## 6 AZE Azerbaijan 4.91 I now have the World Bank and ILO data in similar formats with a country code, country name, and then the variables of interest. I now just need to combine them together into a single dataset. For that I need to learn how to merge data. Merging data In order to merge datasets, you need some kind of id variable(s) that can be used to link observations across datasets. In my data here, both sources use a three letter capitalized code to identify countries. I am crossing my fingers that they are using the same coding system and that I can use this code to link countries across the two datasets. Generally speaking, there are two different kinds of merges one can perform. In a one-to-one merge, each observation in one dataset should be linked to one and only one observation in the other dataset. In this case, the identifier used to link the two datasets should be unique (not be repeated) within each dataset. In this example, I am performing a one-to-one merge. I can check that my identifiers are unique by running the duplicated command: sum(duplicated(worldbank_wide$country_code)) ## [1] 0 sum(duplicated(ilo$country_code)) ## [1] 0 No duplicates, so we are of to a good start. The other kind of merge that can be performed is a many to one merge. In this case, multiple observations in one dataset would all be linked to a single observation in the other dataset. This is most common for multilevel data in which you are linking lower-level observations to the higher level unit they are nested within. For example, I might want to link the academic records of elementary school students to information about their teachers. In this case, multiple students would be linked to the same teacher data. We will see more examples of this kind of merging next term, when we delve into multilevel models. In practice, R handles both of these merge types with the same basic syntax. The merge command in R will take two different data.frame objects and merge them together by matching the columns identified in the by,by.x, or by.y option. Notice that I renamed the country codes in my datasets to both be country_code in order to simplify my merging by allowing me to use the by option rather than separate by.x and by.y options. It is also better to match on these three letter codes than the country_name variable which is more likely to vary between the two datasets (one dataset may refer to “Ivory Coast” while another refers to “Cote d’Ivoire”, for example). You can also not specificy any id variable for the matching, and merge will then try to match on every variable with the same name in the two datasets - this is not recommended. Here is the basic merge command. test &lt;- merge(worldbank_wide, ilo, by=&quot;country_code&quot;, all.x=TRUE, all.y=TRUE) Merging can be tricky and you should always check and double-check your results to make sure the merge worked properly. In this case, I have saved my merged data.frame as a new object named test. The all.x and all.y option indicates that the final merged object should include rows for both the first and second data.frame even if they didn’t find a match in the other data.frame. Lets begin our checks by examining the number of observations in our datasets. nrow(worldbank_wide) ## [1] 217 nrow(ilo) ## [1] 115 nrow(test) ## [1] 223 There is something odd going on here. The World Bank data included 217 countries, while the ILO data only had 115 countries. So the ILO data is presumably missing a lot of countries that are in the World Bank data. We expect this because the world bank data has more coverage than the ILO data. However when we merge the data together, we now get 223 total countries which is six more than the number of countries in the World Bank data. How did we end up with six more countries in total than the world bank data? There are three possible outomes for each observation in our merged test dataset. In some cases, a match was made and there should be valid data for that observation. In other cases, an observation from the World Bank data failed to find a match in the ILO data. As I noted above, this is not problematic because we know the ILO data has lower coverage than the World Bank data. Finally, an observation from the ILO data may fail to find a match with the World Bank data. This last case is the problematic one and the one where the extra six cases are coming from. How can we find out what these extra six cases are? R has a very nice operator called %in% which can be used to identify cases of overlap and non-overlap between two variables. It will return a TRUE/FALSE value which we can use in this case to identify the cases in the ILO data that were not matched to World Bank data. subset(ilo, !(ilo$country_code %in% worldbank_wide$country_code)) ## # A tibble: 6 x 3 ## country_code country_name unemployment ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 IMN Isle of Man 1.98 ## 2 JEY Jersey 4.6 ## 3 KOS Kosovo 35.2 ## 4 PSE Occupied Palestinian Territory 27.0 ## 5 ROU Romania 6.8 ## 6 TWN Taiwan, China 3.96 Notice that by wrapping the entire %in% statement in a !, I basically turned this into “not in.” In colluquial terms, we are identifying all country codes in the ILO data that are not in the World Bank country codes. Why did these six countries fail to find a match? If I look for the corresponding countries in the World Bank data, I can see that three of these cases (Isle of Man, Kosovo, and Romania) have different codes in the two datasets. The three remaining cases don’t exist in the World Bank data (Taiwan and Palestine for political reasons). I can fix the code for Isle of Man, Kosovo, and Romania by changing these codes in the ILO dataset. ilo$country_code[ilo$country_code==&quot;IMN&quot;] &lt;- &quot;IMY&quot; ilo$country_code[ilo$country_code==&quot;KOS&quot;] &lt;- &quot;KSV&quot; ilo$country_code[ilo$country_code==&quot;ROU&quot;] &lt;- &quot;ROM&quot; Now, I should be able to re-run my merge command and pick up those three matches. Note that I am setting all.y=FALSE because I can’t do anything about the last three cases of Palestine, Taiwan, and Jersey, so I might as well drop them. combined &lt;- merge(worldbank_wide, ilo, by=&quot;country_code&quot;, all.x=TRUE, all.y=FALSE) nrow(combined) ## [1] 217 The total size is now what I expected. I can do a last summary command to check that everything looks as expected. summary(combined) ## country_code country_name.x year gdp_cap ## Length:217 Length:217 Min. :2014 Min. : 286 ## Class :character Class :character 1st Qu.:2014 1st Qu.: 1827 ## Mode :character Mode :character Median :2014 Median : 5469 ## Mean :2014 Mean : 15755 ## 3rd Qu.:2014 3rd Qu.: 16676 ## Max. :2014 Max. :178713 ## NA&#39;s :27 ## life_exp pop country_name.y unemployment ## Min. :48.93 Min. :9.893e+03 Length:217 Min. : 0.180 ## 1st Qu.:65.90 1st Qu.:7.647e+05 Class :character 1st Qu.: 4.888 ## Median :73.51 Median :6.061e+06 Mode :character Median : 6.855 ## Mean :71.43 Mean :3.348e+07 Mean : 8.732 ## 3rd Qu.:77.47 3rd Qu.:2.231e+07 3rd Qu.:10.738 ## Max. :83.98 Max. :1.364e+09 Max. :35.150 ## NA&#39;s :18 NA&#39;s :1 NA&#39;s :105 Note that I now have a country_name.x and a country_name.y variable. This is because there was a country_name variable in both datasets. Rather than overwrite them, R has put them both in the final dataset with a subscript identifying which original dataset they came from. Sometimes this can be useful in testing out whether the merge worked correctly. To clean things up, I can remove one of these variables with a subset command and then rename the other one. combined &lt;- subset(combined, select=c(&quot;country_code&quot;,&quot;country_name.x&quot;,&quot;gdp_cap&quot;, &quot;life_exp&quot;,&quot;pop&quot;,&quot;unemployment&quot;)) colnames(combined)[2] &lt;- &quot;country_name&quot; head(combined) ## country_code country_name gdp_cap life_exp pop unemployment ## 1 ABW Aruba NA 75.45110 103441 NA ## 2 ADO Andorra NA NA 72786 NA ## 3 AFG Afghanistan 633.9479 60.37446 31627506 NA ## 4 AGO Angola 5232.7623 52.26688 24227524 NA ## 5 ALB Albania 4568.5688 77.83046 2893654 17.49 ## 6 ARE United Arab Emirates 44238.5998 77.36817 9086139 NA I should note that the World Bank CSV file had notes at the bottom that weren’t part of the data. I had to open up the CSV file in Excel in order to remove this junk, and then re-save it to get data that would be machine-readable by R. This is a Very Bad but Unfortunately Common Practice.↩ "],
["programming.html", "Programming", " Programming R is the free and open-source version of an older program called Splus. Splus is so named because its syntax was designed to be very similar to C++, one of the most powerful programming languages used in real software development. R carries forward all of this programming capability. Because it was built as a programming language from the ground up, you can do almost anything you want with R if you know enough. We are only going to scratch the surface of the programming capabilities of R in this lab. I want to show you two important programming features in R: The ability to create your own custom functions The ability to repeat the same piece of code on new data iteratively. An Example: Theil’s H As an example, we are going to calculate a measure of segregation called the Information Theory Index or sometimes just Theil’s H, after its creator. Theil’s H provides a measure of the diversity in areas within a region (e.g. tracts within a city, counties within a state), relative to the overall diversity of the region. When Theil’s H equals one, there is no diversity in the subregions of a region, and when Theil’s H equals zero, the diversity in each subregion is equal to the overall diversity of the region. In order to measure Theil’s H, one has to first calculate a measure of diversity for each sub-region and the total region. This measure of diversity is called entropy and is based on the proportions of different groups within the area. If \\(p_j\\) is the proportion of group \\(j\\) in the region and there are \\(J\\) total groups, then the formula for entropy (\\(E\\)) is given by: \\[E=\\sum_{j=1}^J p_j\\log(1/p_j)\\] Entropy will be at its maximum value when the proportion \\(p_j\\) is the same for each group, and entropy will be at its minimum value of zero when the area is made up entirely of one group. Lets take a simple example where we have three groups and the first group is 60% of the population of an area and the remaining two groups are 20% each. Entropy would be: \\[E=(0.6)*\\log(1/0.6)+0.2*\\log(1/0.2)+0.2*log(1/0.2)=0.95\\] With the natural log used here for three groups, the maximum value of entropy is \\(\\log(3)=1.0986123\\), so this area would be considered fairly diverse. In order to calculate Theil’s H, one has to first calculate entropy for each sub-region \\(i\\) (\\(E_i\\)) as well as the overall entropy for the whole region (\\(E\\)). One also needs the population totals for each sub-region (\\(t_i\\)) as well as the total population of the region (\\(T\\)). Theil’s H is then given by: \\[H=1-\\sum_{i=1}^n \\frac{t_i*E_i}{T*E}\\] Theil’s H is a weighted average of of how much the diversity of each sub-region varies from the total region. Higher values of H indicate more segregation in the sense that the diversity of the sub-regions is low relatively to the overall diversity of the region. Our Data For this example, I have data on the number of racial groups (white, black, Latino, Asian, indigenous, and other) in 2010 for each census tract in all 50 states (no Washington DC). Here is a glimpse of the data. library(readr) tracts &lt;- read_csv(&quot;resources/tracts_state.csv&quot;) head(tracts) ## # A tibble: 6 x 8 ## statename tractid white latino black asian indigenous other ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 20100 1601 44 217 14 13 23 ## 2 Alabama 20200 844 75 1214 5 5 27 ## 3 Alabama 20300 2538 87 647 17 14 70 ## 4 Alabama 20400 4030 85 191 18 14 48 ## 5 Alabama 20500 8438 355 1418 295 50 210 ## 6 Alabama 20600 2672 176 738 6 10 66 Each observation here is a census tract and we have census tracts identified by state. To see the number of census tracts within each state: table(tracts$statename) ## ## Alabama Alaska Arizona Arkansas California ## 1178 167 1520 686 8024 ## Colorado Connecticut Delaware Florida Georgia ## 1242 828 214 4182 1957 ## Hawaii Idaho Illinois Indiana Iowa ## 321 298 3115 1507 823 ## Kansas Kentucky Louisiana Maine Maryland ## 766 1110 1129 351 1390 ## Massachusetts Michigan Minnesota Mississippi Missouri ## 1467 2756 1334 659 1391 ## Montana Nebraska Nevada New Hampshire New Jersey ## 271 532 680 292 2002 ## New Mexico New York North Carolina North Dakota Ohio ## 498 4870 2175 205 2943 ## Oklahoma Oregon Pennsylvania Rhode Island South Carolina ## 1046 826 3210 241 1091 ## South Dakota Tennessee Texas Utah Vermont ## 222 1489 5238 585 183 ## Virginia Washington West Virginia Wisconsin Wyoming ## 1886 1445 484 1392 131 In order to calculate Theil’s H for each state, we will need to: Create code that calculates Theil’s H for the tract data from a single state. Iteratively run that code for each state. Calculating Theil’s H for a single state Clearly we need to do some work here that is going to be iterative. We want to calculate Theil’s H for each state, so however we do that calculation we will need to repeat it for all fifty states. Below, we will learn easy ways to make our work iterative, but the first step here is to make sure we understand how to do this calculation for a single case. Once we are satisfied with how we handled that case, we can learn how to generalize our code and make it iterative. To get started, I am going to use Alabama as an example. Let me first pull out these census tracts and save them to a different object. alabama_tracts &lt;- subset(tracts, statename==&quot;Alabama&quot;) head(alabama_tracts) ## # A tibble: 6 x 8 ## statename tractid white latino black asian indigenous other ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 20100 1601 44 217 14 13 23 ## 2 Alabama 20200 844 75 1214 5 5 27 ## 3 Alabama 20300 2538 87 647 17 14 70 ## 4 Alabama 20400 4030 85 191 18 14 48 ## 5 Alabama 20500 8438 355 1418 295 50 210 ## 6 Alabama 20600 2672 176 738 6 10 66 The first step is to calculate the proportion of each racial group within each census tract. There are number of ways to do this, but the most straightforward approach would be to just use a prop.table on a subset of the data that just includes the counts, being sure to specify that I want proportions within rows. There is one caveat to this approach. The prop.table command expects a matrix or an array object not a data.frame so I need to recast my subset into a matrix with the as.matrix command. props &lt;- prop.table(as.matrix(alabama_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;,&quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)]), 1) head(props) ## white latino black asian indigenous other ## [1,] 0.8373431 0.02301255 0.11349372 0.007322176 0.006799163 0.01202929 ## [2,] 0.3889401 0.03456221 0.55944700 0.002304147 0.002304147 0.01244240 ## [3,] 0.7524459 0.02579306 0.19181737 0.005040024 0.004150608 0.02075304 ## [4,] 0.9188326 0.01937984 0.04354765 0.004103967 0.003191974 0.01094391 ## [5,] 0.7837637 0.03297418 0.13171094 0.027401077 0.004644250 0.01950585 ## [6,] 0.7284624 0.04798255 0.20119956 0.001635769 0.002726281 0.01799346 Lets make sure that the proportions add up to one in each row: summary(apply(props, 1, sum)) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 1 1 1 1 1 The next step is to use these proportions to calculate entropy for each tract. The only hard part about this part is summing up across groups within the same tract. I can do this again with the apply command: alabama_tracts$entropy &lt;- apply(props*log(1/props), 1, sum, na.rm=TRUE) head(alabama_tracts) ## # A tibble: 6 x 9 ## statename tractid white latino black asian indigenous other entropy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 20100 1601 44 217 14 13 23 0.606 ## 2 Alabama 20200 844 75 1214 5 5 27 0.891 ## 3 Alabama 20300 2538 87 647 17 14 70 0.755 ## 4 Alabama 20400 4030 85 191 18 14 48 0.381 ## 5 Alabama 20500 8438 355 1418 295 50 210 0.771 ## 6 Alabama 20600 2672 176 738 6 10 66 0.798 summary(alabama_tracts$entropy) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.0000 0.4405 0.6734 0.6376 0.8293 1.3033 The na.rm=TRUE is important here because groups with a proportion of zero need to be ignored since the log of zero is not defined. In order to calculate Theil’s H, I also need a couple of other pieces of information. First, I need \\(t_i\\) which is the total population of each tract. I can get this with the apply command. alabama_tracts$total &lt;- apply(alabama_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;,&quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)], 1, sum) head(alabama_tracts) ## # A tibble: 6 x 10 ## statename tractid white latino black asian indigenous other entropy total ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 20100 1601 44 217 14 13 23 0.606 1912 ## 2 Alabama 20200 844 75 1214 5 5 27 0.891 2170 ## 3 Alabama 20300 2538 87 647 17 14 70 0.755 3373 ## 4 Alabama 20400 4030 85 191 18 14 48 0.381 4386 ## 5 Alabama 20500 8438 355 1418 295 50 210 0.771 10766 ## 6 Alabama 20600 2672 176 738 6 10 66 0.798 3668 Now I just need to calculate entropy for the whole state of Alabama. I can do this by using apply again to sum up tract populations by race and then using prop.table again. props_alabama &lt;- prop.table(apply(alabama_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)], 2, sum)) sum(props_alabama) ## [1] 1 alabama_entropy &lt;- sum(props_alabama*log(1/props_alabama)) alabama_entropy ## [1] 0.8825423 Ok, now I have all the pieces that I need to calculate Theil’s H for alabama: 1-sum(alabama_tracts$total * alabama_tracts$entropy)/ (sum(alabama_tracts$total)*alabama_entropy) ## [1] 0.2675507 So, to re-capitulate, here is the code altogether that will calculate entropy for Alabama: state_tracts &lt;- subset(tracts, statename==&quot;Alabama&quot;) props &lt;- prop.table(as.matrix(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)]), 1) state_tracts$entropy &lt;- apply(props*log(1/props), 1, sum, na.rm=TRUE) state_tracts$total &lt;- apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;,&quot;asian&quot;, &quot;indigenous&quot;,&quot;other&quot;)], 1, sum) props_state &lt;- prop.table(apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)], 2, sum)) entropy_state &lt;- sum(props_state*log(1/props_state)) theil_h &lt;- 1-sum(state_tracts$total * state_tracts$entropy)/ (sum(state_tracts$total)*entropy_state) theil_h ## [1] 0.2675507 Notice that I have changed all of the names of objects I create here that contain “alabama” to a more general “state” name. The only place where “Alabama” is referenced is in the first line of the code. This makes the code very re-usable. I can just change the state name that is called in the first subset command and the code will produce a Theil’s H for that state. Lets try that with Oregon: state_tracts &lt;- subset(tracts, statename==&quot;Oregon&quot;) props &lt;- prop.table(as.matrix(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)]), 1) state_tracts$entropy &lt;- apply(props*log(1/props), 1, sum, na.rm=TRUE) state_tracts$total &lt;- apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;,&quot;asian&quot;, &quot;indigenous&quot;,&quot;other&quot;)], 1, sum) props_state &lt;- prop.table(apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)], 2, sum)) entropy_state &lt;- sum(props_state*log(1/props_state)) theil_h &lt;- 1-sum(state_tracts$total * state_tracts$entropy)/ (sum(state_tracts$total)*entropy_state) theil_h ## [1] 0.1135007 Creating Functions In cases like this where you will end up re-using the same code on different but identically structured objects, I strongly recommend that you create your own custom function. By creating a custom function, you will only have to write the code you need in one place. Also, if you realize there are bugs in the code later, you only need to fix it in one place. Custom functions can be created with the following syntax: myfunctionname &lt;- function(argument1, argument2, ...) { #a bunch of code to run on the objects argument1 and argument2 return(value) } In the call to function you can define how many arguments you wish to feed into the function. Then within the curly brackets you can declare code to run on the objects as you named them. Any object referenced within the curly brackets must be an object identified in the initial function call. The last call within the curly brackets is usually the return command which will return whatever value or object you want the function to produce. Here is my code to create a function named calculateTheilH. calculateTheilH &lt;- function(state_tracts) { props &lt;- prop.table(as.matrix(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)]), 1) state_tracts$entropy &lt;- apply(props*log(1/props), 1, sum, na.rm=TRUE) state_tracts$total &lt;- apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;,&quot;asian&quot;, &quot;indigenous&quot;,&quot;other&quot;)], 1, sum) props_state &lt;- prop.table(apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)], 2, sum)) entropy_state &lt;- sum(props_state*log(1/props_state)) theil_h &lt;- 1-sum(state_tracts$total * state_tracts$entropy)/(sum(state_tracts$total)*entropy_state) return(theil_h) } The function feeds in an object called state_tracts which should be the data.frame of tracts for a given state. The function then runs the same lines of code that I created above. Lets try it out on a couple of states. calculateTheilH(subset(tracts, statename==&quot;Alabama&quot;)) ## [1] 0.2675507 calculateTheilH(subset(tracts, statename==&quot;Oregon&quot;)) ## [1] 0.1135007 calculateTheilH(subset(tracts, statename==&quot;California&quot;)) ## [1] 0.2356094 calculateTheilH(subset(tracts, statename==&quot;New York&quot;)) ## [1] 0.3633379 I have now simplified my code considerably. Without the function I would have had to copy and paste the code for calculating entropy fifty different times. If I then changed or corrected something later in that code, it would have been a nightmare to fix it for all fifty versions. Now I can keep all that code in one place and I just need to type one line for each state to calculate Theil’s H. But I can improve this even more by iteration. Iteration Basically, I want to iterate over states and calculate Theil’s H for each state. Computers are great at this kind of iterative process. Iterative methods are very common in real programming languages because they can handle problems where a lot of repitition is necessary and computers can typically execute them quickly. The way this is handled in most programming languages is by looping. In R, we can solve our iteration problem with a for-loop. for(i in some_vector) { #some code to run that will usually call i } A for-loop will sequentially move through all of the values in a vector and run the code within the curly brackets each time. Usually, this code will reference the particular value of the vector. Here is a very simple example of a for-loop that iterates through all the unique names of states and prints them out. for(state in unique(tracts$statename)) { print(state) } ## [1] &quot;Alabama&quot; ## [1] &quot;Alaska&quot; ## [1] &quot;Arizona&quot; ## [1] &quot;Arkansas&quot; ## [1] &quot;California&quot; ## [1] &quot;Colorado&quot; ## [1] &quot;Connecticut&quot; ## [1] &quot;Delaware&quot; ## [1] &quot;Florida&quot; ## [1] &quot;Georgia&quot; ## [1] &quot;Hawaii&quot; ## [1] &quot;Idaho&quot; ## [1] &quot;Illinois&quot; ## [1] &quot;Indiana&quot; ## [1] &quot;Iowa&quot; ## [1] &quot;Kansas&quot; ## [1] &quot;Kentucky&quot; ## [1] &quot;Louisiana&quot; ## [1] &quot;Maine&quot; ## [1] &quot;Maryland&quot; ## [1] &quot;Massachusetts&quot; ## [1] &quot;Michigan&quot; ## [1] &quot;Minnesota&quot; ## [1] &quot;Mississippi&quot; ## [1] &quot;Missouri&quot; ## [1] &quot;Montana&quot; ## [1] &quot;Nebraska&quot; ## [1] &quot;Nevada&quot; ## [1] &quot;New Hampshire&quot; ## [1] &quot;New Jersey&quot; ## [1] &quot;New Mexico&quot; ## [1] &quot;New York&quot; ## [1] &quot;North Carolina&quot; ## [1] &quot;North Dakota&quot; ## [1] &quot;Ohio&quot; ## [1] &quot;Oklahoma&quot; ## [1] &quot;Oregon&quot; ## [1] &quot;Pennsylvania&quot; ## [1] &quot;Rhode Island&quot; ## [1] &quot;South Carolina&quot; ## [1] &quot;South Dakota&quot; ## [1] &quot;Tennessee&quot; ## [1] &quot;Texas&quot; ## [1] &quot;Utah&quot; ## [1] &quot;Vermont&quot; ## [1] &quot;Virginia&quot; ## [1] &quot;Washington&quot; ## [1] &quot;West Virginia&quot; ## [1] &quot;Wisconsin&quot; ## [1] &quot;Wyoming&quot; This is not very useful, but it does give us a hint of a for-loop that would be more useful. We could iterate through unique state names and then use each state name to define a subset of tracts from that state and apply our calculateTheilH function: for(state in unique(tracts$statename)) { print(calculateTheilH(subset(tracts, statename==state))) } ## [1] 0.2675507 ## [1] 0.1682601 ## [1] 0.2372527 ## [1] 0.2664347 ## [1] 0.2356094 ## [1] 0.1620947 ## [1] 0.2333971 ## [1] 0.1489769 ## [1] 0.2653847 ## [1] 0.2448094 ## [1] 0.1145708 ## [1] 0.104742 ## [1] 0.3701819 ## [1] 0.2661208 ## [1] 0.1608354 ## [1] 0.1873829 ## [1] 0.2045492 ## [1] 0.2501826 ## [1] 0.08692125 ## [1] 0.2872802 ## [1] 0.2363536 ## [1] 0.3383812 ## [1] 0.185217 ## [1] 0.2272864 ## [1] 0.291448 ## [1] 0.2111407 ## [1] 0.2229126 ## [1] 0.1324474 ## [1] 0.08889904 ## [1] 0.2874875 ## [1] 0.2334191 ## [1] 0.3633379 ## [1] 0.2035033 ## [1] 0.2375026 ## [1] 0.2930075 ## [1] 0.1489998 ## [1] 0.1135007 ## [1] 0.3331475 ## [1] 0.2328076 ## [1] 0.1739929 ## [1] 0.2732335 ## [1] 0.3013521 ## [1] 0.2681015 ## [1] 0.1120612 ## [1] 0.0518904 ## [1] 0.2046499 ## [1] 0.1501689 ## [1] 0.1359242 ## [1] 0.3002023 ## [1] 0.1187653 That worked! We just have one problem. These results are just being printed to the console. We want to save them to a new vector. We can do this by initializing a NULL vector and then adding each calculated value to this vector with the concatenate command: theil_h &lt;- NULL for(state in unique(tracts$statename)) { theil_h &lt;- c(theil_h, calculateTheilH(subset(tracts, statename==state))) } length(theil_h) ## [1] 50 summary(theil_h) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.05189 0.15284 0.23005 0.21467 0.26727 0.37018 Ok, so now we have a vector of length 50 that gives the Theil’s H for each state. As a final touch-up step, we can go ahead and put this back into a data frame with a state id. state_data &lt;- data.frame(statename=unique(tracts$statename), theil_h) head(state_data) ## statename theil_h ## 1 Alabama 0.2675507 ## 2 Alaska 0.1682601 ## 3 Arizona 0.2372527 ## 4 Arkansas 0.2664347 ## 5 California 0.2356094 ## 6 Colorado 0.1620947 The lapply command It turns out that R is not very efficient with iterative loops. So, while iterative looping can help solve problems, its better to look for a different solution before you start using iteration in R. That different solution is usually the lapply command or its closely related cousin sapply. The lapply command will apply a given function to every object in a list. If we were to put all of our state-specific tract datasets into a list, we could run an lapply command calling calculateTheilH on the entire list at once. Generally, this will be much faster than a for-loop. The code would look something like: theil_h &lt;- lapply(state_list, calculateTheilH) The only trick is figuring out how to create a list of state tract data where each object in the list is a data frame of tract data for a specific state. It turns out that the split command is just what we need. It will split an object into a list of the same objects by some factor variable. In our case: state_list &lt;- split(tracts, tracts$statename) length(state_list) ## [1] 50 head(state_list$Alabama) ## # A tibble: 6 x 8 ## statename tractid white latino black asian indigenous other ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alabama 20100 1601 44 217 14 13 23 ## 2 Alabama 20200 844 75 1214 5 5 27 ## 3 Alabama 20300 2538 87 647 17 14 70 ## 4 Alabama 20400 4030 85 191 18 14 48 ## 5 Alabama 20500 8438 355 1418 295 50 210 ## 6 Alabama 20600 2672 176 738 6 10 66 The state_list object is a list where each object is the tract data.frame for a given state. Now we can run our lapply command from above: theil_h &lt;- lapply(state_list, calculateTheilH) class(theil_h) ## [1] &quot;list&quot; theil_h$Alabama ## [1] 0.2675507 The only downside to the lapply function is that it outputs the results as a list as well. In this case, where the output of our function is a just a simple number, we would like the output to just be a simple vector. It turns out the sapply function can do this. The sapply function is just a wrapper to lapply that converts the output when possible to a simple vector or matrix format. theil_h &lt;- sapply(state_list, calculateTheilH) class(theil_h) ## [1] &quot;numeric&quot; head(theil_h) ## Alabama Alaska Arizona Arkansas California Colorado ## 0.2675507 0.1682601 0.2372527 0.2664347 0.2356094 0.1620947 Putting It All Together Without the use of functions and iterative methods, calculating this measure would have been a nightmare. We would have had to copy and paste code fifty different times for each state. Instead, we have been able to reduce the entire process to just a few lines of code, most of which is just code for the function: calculateTheilH &lt;- function(state_tracts) { props &lt;- prop.table(as.matrix(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)]), 1) state_tracts$entropy &lt;- apply(props*log(1/props), 1, sum, na.rm=TRUE) state_tracts$total &lt;- apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;,&quot;asian&quot;, &quot;indigenous&quot;,&quot;other&quot;)], 1, sum) props_state &lt;- prop.table(apply(state_tracts[,c(&quot;white&quot;,&quot;latino&quot;,&quot;black&quot;, &quot;asian&quot;,&quot;indigenous&quot;,&quot;other&quot;)], 2, sum)) entropy_state &lt;- sum(props_state*log(1/props_state)) theil_h &lt;- 1-sum(state_tracts$total * state_tracts$entropy)/ (sum(state_tracts$total)*entropy_state) return(theil_h) } tracts &lt;- read_csv(&quot;resources/tracts_state.csv&quot;) theil_h &lt;- sapply(split(tracts, tracts$statename), calculateTheilH) theil_h &lt;- data.frame(statename=names(theil_h), theil_h) rownames(theil_h) &lt;- NULL head(theil_h) ## statename theil_h ## 1 Alabama 0.2675507 ## 2 Alaska 0.1682601 ## 3 Arizona 0.2372527 ## 4 Arkansas 0.2664347 ## 5 California 0.2356094 ## 6 Colorado 0.1620947 "],
["using-r-markdown.html", "Using R Markdown", " Using R Markdown Every piece of content in this book including this lab was produced in what is called an “R Markdown” document. R Markdown is a recent innovation, but its part of a larger universe of tools that allow you to write documents with logical markup and embedded executable code. R Markdown documents allow you to combine textual description, code, and the output from code seamlessly into a single document. There are multiple ways this can be used such as to write brief reports, create a lab notebook, or even to write full research articles. More importantly, R Markdown helps to achieve the goals of the open science movement to make the process of data analysis more open and reproducible by combining in one document the actual analysis with a write-up of that analysis. Plain Text Science In order to understand the idea of R Markdown files, its important to have some understanding of how scientific manuscripts are produced. I am drawing here on Kieran Healy’s Plain Person’s Guide to Plain Text Science paper, which I would highly encoruage you to read. There are basically two models for producing manuscripts. The first model is the “office” model: Office solutions tend towards a cluster of tools where something like Microsoft Word is at the center of your work. A Word file or set of files is the most “real” thing in your project. Changes to your work are tracked inside that file or files. Citation and reference managers plug into those files. The outputs of data analyses—tables, figures—get cut and pasted in as well, or are kept alongside them. The master document may be passed around from person to person to be edited and updated. The final output is exported from it, perhaps to PDF or to HTML, but maybe most often the final output just is the .docx file, cleaned up and with the track changes feature turned off. In the office model, manuscripts are in typically written in an application like Microsoft Word that is WYSIWYG (What-You-See-Is-What-You-Get). The actual substantive content of the text and the visual presentation of that text are inseparable. On the other hand, in the “engineering”\" model the approach is very different. As Healy writes: In the Engineering model, meanwhile, plain text files are at the center of your work. The most “real” thing in your project will either be those files or, more likely, the version control repository that stores the project. Changes are tracked outside of files, again using a version control system. Data analysis is managed in code that produces outputs in (ideally) a known and reproducible manner. Citation and reference management will likely also be done in plain text, as with a BibTeX .bib file. Final outputs are assembled from the plain text and turned to .tex, .html, or .pdf using some kind of typesetting or conversion tool. Very often, because of some unavoidable facts about the world, the final output of this kind of solution is also a .docx file. In the engineering model, manuscripts are written in a plain-text file with some kind of logical markup applied. This plain-text file can then be processed by another application to produce the final viewable output in a variety of formats (e.g. HTML, PDF, docx). In this model, the actual substantive content of the text and the visual presentation of that text are separated. The engineering model tends to be more prevalent in the hard sciences and STEM fields, while the office model is more common in the humanities. The social sciences are the warzone, where the majority of disciplines and individuals probably use the office model, to the eternal consternation and annoyance of those who utilize the engineering model. While the office model seems “easier” because it has a gentler learning curve and seems to get people what they want immediately, via visual representation, the engineering model offers numerous advantages of efficiency and reproducibility, especially for quantitative work. In the typical office model, the need to import tables and figures from an external program into a word-processing document is both a hassle and a source of potential user error. Errors in transcription can occur or some tables and figures might not get updated with new data or models. The engineering model removes this problem entirely, by combining text and code into a single file. Its not What-You-See-Is-What-You-Get but it is What-You-See-Is-What-You-Did. Markdown Syntax The development of Markdown has made the engineering model considerably easier. Before Markdown, the preferred markup language for typesetting scientific manuscripts was LaTex. Latex is powerful and can produce beautiful documents, but it is also complex and has a steep learning curve. Markdown, on the other hand, is dead simple. Markdown is one of many “markup” languages. Markup languages provide a way to write a document in which formatting of text is done by using some kind of coding syntax to tag bits of text. The most well-known markup language is, of course, HTML, which stands for hypertext markup language. Markdown was actually originally written as a means for people to write simple documents that were uncluttered by a lot of markup, but could also be easily converted into HTML. With the arrival of a powerful program called pandoc, which can convert between a variety of document formats, markdown has now become the default geek choice for writing documents in all kinds of formats. You can even convert from markdown to Word. As its tongue-in-cheeck name indicates, the beauty of Markdown is that its syntax is simple and produces text documents that are very readable even before processing. Here is an example of Markdown syntax with most of the basic syntax that you would be likely to need for writing an academic report. # Title Here is a sentence. This sentence continues the same paragraph. Here is a sentence that starts another paragraph. ## A subtitle Here is another sentence, but this time I **bolded** and *italicized* some things. ### A sub-subtitle Here is another sentence but this time, I am going to also create a bulleted list. * First Item * First Subitem * Second Subitem * Second Item * Third Item How about a numbered list? Why not. 1. First Item a. Subitem 1 b. Subitem 3 2. Second Item What if I want to blockquote somebody? Sure, lets do that: &gt; I have sworn upon the altar of god eternal hostility against every form of tyranny over the mind of man. See how easy this is? There is some basic markup syntax here, but it is so subtle that the text is easy to read even without processing into a prettier form. But speaking of processing, how would I turn this into a prettier output form? One of the advantages of markdown is that I have numberous options here. I could process (or “knit”) this text into an HTML, PDF, or MS Word document, among other choices. If you create this markdown file in RStudio with the extension *.md, RStudio will give you the option of converting it to any of those types. You can see what this document looks like knitted to HTML here. You can find a variety of markdown cheatsheets online if you want to learn more abou the basic syntax options. Here is a good one to get you started. R Markdown Markdown is so popular that it has spawned a variety of spin-off flavors. One of those flavors is R Markdown. You can create a new R Markdown document in RStudio by going to File &gt; New File &gt; R Markdown and choosing the document option. It will ask you to choose an output option, but you can switch this at any time. The default is HTML. R Markdown has all the basic capabilities of Markdown, but also allows you to run and display R code and output within what are called code chunks.^[Despite the name, you can also use a variety of other code in these code chunks, including Python, C++, SQL, and Bash. Its even possible to run Stata code in R Markdown. When the R Markdown document is “knitted” by RStudio, the code inside of the code chunks will be run and the output will be displayed within the document. Any objects created in code chunks can also be used by later code chunks in the same document, so you can use one code chunk to load in dataand then run analyses on that data in subsequent code chunks. Code chunks are defined by “code fencing” similar to what we have used on Slack and GitHub. In general, you start a code chunk with a “fence” of three grave accents (`). You end a code chunk with another “fence” of three grave accents. When you do this, R Markdown will display whats inside the code chunk in a coding format. If we want to make the code inside of that chunk executable when R Markdown is knitted, then we need to include the syntax {r} immediately after the opening code fence. So to put it all together here is an example of an executable code chunk in R Markdown. Here is an example of a simple code chunk in an R Markdown document. ```{r} tab &lt;- table(titanic$pclass, titanic$survival) prop.table(tab, 1) ``` Note that this code chunk will only run assuming I have another code chunk that loads in the Titanic data. RStudio offers some nice features for working with code chunks interactively. Here is what that same code chunk looks like when embedded in an R Markdown document in RStudio: The code chunk itself is shown on a slightly different background color to help differentiate it from the rest of the document. In addition, there are a few buttons in the upper right that provide additional functionality. The left most button allows you to adjust some chunk settings that I will cover later. The middle button runs all the prior code chunks in the document. This is useful if this code references data and variables that were created by prior chunks. The “play” button on the right runs this code chunk. If you push the play button, RStudio will process the code in the chunk and display any output inline within your document. This allows you to preview and test your output before knitting the full document. You can see an example of this inline output for the example code chunk here: Knitting the Document Once I am ready to process the document I can “knit” it in RStudio. This will convert the R Markdown document into the desired document format. You can do this by hittin the “knit” button showing at the top of your RStudio window. If you click the carrot on that knit button, you will also be able to select which output option you would like to use. You can choose HTML, PDF, or MS Word as your output. Here is an example R Markdown file that includes many additional features I will introduce below. Here is that R Markdown file knitted an HTML file and here it is as a PDF. Although, you can convert to MS word, the quality of figures and tables is usually not as good. HTML conversion will work out of the box in RStudio. MS Word conversion will also work out of the box if you have it installed on your computer. Conversion to PDF is trickier. To convert from R Markdown to PDF, RStudio actually goes through a two-step process. First, the R Markdown files are converted to TeX files and then those TeX files are converted to PDF. In order to do the second step, you must have a version of TeX installed on your system. There are a variety of options out there, but many of them are huge and require manual tracking of additional packages. However, there is a lightweight solution called tinytex that you can install directly from within R. To do so, you need to install the tinytex package and then use its commands to install tinytex on your system, like so: install.packages(&quot;tinytex&quot;) library(tinytex) if(!tinytex:::is_tinytex()) { install_tinytex(force=TRUE) } If you are on a Mac, there have been some reported issues with this installation due to some file permission issues. If you run into problems, then you should try running the following commands into the Terminal tab in RStudio (not the console!): sudo chown -R `whoami`:admin /usr/local/bin ~/Library/TinyTeX/bin/x86_64-darwin/tlmgr path add Additional Chunk Options In the curly brackets for the code chunk, you can specify a variety of additional options that define how your code chunk and its output will be displayed. Generally, you should name each code chunk. You can name a code chunk by feeding in a single name (no spaces!) right after the “r” declaration. For example, I could name my code chunk “tab-pclass” like so: ```{r tab-pclass} tab &lt;- table(titanic$pclass, titanic$survival) prop.table(tab, 1) ``` Naming is not required, but it does provide some benefits. RStudio will show you all code chunks in your outline and naming them helps you to easily find which code chunk you are looking for. If you get an error while processing your R Markdown document, it will be easier to diagnose which code chunk has the error if you have them named as well. Naming does have one drawback. R Markdown will not process a document if two code chunks have the same name. Instead if will give you a “duplicate label” error. Be careful when copying and pasting code chunks to rename new code chunks with a different name. In addition to names, you can specify a variety of other arguments in the curly brackets in a fashion very similar to R function arguments. Arguments are separated by commas and have a syntax of argument=value. The most common arguments concern whether the final knitted document should contain the code and/or output. By default, the final document will show both the code and the output. This is useful if you are making a “lab notebook” in which you want to document what was done as well as what the results were. However, if you are trying to write a manuscript for publication, you typically will not want the code to show, but you will want to see the output. Here are the arguments you can use to control what gets seen: echo=FALSE - If you set echo to FALSE, then the output will be shown but the code will not be. include=FALSE - If you set include to FALSE, then neither the output or the code will be shown although the code chunk will be run. eval=FALSE - If you set eval to FALSE then the code chunk will not be processed and nothing will be shown. This can be useful if you want to temporarily “turn off” a code chunk without actually removing the code. Lets say that I only wanted my output to show for my example code chunk. I would then change my code chunk as follows: ```{r tab-pclass, echo=FALSE} tab &lt;- table(titanic$pclass, titanic$survival) prop.table(tab, 1) ``` Figures in R Markdown You can use R code chunks to directly produce nice figures within your document. You can also put captions on these figures with the fig.cap argument. Lets make a barplot of the conditional distribution of survival on the Titanic using a code chunk: ```{r fig-pclass, echo=FALSE, fig.cap=&#39;Distribution of Titanic survival by gender&#39;} ggplot(titanic, aes(x=survival, y=..prop.., group=sex, fill=sex))+ geom_bar(position=&quot;dodge&quot;)+ scale_y_continuous(label=scales::percent)+ labs(y=&quot;percent surviving&quot;, x=NULL, fill=&quot;gender&quot;)+ theme_bw() ``` Notice that I am using the fig.cap argument in the code chunk header to specify a figure caption rathe than the title label from within ggplot. Generally, I recommend that you create captions for R Markdown documents in this way, because they will be able to take advantage of additional layout options in many of the output documents. If I run this code chunk, I will get a nice inline display of the figure: You can also change the figure width, height and resolution with arguments to the code chunk, but we won’t cover those here. Making Tables in R Markdown Making tables in R Markdown is not quite as smooth or as easy as making figures. You can of course just produce R output directly as a table, as I did above for the passenger class by survival crosstab, but this will not look very polished. There are a few different optional library commands that will produce table output that looks nicer in R Markdown. The major options that I am familiar with are: the kable command in the knitr package. This command can output tables in a native markdown format so that they will be converted properly regardless of what final output you are using. The pandoc.table command from the pander package. This command also produces tables in a native markdown format. Its very similar to the kable command with arguably more customization options. The xtable command in the xtable package. This will produce output either in LaTex (for PDF) or HTML format, so if you switch output types, you will have to remember to make the corresponding change to this format. This command is highly customizable but is also complex. The texreg package has commands for producing tables of regression models using either the htmlreg or texreg command for HTML or PDF output, respectively. You will need to switch output type in the command if you switch output type for the main document. For our purposes, I am going to show you how to make a crosstab with kable or pandoc.table and a regression model table with text. Remember that in all cases, you will need to install these packages using install.packages in order to use them. Using kable and pandoc.table Lets look at a passenger class by survival crosstab in the Titanic data using kable and pandoc.table. The kable command has one special advantage. Because this command comes from the same library that “knits” together our document, RStudio knows that the output should be displayed as a real table rather than as simple R output. Here is the code chunk to produce the table in kable: ```{r kable-table, echo=FALSE} kable(tab, caption = &quot;Cross-tabulation of passenger class by survival on the Titanic&quot;) ``` And the output: Table 25: Cross-tabulation of passenger class by survival on the Titanic Survived Died First 200 123 Second 119 158 Third 181 528 The table looks pretty decent. My major complaints are that the variables themselves are not labeled directly and that the table stretches out across the whole width. Some of this can be adjusted by settings, but if you really want to make the table nicer, you can try the kableExtra package which a variety of nice formatting features to a kable table. To produce the table using pandoc.table, we use almost the exact same syntax except for one important change. We will need to add the argument results='asis' in the code chunk to tell R to treat the output as regular markdown text that can be further processed. ```{r pandoc-table, echo=FALSE, results=&#39;asis&#39;} pandoc.table(tab, caption = &quot;Cross-tabulation of passenger class by survival on the Titanic&quot;) ``` And the output: Cross-tabulation of passenger class by survival on the Titanic Survived Died First 200 123 Second 119 158 Third 181 528 I like the compact nature of this table. I don’t care for the lack of left alignment for the row names, but I could have adjusted this with some further options that you caan explore in the help file. Using texreg The texreg package is an extremely useful package for producing regression tables in the style that is common in journal articles. It is not the only package of this sort. The other major package that is used for this purpose is called stargazer but I think texreg has a few advantages so it is the only I use for our course. I would recommend that you use the development version of the texreg package that is available on GitHub because it has many features for playing nice with R Markdown that are not available in the default package. To install the GitHub development version, you will need to install the devtools package and then use the install_github function: install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;leifeld/texreg&quot;) The texreg package is highly customizable. We will focus on only a few of its customization options here in order to get up and running. To get started lets create a sequence of nested models in R predicing Tomato Meter ratings of movies: model1 &lt;- lm(TomatoMeter~I(Runtime-90), data=movies) model2 &lt;- update(model1,.~.+Rating) model3 &lt;- update(model2,.~.+I(Runtime-90)*Rating) model4 &lt;- update(model3,.~.+I(Year-2001)+Genre+I(BoxOffice-mean(BoxOffice))) The texreg package comes with several different commands that will give the model results in different formats depending on the output that the user wants, but we will only need to use two of these commands directly. screenreg - This command will print the results out as nicely formatted plain text. This is useful when interactively testing out models in an R Markdown document but is not recommended for a “final product.” knitreg - This command actually calls up one of three different commands: htmlreg, texreg, or wordreg for displaying results in HTML, PDF, or Word documents, respectively. It automatically figures out the correct command to call up and handles a lot of the messy details. The output will then be nicely formatted for the selected document output. You will need to add the results='asis' argument to your code chunk header for the results to display nicely. Regardless of which command you use, the basic syntax is the same Therefore, you just need to switch the command name to switch the output. The most important required argument for the texreg commands is a list of the model objects that should go into the table. So, if I want to print my results to the screen, I would use the following basic syntax: ```{r tab-screenreg, echo=FALSE} screenreg(list(model1, model2, model3, model4)) ``` The output will look as follows: ## ## =============================================================================== ## Model 1 Model 2 Model 3 Model 4 ## ------------------------------------------------------------------------------- ## (Intercept) 41.60 *** 53.93 *** 53.97 *** 30.62 *** ## (0.68) (3.33) (3.32) (4.33) ## Runtime - 90 0.41 *** 0.44 *** 0.40 0.31 ## (0.03) (0.03) (0.23) (0.22) ## RatingPG -12.87 *** -12.62 *** -6.35 ## (3.58) (3.69) (3.59) ## RatingPG-13 -18.78 *** -20.95 *** -1.85 ## (3.46) (3.51) (4.07) ## RatingR -8.44 * -6.73 13.23 ** ## (3.43) (3.48) (4.07) ## Runtime - 90:RatingPG 0.02 0.15 ## (0.25) (0.24) ## Runtime - 90:RatingPG-13 0.16 -0.02 ## (0.23) (0.22) ## Runtime - 90:RatingR -0.07 -0.13 ## (0.23) (0.23) ## Year - 2001 0.17 ## (0.13) ## GenreAnimation 24.30 *** ## (3.45) ## GenreComedy 6.10 ** ## (1.88) ## GenreDrama 18.89 *** ## (2.13) ## GenreFamily 9.85 ** ## (3.11) ## GenreHorror -5.02 * ## (2.33) ## GenreMusical/Music 11.90 *** ## (2.86) ## GenreMystery 10.50 * ## (4.14) ## GenreRomance 13.91 *** ## (2.62) ## GenreSciFi/Fantasy 2.98 ## (2.23) ## GenreThriller 6.34 ** ## (2.42) ## BoxOffice - mean(BoxOffice) 0.09 *** ## (0.01) ## ------------------------------------------------------------------------------- ## R^2 0.07 0.10 0.11 0.20 ## Adj. R^2 0.07 0.10 0.11 0.20 ## Num. obs. 2553 2553 2553 2553 ## =============================================================================== ## *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05 You can see that this table has the same structure that people expect to see for tables of regression models. The texreg functions come with a variety of optional commands, however, that will allow me to make this look even nicer. Below, I show off some texreg options that will allow me to produce a nicer table. ```{r tab-screenreg-better, echo=FALSE, results=&#39;asis&#39;} knitreg(list(model1, model2, model3, model4), caption=&quot;Linear models predicting a movie&#39;s tomato meter rating&quot;, custom.coef.names = c(&quot;Intercept&quot;, &quot;Movie runtime in minutes&quot;, &quot;PG&quot;, &quot;PG-13&quot;,&quot;R&quot;, &quot;Runtime*PG&quot;, &quot;Runtime*PG-13&quot;, &quot;Runtime*R&quot;, &quot;Year of release&quot;, &quot;Animation&quot;,&quot;Comedy&quot;,&quot;Drama&quot;,&quot;Family&quot;,&quot;Horror&quot;, &quot;Musical&quot;,&quot;Mystery&quot;,&quot;Romance&quot;,&quot;Sci-Fi/Fantasy&quot;, &quot;Thriller&quot;,&quot;Box office returns (millions USD)&quot;), digits = 3, caption.above=TRUE, include.rsquared=TRUE, include.adjrs=FALSE, include.nobs=TRUE, include.rmse=FALSE) ``` First, notice that I have switched from screenreg to knitreg. That will allow the results to appear nicely formatted in the HTML document you are reading right now. Note also that I have added the results='asis' to my code chunk header. Without this command, the knitting process won’t know to treat the output directly as HTML code and will instead treat it like plain text. What do all of the additional arguments to knitreg do? caption - this specifies a caption for the table. The caption will not show in the inline output to screenreg but will show in the final knitted document. custom.coef.names - this argument requires a vector of character strings to use for the labels of the independent variables. Without this command, the variable names will appear exactly as they are entered into the lm formula, so this argument is necessary for a polished final product. However, I would recommend not adding this argument until you are fairly certain that you have a final table because any changes to models will require changing the values for this argument. digits - This argument specifies how many decimal places each numeric value should be rounded. caption.above - setting this to TRUE will cause your caption to appear above rather than below the table. include. - These commands designate what summary statistics should be displayed in the table. When I enter all this all in, I get the following nicely formatted table: Linear models predicting a movie’s tomato meter rating Model 1 Model 2 Model 3 Model 4 Intercept 41.602*** 53.930*** 53.966*** 30.616*** (0.680) (3.326) (3.325) (4.334) Movie runtime in minutes 0.405*** 0.443*** 0.398 0.310 (0.030) (0.030) (0.229) (0.222) PG -12.870*** -12.618*** -6.353 (3.584) (3.685) (3.594) PG-13 -18.776*** -20.945*** -1.848 (3.458) (3.512) (4.074) R -8.437* -6.734 13.228** (3.435) (3.476) (4.073) Runtime*PG 0.015 0.148 (0.247) (0.235) Runtime*PG-13 0.163 -0.018 (0.233) (0.224) Runtime*R -0.069 -0.134 (0.233) (0.225) Year of release 0.165 (0.128) Animation 24.297*** (3.446) Comedy 6.100** (1.880) Drama 18.890*** (2.128) Family 9.851** (3.109) Horror -5.022* (2.334) Musical 11.901*** (2.861) Mystery 10.500* (4.138) Romance 13.915*** (2.616) Sci-Fi/Fantasy 2.982 (2.231) Thriller 6.339** (2.420) Box office returns (millions USD) 0.095*** (0.009) R2 0.067 0.104 0.108 0.204 Num. obs. 2553 2553 2553 2553 ***p &lt; 0.001; **p &lt; 0.01; *p &lt; 0.05 Citations in R Markdown In order to add citations in R Markdown, you will need to create a BibTex bibliography (*.bib file) of your references. Most reference managers (including Zotero and Mendeley) will allow you to do this. You will then need to put your .bib file in the same directory as your R Markdown file and add a line to the top header in your R Markdown file that looks like this: bibliography: bibliography.bib Where you replace “bibliography.bib” with whatever the actual name of your bibliography is. You can then call up references in the text using the citation key for each entry. You can see examples of how this citation syntax works here. When you knit your document, the citations will be filled in with proper names and a reference list will be placed at the end of your document. Writing Equations in R Markdown One final nice feature of R Markdown is that you can write complex equations. The syntax for writing equations is borrowed from the typesetting program LaTex. This syntax can take some time to learn, but is worth the effort if you will be using complex equations in your work. You can start math equations by surrounding them on either side with double dollar signs, like so: $$\\hat{y}_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}$$ This will be processed by RStudio as: \\[\\hat{y}_i=\\beta_0+\\beta_1x_{i1}+\\beta_2x_{i2}+\\ldots+\\beta_px_{ip}\\] "]
]
