<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Generalized Linear Model | Statistical Analysis in Sociology</title>
  <meta name="description" content="Generalized Linear Model | Statistical Analysis in Sociology, an entry level textbook for practical statistics in R" />
  <meta name="generator" content="bookdown 0.17 and GitBook 2.6.7" />

  <meta property="og:title" content="Generalized Linear Model | Statistical Analysis in Sociology" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="https://stat-analysis.netlify.app/" />
  
  <meta property="og:description" content="Generalized Linear Model | Statistical Analysis in Sociology, an entry level textbook for practical statistics in R" />
  <meta name="github-repo" content="AaronGullickson/combined_stats" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Generalized Linear Model | Statistical Analysis in Sociology" />
  
  <meta name="twitter:description" content="Generalized Linear Model | Statistical Analysis in Sociology, an entry level textbook for practical statistics in R" />
  

<meta name="author" content="Aaron Gullickson" />


<meta name="date" content="2020-05-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-probability-model.html"/>
<link rel="next" href="logit-model.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.1/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.49.4/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.49.4/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="my_styles.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="understanding-data.html"><a href="understanding-data.html"><i class="fa fa-check"></i>Understanding Data</a><ul>
<li class="chapter" data-level="" data-path="what-does-data-look-like.html"><a href="what-does-data-look-like.html"><i class="fa fa-check"></i>What Does Data Look Like?</a><ul>
<li class="chapter" data-level="" data-path="what-does-data-look-like.html"><a href="what-does-data-look-like.html#the-observations"><i class="fa fa-check"></i>The observations</a></li>
<li class="chapter" data-level="" data-path="what-does-data-look-like.html"><a href="what-does-data-look-like.html#the-variables"><i class="fa fa-check"></i>The variables</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="what-can-we-do-with-data.html"><a href="what-can-we-do-with-data.html"><i class="fa fa-check"></i>What Can We Do With Data?</a><ul>
<li class="chapter" data-level="" data-path="what-can-we-do-with-data.html"><a href="what-can-we-do-with-data.html#how-is-a-variable-distributed"><i class="fa fa-check"></i>How is a variable distributed?</a></li>
<li class="chapter" data-level="" data-path="what-can-we-do-with-data.html"><a href="what-can-we-do-with-data.html#measuring-association"><i class="fa fa-check"></i>Measuring association</a></li>
<li class="chapter" data-level="" data-path="what-can-we-do-with-data.html"><a href="what-can-we-do-with-data.html#making-statistical-inferences"><i class="fa fa-check"></i>Making statistical inferences</a></li>
<li class="chapter" data-level="" data-path="what-can-we-do-with-data.html"><a href="what-can-we-do-with-data.html#building-models"><i class="fa fa-check"></i>Building Models</a></li>
<li class="chapter" data-level="" data-path="what-can-we-do-with-data.html"><a href="what-can-we-do-with-data.html#observational-data-experimental-thinking"><i class="fa fa-check"></i>Observational Data, Experimental Thinking</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="the-distribution-of-a-variable.html"><a href="the-distribution-of-a-variable.html"><i class="fa fa-check"></i>The Distribution of a Variable</a><ul>
<li class="chapter" data-level="" data-path="looking-at-distributions.html"><a href="looking-at-distributions.html"><i class="fa fa-check"></i>Looking at Distributions</a><ul>
<li class="chapter" data-level="" data-path="looking-at-distributions.html"><a href="looking-at-distributions.html#looking-at-the-distribution-of-a-categorical-variable"><i class="fa fa-check"></i>Looking at the distribution of a categorical variable</a></li>
<li class="chapter" data-level="" data-path="looking-at-distributions.html"><a href="looking-at-distributions.html#looking-at-the-distribution-of-a-quantitative-variable"><i class="fa fa-check"></i>Looking at the distribution of a quantitative variable</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="measuring-the-center-of-a-distribution.html"><a href="measuring-the-center-of-a-distribution.html"><i class="fa fa-check"></i>Measuring the Center of a Distribution</a><ul>
<li class="chapter" data-level="" data-path="measuring-the-center-of-a-distribution.html"><a href="measuring-the-center-of-a-distribution.html#the-mean"><i class="fa fa-check"></i>The mean</a></li>
<li class="chapter" data-level="" data-path="measuring-the-center-of-a-distribution.html"><a href="measuring-the-center-of-a-distribution.html#the-median"><i class="fa fa-check"></i>The median</a></li>
<li class="chapter" data-level="" data-path="measuring-the-center-of-a-distribution.html"><a href="measuring-the-center-of-a-distribution.html#the-mode"><i class="fa fa-check"></i>The mode</a></li>
<li class="chapter" data-level="" data-path="measuring-the-center-of-a-distribution.html"><a href="measuring-the-center-of-a-distribution.html#comparing-the-mean-and-median"><i class="fa fa-check"></i>Comparing the mean and median</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="percentiles-and-the-five-number-summary.html"><a href="percentiles-and-the-five-number-summary.html"><i class="fa fa-check"></i>Percentiles and the Five Number Summary</a><ul>
<li class="chapter" data-level="" data-path="percentiles-and-the-five-number-summary.html"><a href="percentiles-and-the-five-number-summary.html#percentiles"><i class="fa fa-check"></i>Percentiles</a></li>
<li class="chapter" data-level="" data-path="percentiles-and-the-five-number-summary.html"><a href="percentiles-and-the-five-number-summary.html#the-five-number-summary"><i class="fa fa-check"></i>The five number summary</a></li>
<li class="chapter" data-level="" data-path="percentiles-and-the-five-number-summary.html"><a href="percentiles-and-the-five-number-summary.html#boxplots"><i class="fa fa-check"></i>Boxplots</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="measuring-the-spread-of-a-distribution.html"><a href="measuring-the-spread-of-a-distribution.html"><i class="fa fa-check"></i>Measuring the Spread of a Distribution</a><ul>
<li class="chapter" data-level="" data-path="measuring-the-spread-of-a-distribution.html"><a href="measuring-the-spread-of-a-distribution.html#range-and-interquartile-range"><i class="fa fa-check"></i>Range and interquartile range</a></li>
<li class="chapter" data-level="" data-path="measuring-the-spread-of-a-distribution.html"><a href="measuring-the-spread-of-a-distribution.html#variance-and-standard-deviation"><i class="fa fa-check"></i>Variance and standard deviation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="measuring-association-1.html"><a href="measuring-association-1.html"><i class="fa fa-check"></i>Measuring Association</a><ul>
<li class="chapter" data-level="" data-path="the-two-way-table.html"><a href="the-two-way-table.html"><i class="fa fa-check"></i>The Two-Way Table</a><ul>
<li class="chapter" data-level="" data-path="the-two-way-table.html"><a href="the-two-way-table.html#conditional-distributions"><i class="fa fa-check"></i>Conditional distributions</a></li>
<li class="chapter" data-level="" data-path="the-two-way-table.html"><a href="the-two-way-table.html#odds-ratio-advanced"><i class="fa fa-check"></i>Odds ratio (advanced)</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mean-differences.html"><a href="mean-differences.html"><i class="fa fa-check"></i>Mean Differences</a><ul>
<li class="chapter" data-level="" data-path="mean-differences.html"><a href="mean-differences.html#graphically-examining-differences-in-distributions"><i class="fa fa-check"></i>Graphically examining differences in distributions</a></li>
<li class="chapter" data-level="" data-path="mean-differences.html"><a href="mean-differences.html#comparing-differences-in-the-mean"><i class="fa fa-check"></i>Comparing differences in the mean</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="scatterplot-and-correlation-coefficient.html"><a href="scatterplot-and-correlation-coefficient.html"><i class="fa fa-check"></i>Scatterplot and Correlation Coefficient</a><ul>
<li class="chapter" data-level="" data-path="scatterplot-and-correlation-coefficient.html"><a href="scatterplot-and-correlation-coefficient.html#the-scatterplot"><i class="fa fa-check"></i>The scatterplot</a></li>
<li class="chapter" data-level="" data-path="scatterplot-and-correlation-coefficient.html"><a href="scatterplot-and-correlation-coefficient.html#the-correlation-coefficient"><i class="fa fa-check"></i>The correlation coefficient</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="statistical-inference.html"><a href="statistical-inference.html"><i class="fa fa-check"></i>Statistical Inference</a><ul>
<li class="chapter" data-level="" data-path="the-problem-of-statistical-inference.html"><a href="the-problem-of-statistical-inference.html"><i class="fa fa-check"></i>The Problem of Statistical Inference</a></li>
<li class="chapter" data-level="" data-path="the-concept-of-the-sampling-distribution.html"><a href="the-concept-of-the-sampling-distribution.html"><i class="fa fa-check"></i>The Concept of the Sampling Distribution</a><ul>
<li class="chapter" data-level="" data-path="the-concept-of-the-sampling-distribution.html"><a href="the-concept-of-the-sampling-distribution.html#example-class-height"><i class="fa fa-check"></i>Example: class height</a></li>
<li class="chapter" data-level="" data-path="the-concept-of-the-sampling-distribution.html"><a href="the-concept-of-the-sampling-distribution.html#central-limit-theorem-and-the-normal-distribution"><i class="fa fa-check"></i>Central limit theorem and the normal distribution</a></li>
<li class="chapter" data-level="" data-path="the-concept-of-the-sampling-distribution.html"><a href="the-concept-of-the-sampling-distribution.html#what-can-we-do-with-the-sampling-distribution"><i class="fa fa-check"></i>What can we do with the sampling distribution?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html"><i class="fa fa-check"></i>Confidence Intervals</a><ul>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html#what-do-we-mean-by-confident"><i class="fa fa-check"></i>What do we mean by “confident?”</a></li>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-the-confidence-interval-for-the-sample-mean"><i class="fa fa-check"></i>Calculating the confidence interval for the sample mean</a></li>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html#calculating-the-confidence-interval-for-other-sample-statistics"><i class="fa fa-check"></i>Calculating the confidence interval for other sample statistics</a></li>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html#example-with-proportions"><i class="fa fa-check"></i>Example with proportions</a></li>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html#example-with-mean-differences"><i class="fa fa-check"></i>Example with mean differences</a></li>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html#example-with-proportion-differences"><i class="fa fa-check"></i>Example with proportion differences</a></li>
<li class="chapter" data-level="" data-path="confidence-intervals.html"><a href="confidence-intervals.html#example-with-correlation-coefficient"><i class="fa fa-check"></i>Example with correlation coefficient</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html"><i class="fa fa-check"></i>Hypothesis Tests</a><ul>
<li class="chapter" data-level="" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#example-coke-winners"><i class="fa fa-check"></i>Example: Coke winners</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#the-general-procedure-of-hypothesis-testing"><i class="fa fa-check"></i>The general procedure of hypothesis testing</a></li>
<li class="chapter" data-level="" data-path="hypothesis-tests.html"><a href="hypothesis-tests.html#hypothesis-tests-of-relationships"><i class="fa fa-check"></i>Hypothesis tests of relationships</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="building-models-1.html"><a href="building-models-1.html"><i class="fa fa-check"></i>Building Models</a><ul>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html"><i class="fa fa-check"></i>The OLS Regression Line</a><ul>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html#the-formula-for-a-line"><i class="fa fa-check"></i>The Formula for a Line</a></li>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html#calculating-the-best-fitting-line"><i class="fa fa-check"></i>Calculating the Best-Fitting Line</a></li>
<li><a href="the-ols-regression-line.html#using-the-lm-command-to-calculate-ols-regression-lines-in-r">Using the <code>lm</code> command to calculate OLS regression lines in <em>R</em></a></li>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html#adding-an-ols-regression-line-to-a-plot"><i class="fa fa-check"></i>Adding an OLS regression line to a plot</a></li>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html#the-ols-regression-line-as-a-model"><i class="fa fa-check"></i>The OLS regression line as a model</a></li>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html#interpeting-slopes-and-intercepts"><i class="fa fa-check"></i>Interpeting Slopes and Intercepts</a></li>
<li><a href="the-ols-regression-line.html#how-good-is-x-as-a-predictor-of-y">How good is <span class="math inline">\(x\)</span> as a predictor of <span class="math inline">\(y\)</span>?</a></li>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html#inference-for-ols-regression-models"><i class="fa fa-check"></i>Inference for OLS Regression models</a></li>
<li class="chapter" data-level="" data-path="the-ols-regression-line.html"><a href="the-ols-regression-line.html#regression-line-cautions"><i class="fa fa-check"></i>Regression Line Cautions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="the-power-of-controlling-for-other-variables.html"><a href="the-power-of-controlling-for-other-variables.html"><i class="fa fa-check"></i>The Power of Controlling for Other Variables</a><ul>
<li class="chapter" data-level="" data-path="the-power-of-controlling-for-other-variables.html"><a href="the-power-of-controlling-for-other-variables.html#interpreting-results-in-a-multivariate-ols-regression-models"><i class="fa fa-check"></i>Interpreting results in a multivariate OLS regression models</a></li>
<li class="chapter" data-level="" data-path="the-power-of-controlling-for-other-variables.html"><a href="the-power-of-controlling-for-other-variables.html#including-more-than-two-independent-variables"><i class="fa fa-check"></i>Including more than two independent variables</a></li>
<li class="chapter" data-level="" data-path="the-power-of-controlling-for-other-variables.html"><a href="the-power-of-controlling-for-other-variables.html#how-to-read-a-table-of-regression-results"><i class="fa fa-check"></i>How to read a table of regression results</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="including-categorical-variables-as-predictors.html"><a href="including-categorical-variables-as-predictors.html"><i class="fa fa-check"></i>Including Categorical Variables as Predictors</a><ul>
<li class="chapter" data-level="" data-path="including-categorical-variables-as-predictors.html"><a href="including-categorical-variables-as-predictors.html#indicator-variables"><i class="fa fa-check"></i>Indicator variables</a></li>
<li class="chapter" data-level="" data-path="including-categorical-variables-as-predictors.html"><a href="including-categorical-variables-as-predictors.html#categorical-variables-with-more-than-two-categories"><i class="fa fa-check"></i>Categorical variables with more than two categories</a></li>
<li class="chapter" data-level="" data-path="including-categorical-variables-as-predictors.html"><a href="including-categorical-variables-as-predictors.html#categorical-and-quantitative-variables-combined-in-a-single-model"><i class="fa fa-check"></i>Categorical and quantitative variables combined in a single model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="interaction-terms.html"><a href="interaction-terms.html"><i class="fa fa-check"></i>Interaction Terms</a><ul>
<li class="chapter" data-level="" data-path="interaction-terms.html"><a href="interaction-terms.html#the-nature-of-additive-models"><i class="fa fa-check"></i>The nature of additive models</a></li>
<li class="chapter" data-level="" data-path="interaction-terms.html"><a href="interaction-terms.html#the-interaction-term"><i class="fa fa-check"></i>The interaction term</a></li>
<li class="chapter" data-level="" data-path="interaction-terms.html"><a href="interaction-terms.html#interpreting-interaction-terms"><i class="fa fa-check"></i>Interpreting interaction terms</a></li>
<li><a href="interaction-terms.html#interaction-terms-in-r">Interaction terms in <em>R</em></a></li>
<li class="chapter" data-level="" data-path="interaction-terms.html"><a href="interaction-terms.html#interaction-terms-with-multiple-categories"><i class="fa fa-check"></i>Interaction terms with multiple categories</a></li>
<li class="chapter" data-level="" data-path="interaction-terms.html"><a href="interaction-terms.html#interaction-terms-with-two-categorical-variables"><i class="fa fa-check"></i>Interaction terms with two categorical variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-complications.html"><a href="model-complications.html"><i class="fa fa-check"></i>Model Complications</a><ul>
<li class="chapter" data-level="" data-path="the-linear-model-revisited.html"><a href="the-linear-model-revisited.html"><i class="fa fa-check"></i>The Linear Model, Revisited</a><ul>
<li class="chapter" data-level="" data-path="the-linear-model-revisited.html"><a href="the-linear-model-revisited.html#reformulating-the-linear-model"><i class="fa fa-check"></i>Reformulating the linear model</a></li>
<li class="chapter" data-level="" data-path="the-linear-model-revisited.html"><a href="the-linear-model-revisited.html#marginal-effects"><i class="fa fa-check"></i>Marginal effects</a></li>
<li class="chapter" data-level="" data-path="the-linear-model-revisited.html"><a href="the-linear-model-revisited.html#linear-model-assumptions"><i class="fa fa-check"></i>Linear model assumptions</a></li>
<li class="chapter" data-level="" data-path="the-linear-model-revisited.html"><a href="the-linear-model-revisited.html#estimating-a-linear-model"><i class="fa fa-check"></i>Estimating a linear model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="modeling-non-linearity.html"><a href="modeling-non-linearity.html"><i class="fa fa-check"></i>Modeling Non-Linearity</a><ul>
<li class="chapter" data-level="" data-path="modeling-non-linearity.html"><a href="modeling-non-linearity.html#smoothing"><i class="fa fa-check"></i>Smoothing</a></li>
<li class="chapter" data-level="" data-path="modeling-non-linearity.html"><a href="modeling-non-linearity.html#residual-plots"><i class="fa fa-check"></i>Residual Plots</a></li>
<li class="chapter" data-level="" data-path="modeling-non-linearity.html"><a href="modeling-non-linearity.html#transformations"><i class="fa fa-check"></i>Transformations</a></li>
<li class="chapter" data-level="" data-path="modeling-non-linearity.html"><a href="modeling-non-linearity.html#polynomial-models"><i class="fa fa-check"></i>Polynomial Models</a></li>
<li class="chapter" data-level="" data-path="modeling-non-linearity.html"><a href="modeling-non-linearity.html#splines"><i class="fa fa-check"></i>Splines</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="the-iid-violation-and-robust-standard-errors.html"><a href="the-iid-violation-and-robust-standard-errors.html"><i class="fa fa-check"></i>The IID Violation and Robust Standard Errors</a><ul>
<li class="chapter" data-level="" data-path="the-iid-violation-and-robust-standard-errors.html"><a href="the-iid-violation-and-robust-standard-errors.html#violations-of-independence"><i class="fa fa-check"></i>Violations of independence</a></li>
<li class="chapter" data-level="" data-path="the-iid-violation-and-robust-standard-errors.html"><a href="the-iid-violation-and-robust-standard-errors.html#heteroscedasticity"><i class="fa fa-check"></i>Heteroscedasticity</a></li>
<li class="chapter" data-level="" data-path="the-iid-violation-and-robust-standard-errors.html"><a href="the-iid-violation-and-robust-standard-errors.html#fixing-iid-violations"><i class="fa fa-check"></i>Fixing IID violations</a></li>
<li class="chapter" data-level="" data-path="the-iid-violation-and-robust-standard-errors.html"><a href="the-iid-violation-and-robust-standard-errors.html#weighted-least-squares"><i class="fa fa-check"></i>Weighted least squares</a></li>
<li class="chapter" data-level="" data-path="the-iid-violation-and-robust-standard-errors.html"><a href="the-iid-violation-and-robust-standard-errors.html#robust-standard-errors"><i class="fa fa-check"></i>Robust standard errors</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="sample-design-and-weighting.html"><a href="sample-design-and-weighting.html"><i class="fa fa-check"></i>Sample Design and Weighting</a><ul>
<li class="chapter" data-level="" data-path="sample-design-and-weighting.html"><a href="sample-design-and-weighting.html#clustermultistage-sampling"><i class="fa fa-check"></i>Cluster/multistage sampling</a></li>
<li class="chapter" data-level="" data-path="sample-design-and-weighting.html"><a href="sample-design-and-weighting.html#stratification"><i class="fa fa-check"></i>Stratification</a></li>
<li class="chapter" data-level="" data-path="sample-design-and-weighting.html"><a href="sample-design-and-weighting.html#weights"><i class="fa fa-check"></i>Weights</a></li>
<li class="chapter" data-level="" data-path="sample-design-and-weighting.html"><a href="sample-design-and-weighting.html#correcting-for-sample-design-in-models"><i class="fa fa-check"></i>Correcting for sample design in models</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="missing-values.html"><a href="missing-values.html"><i class="fa fa-check"></i>Missing Values</a><ul>
<li class="chapter" data-level="" data-path="missing-values.html"><a href="missing-values.html#identifying-valid-skips"><i class="fa fa-check"></i>Identifying Valid Skips</a></li>
<li class="chapter" data-level="" data-path="missing-values.html"><a href="missing-values.html#kinds-of-missingness"><i class="fa fa-check"></i>Kinds of Missingness</a></li>
<li class="chapter" data-level="" data-path="missing-values.html"><a href="missing-values.html#removing-cases"><i class="fa fa-check"></i>Removing Cases</a></li>
<li class="chapter" data-level="" data-path="missing-values.html"><a href="missing-values.html#imputation"><i class="fa fa-check"></i>Imputation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="multicollinearity-and-scale-creation.html"><a href="multicollinearity-and-scale-creation.html"><i class="fa fa-check"></i>Multicollinearity and Scale Creation</a><ul>
<li class="chapter" data-level="" data-path="multicollinearity-and-scale-creation.html"><a href="multicollinearity-and-scale-creation.html#avoid-the-singularity"><i class="fa fa-check"></i>Avoid the Singularity</a></li>
<li class="chapter" data-level="" data-path="multicollinearity-and-scale-creation.html"><a href="multicollinearity-and-scale-creation.html#detecting-data-based-multicollinearity"><i class="fa fa-check"></i>Detecting Data-Based Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="multicollinearity-and-scale-creation.html"><a href="multicollinearity-and-scale-creation.html#addressing-multicollinearity"><i class="fa fa-check"></i>Addressing Multicollinearity</a></li>
<li class="chapter" data-level="" data-path="multicollinearity-and-scale-creation.html"><a href="multicollinearity-and-scale-creation.html#creating-scales"><i class="fa fa-check"></i>Creating Scales</a></li>
<li class="chapter" data-level="" data-path="multicollinearity-and-scale-creation.html"><a href="multicollinearity-and-scale-creation.html#factor-analysis"><i class="fa fa-check"></i>Factor Analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i>Model Selection</a><ul>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#there-is-no-right-model"><i class="fa fa-check"></i>There is no “right” model</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#the-accuracy-vs.-parsimony-tradeoff"><i class="fa fa-check"></i>The accuracy vs. parsimony tradeoff</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#null-vs.-saturated-model"><i class="fa fa-check"></i>Null vs. saturated model</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#a-not-very-useful-tool-the-f-test"><i class="fa fa-check"></i>A not-very-useful tool: the F-test</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#tools-with-a-parsimony-penalty"><i class="fa fa-check"></i>Tools with a parsimony penalty</a></li>
<li class="chapter" data-level="" data-path="model-selection.html"><a href="model-selection.html#model-averaging"><i class="fa fa-check"></i>Model Averaging</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="modeling-categorical-outcomes.html"><a href="modeling-categorical-outcomes.html"><i class="fa fa-check"></i>Modeling Categorical Outcomes</a><ul>
<li class="chapter" data-level="" data-path="dichotomous-outcomes-and-the-binomial-distribution.html"><a href="dichotomous-outcomes-and-the-binomial-distribution.html"><i class="fa fa-check"></i>Dichotomous Outcomes and The Binomial Distribution</a><ul>
<li class="chapter" data-level="" data-path="dichotomous-outcomes-and-the-binomial-distribution.html"><a href="dichotomous-outcomes-and-the-binomial-distribution.html#the-binomial-distribution"><i class="fa fa-check"></i>The binomial distribution</a></li>
<li class="chapter" data-level="" data-path="dichotomous-outcomes-and-the-binomial-distribution.html"><a href="dichotomous-outcomes-and-the-binomial-distribution.html#the-binomial-distribution-as-a-data-generating-process"><i class="fa fa-check"></i>The binomial distribution as a data-generating process</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="linear-probability-model.html"><a href="linear-probability-model.html"><i class="fa fa-check"></i>Linear Probability Model</a><ul>
<li class="chapter" data-level="" data-path="linear-probability-model.html"><a href="linear-probability-model.html#heteroscedasticity-1"><i class="fa fa-check"></i>Heteroscedasticity</a></li>
<li class="chapter" data-level="" data-path="linear-probability-model.html"><a href="linear-probability-model.html#nonsense-values"><i class="fa fa-check"></i>Nonsense values</a></li>
<li class="chapter" data-level="" data-path="linear-probability-model.html"><a href="linear-probability-model.html#logit-transformation-to-the-rescue"><i class="fa fa-check"></i>Logit transformation to the rescue</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html"><i class="fa fa-check"></i>Generalized Linear Model</a><ul>
<li class="chapter" data-level="" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html#generalized-linear-model-framework"><i class="fa fa-check"></i>Generalized linear model framework</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html#a-glm-for-dichotomous-outcomes"><i class="fa fa-check"></i>A GLM for dichotomous outcomes</a></li>
<li class="chapter" data-level="" data-path="generalized-linear-model.html"><a href="generalized-linear-model.html#maximum-likelihood-estimation"><i class="fa fa-check"></i>Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="logit-model.html"><a href="logit-model.html"><i class="fa fa-check"></i>Logit Model</a><ul>
<li class="chapter" data-level="" data-path="logit-model.html"><a href="logit-model.html#interpreting-results"><i class="fa fa-check"></i>Interpreting results</a></li>
<li class="chapter" data-level="" data-path="logit-model.html"><a href="logit-model.html#marginal-effects-1"><i class="fa fa-check"></i>Marginal effects</a></li>
<li class="chapter" data-level="" data-path="logit-model.html"><a href="logit-model.html#the-probit-link"><i class="fa fa-check"></i>The probit link</a></li>
<li class="chapter" data-level="" data-path="logit-model.html"><a href="logit-model.html#assessing-model-fit"><i class="fa fa-check"></i>Assessing model fit</a></li>
<li class="chapter" data-level="" data-path="logit-model.html"><a href="logit-model.html#separation"><i class="fa fa-check"></i>Separation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="models-for-polytomous-outcomes.html"><a href="models-for-polytomous-outcomes.html"><i class="fa fa-check"></i>Models for Polytomous Outcomes</a></li>
</ul></li>
<li class="appendix"><span><b>Appendices</b></span></li>
<li class="chapter" data-level="" data-path="useful-references.html"><a href="useful-references.html"><i class="fa fa-check"></i>Useful References</a><ul>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html"><i class="fa fa-check"></i>Example Datasets</a><ul>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html#crimes"><i class="fa fa-check"></i>Crimes</a></li>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html#movies"><i class="fa fa-check"></i>Movies</a></li>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html#politics"><i class="fa fa-check"></i>Politics</a></li>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html#popularity"><i class="fa fa-check"></i>Popularity</a></li>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html#sex"><i class="fa fa-check"></i>Sex</a></li>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html#titanic"><i class="fa fa-check"></i>Titanic</a></li>
<li class="chapter" data-level="" data-path="example-datasets.html"><a href="example-datasets.html#wages"><i class="fa fa-check"></i>Wages</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="common-r-commands.html"><a href="common-r-commands.html"><i class="fa fa-check"></i>Common R Commands</a><ul>
<li class="chapter" data-level="" data-path="common-r-commands.html"><a href="common-r-commands.html#univariate-statistics"><i class="fa fa-check"></i>Univariate Statistics</a></li>
<li class="chapter" data-level="" data-path="common-r-commands.html"><a href="common-r-commands.html#bivariate-statistics"><i class="fa fa-check"></i>Bivariate Statistics</a></li>
<li class="chapter" data-level="" data-path="common-r-commands.html"><a href="common-r-commands.html#statistical-inference-1"><i class="fa fa-check"></i>Statistical Inference</a></li>
<li class="chapter" data-level="" data-path="common-r-commands.html"><a href="common-r-commands.html#ols-regression-models"><i class="fa fa-check"></i>OLS Regression Models</a></li>
<li class="chapter" data-level="" data-path="common-r-commands.html"><a href="common-r-commands.html#utility-functions"><i class="fa fa-check"></i>Utility functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="plotting-cookbook.html"><a href="plotting-cookbook.html"><i class="fa fa-check"></i>Plotting Cookbook</a><ul>
<li class="chapter" data-level="" data-path="plotting-cookbook.html"><a href="plotting-cookbook.html#barplots"><i class="fa fa-check"></i>Barplots</a></li>
<li class="chapter" data-level="" data-path="plotting-cookbook.html"><a href="plotting-cookbook.html#histograms"><i class="fa fa-check"></i>Histograms</a></li>
<li class="chapter" data-level="" data-path="plotting-cookbook.html"><a href="plotting-cookbook.html#boxplots-1"><i class="fa fa-check"></i>Boxplots</a></li>
<li class="chapter" data-level="" data-path="plotting-cookbook.html"><a href="plotting-cookbook.html#comparative-barplots"><i class="fa fa-check"></i>Comparative Barplots</a></li>
<li class="chapter" data-level="" data-path="plotting-cookbook.html"><a href="plotting-cookbook.html#comparative-boxplots"><i class="fa fa-check"></i>Comparative Boxplots</a></li>
<li class="chapter" data-level="" data-path="plotting-cookbook.html"><a href="plotting-cookbook.html#scatterplots"><i class="fa fa-check"></i>Scatterplots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="r-stat-lab.html"><a href="r-stat-lab.html"><i class="fa fa-check"></i>R Stat Lab</a><ul>
<li class="chapter" data-level="" data-path="using-scripts.html"><a href="using-scripts.html"><i class="fa fa-check"></i>Using Scripts</a><ul>
<li class="chapter" data-level="" data-path="using-scripts.html"><a href="using-scripts.html#getting-started-with-scripts"><i class="fa fa-check"></i>Getting Started with Scripts</a></li>
<li class="chapter" data-level="" data-path="using-scripts.html"><a href="using-scripts.html#not-everything-goes-into-your-script"><i class="fa fa-check"></i>Not Everything Goes Into Your Script</a></li>
<li class="chapter" data-level="" data-path="using-scripts.html"><a href="using-scripts.html#commenting-for-sanity"><i class="fa fa-check"></i>Commenting for Sanity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html"><i class="fa fa-check"></i>Object Types</a><ul>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html#atomic-modes"><i class="fa fa-check"></i>Atomic Modes</a></li>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html#vectors-and-matrices"><i class="fa fa-check"></i>Vectors and Matrices</a></li>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html#factors"><i class="fa fa-check"></i>Factors</a></li>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html#logical-values-and-boolean-statements"><i class="fa fa-check"></i>Logical Values and Boolean Statements</a></li>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html#missing-values-1"><i class="fa fa-check"></i>Missing Values</a></li>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html#lists"><i class="fa fa-check"></i>Lists</a></li>
<li class="chapter" data-level="" data-path="object-types.html"><a href="object-types.html#data-frames"><i class="fa fa-check"></i>Data Frames</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pretty-pictures.html"><a href="pretty-pictures.html"><i class="fa fa-check"></i>Pretty Pictures</a><ul>
<li class="chapter" data-level="" data-path="pretty-pictures.html"><a href="pretty-pictures.html#base-plot"><i class="fa fa-check"></i>Base Plot</a></li>
<li class="chapter" data-level="" data-path="pretty-pictures.html"><a href="pretty-pictures.html#ggplot"><i class="fa fa-check"></i>ggplot</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="using-git.html"><a href="using-git.html"><i class="fa fa-check"></i>Using Git</a><ul>
<li class="chapter" data-level="" data-path="using-git.html"><a href="using-git.html#plain-text-is-better"><i class="fa fa-check"></i>Plain Text is Better</a></li>
<li class="chapter" data-level="" data-path="using-git.html"><a href="using-git.html#git"><i class="fa fa-check"></i>Git</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html"><i class="fa fa-check"></i>Reading and Writing Data</a><ul>
<li class="chapter" data-level="" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html#data-formats"><i class="fa fa-check"></i>Data Formats</a></li>
<li class="chapter" data-level="" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html#plain-text-files"><i class="fa fa-check"></i>Plain text files</a></li>
<li class="chapter" data-level="" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html#data-in-binary-format"><i class="fa fa-check"></i>Data in binary format</a></li>
<li class="chapter" data-level="" data-path="reading-and-writing-data.html"><a href="reading-and-writing-data.html#saving-data"><i class="fa fa-check"></i>Saving data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="cleaning-data.html"><a href="cleaning-data.html"><i class="fa fa-check"></i>Cleaning Data</a><ul>
<li><a href="cleaning-data.html#the-most-important-rule-check-yourself-before-you-wreck-yourself">The Most Important Rule: <span>Check yourself before you wreck yourself</span></a></li>
<li class="chapter" data-level="" data-path="cleaning-data.html"><a href="cleaning-data.html#assigning-missing-values"><i class="fa fa-check"></i>Assigning missing values</a></li>
<li class="chapter" data-level="" data-path="cleaning-data.html"><a href="cleaning-data.html#recoding"><i class="fa fa-check"></i>Recoding</a></li>
<li class="chapter" data-level="" data-path="cleaning-data.html"><a href="cleaning-data.html#collapsing-categorical-variables"><i class="fa fa-check"></i>Collapsing Categorical Variables</a></li>
<li class="chapter" data-level="" data-path="cleaning-data.html"><a href="cleaning-data.html#transforming-quantitative-variables"><i class="fa fa-check"></i>Transforming Quantitative Variables</a></li>
<li class="chapter" data-level="" data-path="cleaning-data.html"><a href="cleaning-data.html#after-cleaning-you-still-need-to-tidy"><i class="fa fa-check"></i>After Cleaning You Still Need to Tidy</a></li>
<li class="chapter" data-level="" data-path="cleaning-data.html"><a href="cleaning-data.html#aggregating-data"><i class="fa fa-check"></i>Aggregating Data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="reshaping-and-merging-data.html"><a href="reshaping-and-merging-data.html"><i class="fa fa-check"></i>Reshaping and Merging Data</a><ul>
<li class="chapter" data-level="" data-path="reshaping-and-merging-data.html"><a href="reshaping-and-merging-data.html#reshaping"><i class="fa fa-check"></i>Reshaping</a></li>
<li class="chapter" data-level="" data-path="reshaping-and-merging-data.html"><a href="reshaping-and-merging-data.html#merging-data"><i class="fa fa-check"></i>Merging data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html"><i class="fa fa-check"></i>Programming</a><ul>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#an-example-theils-h"><i class="fa fa-check"></i>An Example: Theil’s H</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#our-data"><i class="fa fa-check"></i>Our Data</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#calculating-theils-h-for-a-single-state"><i class="fa fa-check"></i>Calculating Theil’s H for a single state</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#creating-functions"><i class="fa fa-check"></i>Creating Functions</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#iteration"><i class="fa fa-check"></i>Iteration</a></li>
<li class="chapter" data-level="" data-path="programming.html"><a href="programming.html#putting-it-all-together"><i class="fa fa-check"></i>Putting It All Together</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="using-r-markdown.html"><a href="using-r-markdown.html"><i class="fa fa-check"></i>Using R Markdown</a><ul>
<li class="chapter" data-level="" data-path="using-r-markdown.html"><a href="using-r-markdown.html#plain-text-science"><i class="fa fa-check"></i>Plain Text Science</a></li>
<li class="chapter" data-level="" data-path="using-r-markdown.html"><a href="using-r-markdown.html#markdown-syntax"><i class="fa fa-check"></i>Markdown Syntax</a></li>
<li class="chapter" data-level="" data-path="using-r-markdown.html"><a href="using-r-markdown.html#r-markdown"><i class="fa fa-check"></i>R Markdown</a></li>
<li class="chapter" data-level="" data-path="using-r-markdown.html"><a href="using-r-markdown.html#figures-in-r-markdown"><i class="fa fa-check"></i>Figures in R Markdown</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Statistical Analysis in Sociology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="generalized-linear-model" class="section level2">
<h2>Generalized Linear Model</h2>
<iframe src="https://www.youtube.com/embed/FbbHf4rdVgI" width="672" height="400px">
</iframe>
<p>Generalized linear models (GLM) will allow us to extend the basic idea of our linear model to incorporate more diverse outcomes and to specify more directly the <strong>data generating process</strong> behind our data.</p>
<p>To better understand what GLMs do, I want to return to a particular set-up of the linear model. In this set-up, there are two equations. The first equation partitions the value of an actual outcome <span class="math inline">\((y_i)\)</span> into the part accounted for by our model <span class="math inline">\((\hat{y}_i)\)</span> and the random residual “leftover” bits <span class="math inline">\((\epsilon_i)\)</span>:</p>
<p><span class="math display">\[y_i=\hat{y}_i+\epsilon_i\]</span></p>
<p>We then have a second equation that details how the structural model part <span class="math inline">\((\hat{y}_i)\)</span> is specified by a linear function of the independent variables:</p>
<p><span class="math display">\[\hat{y_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}\]</span></p>
<p>We should have a good sense of the second equation by now as its the basic linear model set-up. I want to focus more on the first equation right now. Its important to remember that the error terms <span class="math inline">\((\epsilon_i)\)</span> are randomly drawn from some distribution. Its not important what that distribution is so long as all of the <span class="math inline">\(\epsilon_i\)</span> are drawn from the same distribution independently. However, for the purposes of illustration lets assume that the <span class="math inline">\(\epsilon_i\)</span> are being drawn from a normal distribution. We can then describe that distribution mathematically as:</p>
<p><span class="math display">\[\epsilon_i \sim N(0, \sigma)\]</span></p>
<p>The <span class="math inline">\(\sim\)</span> sign means “distributed as.” In this case, our error terms are distributed as a normal distribution that is centered on zero and has some standard deviation <span class="math inline">\(\sigma\)</span>. It doesn’t really matter what that <span class="math inline">\(\sigma\)</span> is for our purposes here.</p>
<p>Think about this from a data-generating perspective. To get an actual value of <span class="math inline">\(y\)</span> for the <span class="math inline">\(i\)</span>th observation:</p>
<ol style="list-style-type: decimal">
<li>We feed all of that observation’s values for <span class="math inline">\(x\)</span> into our linear function which gives us a predicted value of <span class="math inline">\(y\)</span>, <span class="math inline">\(\hat{y}_i\)</span>. This is only the structural part. All observations with the same values of <span class="math inline">\(x\)</span> will get the same <span class="math inline">\(\hat{y}_i\)</span> because we have not added the random bit.</li>
<li>Reach into our normal distribution and pick out a residual that we add onto the end of our predicted value to get the actual value. This residual adjusts for all the random factors not accounted for in our model that might cause variation between observations with the exact same predicted value.</li>
</ol>
<p>One way of thinking about the second part is that instead of reaching into a normal distribution centered on zero for the residual, we are reaching into a normal distribution centered on <span class="math inline">\(\hat{y}_i\)</span> because all we are going to do is add the constant value of <span class="math inline">\(\hat{y}_i\)</span> to whatever we pull out of that distribution. Therefore, we can actually re-write the first equation parsing <span class="math inline">\(y\)</span> into the structural and stochastic components as:</p>
<p><span class="math display">\[y_i \sim N(\hat{y}_i, \sigma)\]</span></p>
<p>We can now have all the pieces to reformulate this linear model in the framework of the generalized linear model.</p>
<div id="generalized-linear-model-framework" class="section level3">
<h3>Generalized linear model framework</h3>
<p>The generalized linear model requires two components: the <strong>error distribution</strong> and the <strong>link function</strong>.</p>
<p>The <strong>error distribution</strong> specifies how the outcome that we actually measure in our data is distributed. In this case, the error distribution is given by:</p>
<p><span class="math display">\[y_i \sim N(\hat{y}_i, \sigma)\]</span></p>
<p>Therefore, the error distribution is normal. The normal distribution is also sometimes called the **gaussian* distribution and that is how we will specify it in <em>R</em> below.</p>
<p>The <strong>link function</strong> specifies how the linear function of the independent variables is related to the key parameter of the error distribution. In this case the key parameter of the error distribution is <span class="math inline">\(\hat{y}_i\)</span> and our linear function is related directly to that parameter:</p>
<p><span class="math display">\[\hat{y_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}\]</span></p>
<p>In this case, the link function is somewhat invisible because there is not really a link function at all. We are simply relating the linear function of the independent variables directly to the key parameter of the error distribution. In practice this si called the <em>identity</em> link.</p>
<p>So within the generalized linear model framework, we can express our traditional linear model as using a gaussian error distribution and an identity link. Lets go ahead and try that out. The command <code>glm</code> in <em>R</em> will estimate a generalized linear model. We will talk later in this section about how that estimation works, but for now I just want to focus on the results.</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb519-1" title="1"><span class="kw">summary</span>(<span class="kw">glm</span>(indegree<span class="op">~</span>nsports, <span class="dt">data=</span>popularity,</a>
<a class="sourceLine" id="cb519-2" title="2">            <span class="dt">family=</span><span class="kw">gaussian</span>(identity)))<span class="op">$</span>coef</a></code></pre></div>
<pre><code>##              Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 3.9969128 0.07221383 55.34830 0.000000e+00
## nsports     0.5044705 0.04282271 11.78044 1.462129e-31</code></pre>
<p>To specify the error distribution and link function, I used the <code>family</code> argument in the <code>glm</code> command. Lets compare this result to the traditional <code>lm</code> command that is estimated via OLS regression:</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb521-1" title="1"><span class="kw">summary</span>(<span class="kw">lm</span>(indegree<span class="op">~</span>nsports, <span class="dt">data=</span>popularity))<span class="op">$</span>coef</a></code></pre></div>
<pre><code>##              Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 3.9969128 0.07221383 55.34830 0.000000e+00
## nsports     0.5044705 0.04282271 11.78044 1.462129e-31</code></pre>
<p>The results are identical. Keep in mind that these two commands used completely different estimation strategies. We know that OLS regression minimizes the sum of squared residuals. The GLM uses a technique called <strong>maximum likelihood estimation</strong> that we will learn about later in this section. However, the key point is that they both produced the same estimate. When we specify the data-generating process as a guassian error distribution with an identity link, we are estimating a traditional linear model.</p>
<p>At this point, this new framework and estimation approach hasn’t really done much for us. The <code>lm</code> command will work fine and we also don’t need to make the assumption that our residual terms are normally distributed for OLS regression models to be valid. So why set up this more complex framework? The answer is that by changin the error distribution and link function, we can accomodate a broad set of models that are cannot be estimated well by OLS regression techniques. Most of those models involve categorical variables. The most common model is the <strong>logit model</strong> (also called the logistic regression model) that can be used for dichotomous outcomes.</p>
</div>
<div id="a-glm-for-dichotomous-outcomes" class="section level3">
<h3>A GLM for dichotomous outcomes</h3>
<iframe src="https://www.youtube.com/embed/2uClzwRGMuk" width="672" height="400px">
</iframe>
<p>In order to help us understand the logit model better, I want to start this in reverse. We will play god and actually generate the data. Then we can use the model to help recover the process we used to generate the data.</p>
<p>For this example, I want to stick with the theme of ocean liner disasters, but I want to create my own disaster. To do that, we are going to use the fictional example of the <em>Good Ship Lollipop</em>. The <em>Good Ship Lollipop</em> has a large number of 10,000 passengers. We know two things about these passengers: their gender and the amount they paid in fare. To create the passengers of the <em>Good Ship Lollipop</em> I am going to use some handy functions in R for producing random outcomes:</p>
<div class="sourceCode" id="cb523"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb523-1" title="1">good_ship &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">gender=</span><span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Male&quot;</span>,<span class="st">&quot;Female&quot;</span>),<span class="dv">10000</span>,<span class="dt">replace=</span><span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb523-2" title="2">                        <span class="dt">fare=</span><span class="kw">rgamma</span>(<span class="dv">10000</span>, <span class="dv">1</span>,<span class="fl">0.01</span>))</a>
<a class="sourceLine" id="cb523-3" title="3"><span class="kw">summary</span>(good_ship)</a></code></pre></div>
<pre><code>##     gender          fare         
##  Female:5057   Min.   :  0.0031  
##  Male  :4943   1st Qu.: 28.4345  
##                Median : 69.1671  
##                Mean   : 99.8009  
##                3rd Qu.:137.6850  
##                Max.   :872.0241</code></pre>
<p>The data looks pretty reasonable. Unfortunately, the <em>Good Ship Lollipop</em> is going to hit an iceberg and sink on its first voyage because the crew were too busy singing and drinking spiked Shirley Temples to keep an eye out. Some passengers will survive this sinking and some will not. Since I am playing god, the first thing I need to do is figure out the underlying <span class="math inline">\(p_i\)</span> probability of survival for each passenger.</p>
<p>I want <span class="math inline">\(p_i\)</span> to be a function of gender and fiare paid. However, I know that its not safe or sensible to make it a direct linear function because I am end up with nonsensical values of <span class="math inline">\(p_i\)</span>. If I instead make the log-odds (or logit) of survival be a linear function of gender and fare paid, then I will be assured that when the log-odds are converted to probabilities, all the probabilities will fall between zero and one. So I set up my model:</p>
<p><span class="math display">\[log(\frac{p_i}{1-p_i})=0.05-0.40(male_i)+0.005(fare_i)\]</span></p>
<p>The numbers I put in here aren’t particular important and I could vary them if I wanted to change how they related to survival. Right now, I am saying that men were less likely to survive and fare was positively associated with survival. The baseline log-odds of survival for a man who paid no fare is 0.05 which works out to a probability of 0.512 or 51.2%.</p>
<p>Lets go ahead and feed this equation into my data to get predicted log-odds. We can then convert from those log-odds to probabilities:</p>
<div class="sourceCode" id="cb525"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb525-1" title="1">good_ship<span class="op">$</span>log_odds &lt;-<span class="st"> </span><span class="fl">0.05-0.4</span><span class="op">*</span>(good_ship<span class="op">$</span>gender<span class="op">==</span><span class="st">&quot;Male&quot;</span>)<span class="op">+</span><span class="fl">0.005</span><span class="op">*</span>good_ship<span class="op">$</span>fare</a>
<a class="sourceLine" id="cb525-2" title="2">good_ship<span class="op">$</span>odds &lt;-<span class="st"> </span><span class="kw">exp</span>(good_ship<span class="op">$</span>log_odds)</a>
<a class="sourceLine" id="cb525-3" title="3">good_ship<span class="op">$</span>probs &lt;-<span class="st"> </span>good_ship<span class="op">$</span>odds<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>good_ship<span class="op">$</span>odds)</a></code></pre></div>
<p>The <code>probs</code> vector gives us the probability of survival for every passenger. In order to complete this process I now need to have every passenger make their Bernoulli trial to see if they survive the disaster. I can do this easily in <em>R</em> by using the <code>rbinom</code> function:</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb526-1" title="1">good_ship<span class="op">$</span>survived &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">10000</span>, <span class="dv">1</span>, good_ship<span class="op">$</span>probs)</a></code></pre></div>
<p>Lets use <code>ggplot</code> to visualize how this all played out:</p>
<div class="sourceCode" id="cb527"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb527-1" title="1"><span class="kw">ggplot</span>(good_ship, <span class="kw">aes</span>(<span class="dt">x=</span>fare, <span class="dt">color=</span>gender))<span class="op">+</span></a>
<a class="sourceLine" id="cb527-2" title="2"><span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y=</span>survived), <span class="dt">alpha=</span><span class="fl">0.2</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb527-3" title="3"><span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y=</span>probs))<span class="op">+</span></a>
<a class="sourceLine" id="cb527-4" title="4"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;fare paid&quot;</span>, <span class="dt">y=</span><span class="st">&quot;probability of survival&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb527-5" title="5"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:gsl-visualize"></span>
<img src="stat_book_files/figure-html/gsl-visualize-1.png" alt="Life and death on the Good Ship Lollipop. The lines show the underlying probabilities of survival by gender and survival. The dots show actual outcomes after drawing a bernoulli trial for each passenger." width="672" />
<p class="caption">
Figure 98: Life and death on the Good Ship Lollipop. The lines show the underlying probabilities of survival by gender and survival. The dots show actual outcomes after drawing a bernoulli trial for each passenger.
</p>
</div>
<p>We can see that women were more likely to survive. This difference in survival shrank as fare paid increased because both groups started to approach the 100% probability threshold. We can also see from the dots that more women ended up in the survivor group as we would expect and that people were more likely to survive at higher fares paid. What we are seeing in Figure <a href="generalized-linear-model.html#fig:gsl-visualize">98</a> is the data-generating process. The lines give us the underlying probabilities and the dots show us the realization of those probabilities into the ones and zeros we actually would have in the data.</p>
<p>Can we reverse this data-generating process to recover the values I used to construct the probabilities? We can do so using a GLM approach. In this case we know the error distribution and link function.</p>
<p>The error distribution tells us how our dependent variable is distributed. In this case, we either have a 1 (survived) or a 0 (died). Each of these values was produced by a binomial distribution with <span class="math inline">\(n=1\)</span> and a probability equal to <span class="math inline">\(p_i\)</span>. So:</p>
<p><span class="math display">\[y_i \sim binom(1, p_i)\]</span></p>
<p>The key parameter in this error distribution is <span class="math inline">\(p_i\)</span>. The link function should tell us how we relate <span class="math inline">\(p_i\)</span> to the linear function of the independent variables of gender and fare. In this case, the relationship is not direct. We related the linear function of the independent variables to the log-odds or logit of the probability of survival:</p>
<p><span class="math display">\[log(\frac{p_i}{1-p_i})=\beta_0+\beta_1(male_i)+\beta_2(fare_i)\]</span></p>
<p>So to run this model we need to specify a <strong>binomial</strong> error distribution and a <strong>logit</strong> link function in a <code>glm</code> framework. Lets try it out:</p>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb528-1" title="1">model.glm &lt;-<span class="st"> </span><span class="kw">glm</span>(survived<span class="op">~</span>gender<span class="op">+</span>fare, <span class="dt">data=</span>good_ship, </a>
<a class="sourceLine" id="cb528-2" title="2">                  <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span>logit))</a>
<a class="sourceLine" id="cb528-3" title="3"><span class="kw">summary</span>(model.glm)<span class="op">$</span>coef</a></code></pre></div>
<pre><code>##                 Estimate   Std. Error   z value     Pr(&gt;|z|)
## (Intercept)  0.043243079 0.0366186671  1.180903 2.376414e-01
## genderMale  -0.405344191 0.0418497872 -9.685693 3.468482e-22
## fare         0.005410885 0.0002589993 20.891509 6.396671e-97</code></pre>
<p>It worked! Note that my coefficient estimates are very close to the actual values I used when playing god. They differ slightly because there is some inherent randomness in choosing survivors and deaths by the binomial distribution.</p>
<p>We now have a framework for a model that predicts dichotomous outcomes. This is called the <strong>logit model</strong> or <strong>logistic regression model</strong>. In practice, it is one specification of the generalized linear model.</p>
<p>However, we still need to understand two things. First, how are those values actually estimated in the generalized linear model? We will take that up below. Second, how do we interpret those results? We will take that up with a more thorough discussion of the logit model in the next section.</p>
</div>
<div id="maximum-likelihood-estimation" class="section level3">
<h3>Maximum likelihood estimation</h3>
<iframe src="https://www.youtube.com/embed/FpAMN6tHvLA" width="672" height="400px">
</iframe>
<p>The technique that the generalized linear model uses to estimate parameters (i.e. intercept and slopes) is called maximum likelihood estimation. This technique is different than the technique we learned for OLS regression model which sought to minimize the sum of squared residuals, although it turns out that OLS regression technique is the maximum likelihood estimate for a simple linear model. However, we now have a more complex model that does not even have residuals in the sense that we have been using up to this point.</p>
<p>The basic principle of maximum likelihood estimation (MLE) is fairly straightforward: <strong>We choose the parameters that maximize the likelihood of observing the data that we actually have</strong>. More formally, the process of maximum likelihood estimation operates as follows:</p>
<ol style="list-style-type: decimal">
<li>We have some data-generating process that produces a set of observed outcomes. In the case of the Titanic of the Good Ship Lollipop, these outcomes are the vectors of zero and one that indicate death or survival. This data-generating process is governed by some unknown parameters. In our case, those unknown parameters are the intercept and slopes of the linear function in our link function. In general, we call the set of unknown parameters <span class="math inline">\(\theta\)</span> as a shorthand.</li>
<li>Based on the data-generating process, we can construct a <strong>likelihood function</strong>, <span class="math inline">\(L(\theta)\)</span> that determines the probability of observing the data that we actually observe, assuming some <span class="math inline">\(\theta\)</span>. The important thing to note about the likelihood function is that its a function of the unknown parameters <span class="math inline">\(\theta\)</span>, not the actual data, which are known.</li>
<li>We determine what values of <span class="math inline">\(\theta\)</span> would produce the maximum possible value in the likelihood function <span class="math inline">\(L(\theta)\)</span>. These are the maximum likelihood estimates. As usual to find maximums of functions, we need to use calculus. In practice, we typically log the likelihood function to get the log-likelihood function and find the maximum for that function which will give us the same result. We prefer the log-likelihood function because it makes multiplicative functions additive, which simplifies the math.</li>
</ol>
<p>This is all very abstract, so lets take a simple example. Lets say I flip a coin 50 times and observe 20 heads. What is my maximum likelihood estimate of <span class="math inline">\(p\)</span>, the probability that we get a head on a single coin toss?</p>
<p>Now, you may be saying to yourself, isn’t the probability of a head on a coin toss 50%? It is true that we have strong prior beliefs that this should be so. However, the principle of maximum likelihood estimation is to produce estimates purely driven by the data at hand, not prior beliefs. So we are going to act naively and estimate <span class="math inline">\(p\)</span> via MLE.</p>
<p>What is our data-generating process. We know that this is a basic binomial process. However, we are making some changes here from how we usually conceptualize this binomial process. Before, we have always known <span class="math inline">\(p\)</span> and <span class="math inline">\(n\)</span> (number of trials) and then considered the probability of getting <span class="math inline">\(k\)</span> successes. Now, we know <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> and want to figure out <span class="math inline">\(p\)</span>. The good news is that our likelihood function is familiar:</p>
<p><span class="math display">\[L(p)={50 \choose 20}p^{20}(1-p)^{30}\]</span></p>
<p>The likelihood function is just the binomial formula. Except now it is a function of <span class="math inline">\(p\)</span> with the values for <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span> filled in. Remember that a likelihood function gives us the probability of getting the data that we actually have. This function will give us the probability of getting 20 heads in 50 coin tosses for a given value of <span class="math inline">\(p\)</span>. We just need to figure out which value of <span class="math inline">\(p\)</span> will actually give us the maximum possible value for <span class="math inline">\(L(p)\)</span>. We can do this by brute force by just using <em>R</em> to try every possible value of <span class="math inline">\(p\)</span> from 0.001 to 0.999:</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb530-1" title="1">p &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span>.<span class="dv">001</span>, <span class="dt">to=</span>.<span class="dv">999</span>, <span class="dt">by=</span>.<span class="dv">001</span>)</a>
<a class="sourceLine" id="cb530-2" title="2">likelihood &lt;-<span class="st"> </span><span class="kw">choose</span>(<span class="dv">50</span>,<span class="dv">20</span>)<span class="op">*</span>p<span class="op">^</span><span class="dv">20</span><span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)<span class="op">^</span><span class="dv">30</span></a>
<a class="sourceLine" id="cb530-3" title="3"><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(p, likelihood), <span class="kw">aes</span>(<span class="dt">x=</span>p, <span class="dt">y=</span>likelihood))<span class="op">+</span></a>
<a class="sourceLine" id="cb530-4" title="4"><span class="st">  </span><span class="kw">geom_line</span>()<span class="op">+</span></a>
<a class="sourceLine" id="cb530-5" title="5"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&quot;L(p)&quot;</span>)<span class="op">+</span></a>
<a class="sourceLine" id="cb530-6" title="6"><span class="st">  </span><span class="kw">theme_bw</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:mle-binom"></span>
<img src="stat_book_files/figure-html/mle-binom-1.png" alt="Likelihood function for p from binomial process with 50 trials and 20 successes. It is clearly maximized at p=0.4." width="672" />
<p class="caption">
Figure 99: Likelihood function for p from binomial process with 50 trials and 20 successes. It is clearly maximized at p=0.4.
</p>
</div>
<p>Figure <a href="generalized-linear-model.html#fig:mle-binom">99</a> shows the likelihood function for <span class="math inline">\(p\)</span> calculated by hand. It is clear that the reasonable range of values of <span class="math inline">\(p\)</span> are between about 0.25 and 0.55 and <span class="math inline">\(L(p)\)</span> is maximized at <span class="math inline">\(p=0.4\)</span>. This should not be surprising. If we have a binomial process with 50 trials and 20 successes, then our best guess for the probability of a success is <span class="math inline">\(20/50=0.4\)</span>.</p>
<p>Now lets try to derive the <span class="math inline">\(p\)</span> that maximized <span class="math inline">\(L(p)\)</span> more formally. To find the maximum of a function we need to take the derivative of that function and solve for zero<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. As I noted earlier, it is usually easier to take the derivative of a log-likelihood function, so lets first log our likelihood function:</p>
<p><span class="math display">\[
\begin{aligned}
\log L(p)&amp;=\log({50 \choose 20}p^{20}(1-p)^{30})\\
\log L(p)&amp;=\log {50 \choose 20} + 20 \log(p) + 30\log(1-p)\\
\end{aligned}
\]</span></p>
<p>Now, we just need to take the derivative of this function with respect to <span class="math inline">\(p\)</span>. If you know calculus, this is a mildly complex and enjoyable calculation that you can try out, but for others I will tell you mathemagically, the derivative comes out to:</p>
<p><span class="math display">\[\frac{\partial \log L(p)}{\partial p}=\frac{20}{p}-\frac{30}{1-p}\]</span></p>
<p>We now just need to set this equal to zero on the left-hand side and solve for <span class="math inline">\(p\)</span>. Thats just alegbra:</p>
<p><span class="math display">\[
\begin{aligned}
0&amp;=\frac{20}{p}-\frac{30}{1-p}\\
\frac{30}{1-p}&amp;=\frac{20}{p}\\
30p&amp;=20(1-p)\\
30p&amp;=20-20p\\
50p&amp;=20\\
p&amp;=20/50=0.4
\end{aligned}
\]</span></p>
<p>In general, if you replace 20 and 50 with <span class="math inline">\(n\)</span> and <span class="math inline">\(k\)</span>, then this result will give you the MLE solution of <span class="math inline">\(p=k/n\)</span>.</p>
<div id="mle-for-glms" class="section level4">
<h4>MLE for GLMs</h4>
<p>The previous example is fairly simple and we are able to get a closed-form solution that is satisfying and straightforward. Maximum likelihood estimation for GLMs is not so easy.</p>
<p>Lets consider our case for the logit model. Each observation gets a bernoulli trial to determine success where the outcome of 0 or 1 is recorded at <span class="math inline">\(y_i\)</span>. Therefore, the likelihood <span class="math inline">\(L_i\)</span> for a single observation is given by:</p>
<p><span class="math display">\[L_i=p_i^{y_i}(1-p_i)^{1-y_i}\]</span></p>
<p>This formula may look complicated, but its fairly straightforward. If the passenger survived <span class="math inline">\((y_i=1)\)</span>, then this formula just simplifies to <span class="math inline">\(p_i\)</span>,the probability of survival for that passenger. If the person did not survive <span class="math inline">\((y_i=0)\)</span>, then this formula just simplifies to <span class="math inline">\((1-p_i)\)</span>, the probabiility of not surviving for that passenger.</p>
<p>As usual, lets take the log of that likelihood to get the log-likelihood:</p>
<p><span class="math display">\[\log L_i=y_i\log(p_i)+(1-y_i) \log (1-p_i)\]</span></p>
<p>To get the log-likelihood of all observations, we just need to add up these log-likelihoods. Why do we add them? On the original scale of likelihoods, we would multiply them together, so on the log-scale we add them.</p>
<p><span class="math display">\[\log L= \sum_{i=1}^n \log L_i= \sum_{i=1}^n y_i\log(p_i)+(1-y_i) \log (1-p_i)\]</span></p>
<p>Now, the only problem we have at the moment is that our goal is to maximize the slopes and intercept of our linear model but we don’t have those in the formulat the moment. However, we can add that by remembering that we were predicting the log-odds of success as a linear function of the independent variables:</p>
<p><span class="math display">\[log(\frac{p_i}{1-p_i})=\mathbf{x_i&#39;\beta}\]</span></p>
<p>I am writing the linear function in matrix notation here <span class="math inline">\((\mathbf{x_i&#39;\beta})\)</span> for brevity. We can convert this back to <span class="math inline">\(p\)</span> by using the formula for converting from log-odds to probabilities:</p>
<p><span class="math display">\[{p}_i=\frac{e^{\mathbf{x_i&#39;\beta}}}{1+e^{\mathbf{x_i&#39;\beta}}}\]</span></p>
<p>We can then plug this into our log-likelihood function for <span class="math inline">\(p_i\)</span>:</p>
<p><span class="math display">\[\log L(\beta) = \sum_{i=1}^n y_i\log(\frac{e^{\mathbf{x_i&#39;\beta}}}{1+e^{\mathbf{x_i&#39;\beta}}})+(1-y_i) \log (1-\frac{e^{\mathbf{x_i&#39;\beta}}}{1+e^{\mathbf{x_i&#39;\beta}}})\]</span></p>
<p>We now have the full log-likelihood function for a logit model. To figure out <span class="math inline">\(\beta\)</span> we just have the rather difficult task of finding the <span class="math inline">\(\beta\)</span> values that maximize this function.</p>
<p>Finding the MLE for this log-likelihood function is no easy task and there are no closed-form solutions as there were for the simple binomial process case. We need to use iterative estimation procedures to estimate the best values for <span class="math inline">\(\beta\)</span>.</p>
<p>Several algorithms have been developed for finding MLE for this case and for other forms of the GLMs. The technique that I will show you below uses a version of iteratively-reweighted least squares (IRLS). You will never typically have to do this as the <code>glm</code> command will do all of the heavy-lifting here, but its worthwhile to have some familiarity with how <code>glm</code> is doing what it does. I will show you below how this procedure works using the case of the Titanic where we predict survival by fare paid. Lets first set up our outcome vector of zeros and ones:</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb531-1" title="1">y &lt;-<span class="st"> </span><span class="kw">as.vector</span>(<span class="kw">as.numeric</span>(titanic<span class="op">$</span>survival<span class="op">==</span><span class="st">&quot;Survived&quot;</span>))</a>
<a class="sourceLine" id="cb531-2" title="2"><span class="kw">head</span>(y)</a></code></pre></div>
<pre><code>## [1] 1 1 0 0 0 1</code></pre>
<p>Beause we will also be using matrix notation, I am also going to set up my design matrix of independent variables, taking consideration to add a column of ones for my intercept.</p>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb533-1" title="1">X &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(<span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(titanic)), titanic[,<span class="st">&quot;fare&quot;</span>]))</a>
<a class="sourceLine" id="cb533-2" title="2"><span class="kw">head</span>(X)</a></code></pre></div>
<pre><code>##      [,1]     [,2]
## [1,]    1 211.3375
## [2,]    1 151.5500
## [3,]    1 151.5500
## [4,]    1 151.5500
## [5,]    1 151.5500
## [6,]    1  26.5500</code></pre>
<p>The basic IRLS procedure is to estimate via an iterative procedure. To get the process started we first fit the null model as our logit model. The null model just assumes that every observation had an equal probability of survival.</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb535-1" title="1"><span class="kw">mean</span>(y)</a></code></pre></div>
<pre><code>## [1] 0.381971</code></pre>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb537-1" title="1"><span class="co">#convert to log-odds</span></a>
<a class="sourceLine" id="cb537-2" title="2">lodds &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="kw">mean</span>(y)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">mean</span>(y)))</a>
<a class="sourceLine" id="cb537-3" title="3">lodds</a></code></pre></div>
<pre><code>## [1] -0.4811908</code></pre>
<p>Based on the proportion of survivors, the log-odds of survival as a whole was -0.4811908. So my null model is:</p>
<p><span class="math display">\[\log(\frac{p_i}{1-p_i})=-0.4811908\]</span></p>
<p>Lets set this up as my initial <span class="math inline">\(\beta\)</span> vector:</p>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb539-1" title="1">beta &lt;-<span class="st"> </span><span class="kw">c</span>(lodds, <span class="dv">0</span>)</a></code></pre></div>
<p>I can matrix-multiply this vector by my design matrix to get predicted log-odds which I can convert to <span class="math inline">\(p\)</span>. At this point, they all should give me the same value.</p>
<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb540-1" title="1">pred_lodds &lt;-<span class="st"> </span>X<span class="op">%*%</span>beta</a>
<a class="sourceLine" id="cb540-2" title="2">p &lt;-<span class="st"> </span><span class="kw">exp</span>(pred_lodds)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(pred_lodds))</a>
<a class="sourceLine" id="cb540-3" title="3"><span class="kw">head</span>(p)</a></code></pre></div>
<pre><code>##          [,1]
## [1,] 0.381971
## [2,] 0.381971
## [3,] 0.381971
## [4,] 0.381971
## [5,] 0.381971
## [6,] 0.381971</code></pre>
<p>I can also use that <code>p</code> vector to calculate my log-likelihood for the current model:</p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb542-1" title="1">logL &lt;-<span class="st"> </span><span class="kw">sum</span>(y<span class="op">*</span><span class="kw">log</span>(p)<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p))</a>
<a class="sourceLine" id="cb542-2" title="2">logL</a></code></pre></div>
<pre><code>## [1] -870.5122</code></pre>
<p>Now I want to iterate this process and improve my estimation. At each iteration <span class="math inline">\(t\)</span> of the process, I estimate the best-fitting <span class="math inline">\(\beta\)</span> vector for <span class="math inline">\(t+1\)</span> by the following formula:</p>
<p><span class="math display">\[\beta^{(t+1)}=\mathbf{(X&#39;W^{(t)}X)^{-1}X&#39;W^{(t)}z^{(t)}}\]</span></p>
<p>This formula is somewhat similar to the OLS regression formula we saw in the previous module. We can see the design matrix <span class="math inline">\(\mathbf{X}\)</span> of independent variables. We also have a <span class="math inline">\(\mathbf{X}\)</span> weighting matrix to consider. You will also notice a <span class="math inline">\(z\)</span> vector where we would usually have a <span class="math inline">\(y\)</span> vector.</p>
<p>The weighting matrix only has non-zero values along the diagonal. These values are given by:</p>
<p><span class="math display">\[w_i=\hat{p}_i(1-\hat{p}_i)\]</span></p>
<p>Where <span class="math inline">\(hat{p}_i\)</span> is estimated probability of success for observation <span class="math inline">\(i\)</span> from the current iteration of the model. I can estimate this weighting matrix from my initial model as follows:</p>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb544-1" title="1">W &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(X), <span class="kw">nrow</span>(X))</a>
<a class="sourceLine" id="cb544-2" title="2"><span class="kw">diag</span>(W) &lt;-<span class="st"> </span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)</a></code></pre></div>
<p>The <span class="math inline">\(z\)</span> vector accounts for the transformation of the dependent variable and is estimated as:</p>
<p><span class="math display">\[z_i=\mathbf{x_i&#39;\beta}+\frac{y_i-\hat{p}_i}{\hat{p}_i(1-\hat{p}_i)}\]</span></p>
<p>I can calculate this from my null model as:</p>
<div class="sourceCode" id="cb545"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb545-1" title="1">z &lt;-<span class="st"> </span>pred_lodds <span class="op">+</span><span class="st"> </span>(y<span class="op">-</span>p)<span class="op">/</span>(p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p))</a></code></pre></div>
<p>I now have all the pieces for my first iteration:</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb546-1" title="1">beta &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>W<span class="op">%*%</span>X)<span class="op">%*%</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>W<span class="op">%*%</span>z)</a>
<a class="sourceLine" id="cb546-2" title="2">beta</a></code></pre></div>
<pre><code>##              [,1]
## [1,] -0.804907036
## [2,]  0.009728163</code></pre>
<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb548-1" title="1">pred_lodds &lt;-<span class="st"> </span>X<span class="op">%*%</span>beta</a>
<a class="sourceLine" id="cb548-2" title="2">p &lt;-<span class="st"> </span><span class="kw">exp</span>(pred_lodds)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(pred_lodds))</a>
<a class="sourceLine" id="cb548-3" title="3">logL &lt;-<span class="st"> </span><span class="kw">sum</span>(y<span class="op">*</span><span class="kw">log</span>(p)<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p))</a>
<a class="sourceLine" id="cb548-4" title="4">logL</a></code></pre></div>
<pre><code>## [1] -828.965</code></pre>
<p>You can see that I now have a non-zero slope for fare paid and a less negative log-likelihood which means a higher overall likelihood. I can keep iterating this procedure until my <span class="math inline">\(\beta\)</span> values stop moving around. Lets try a total of six iterations from the top:</p>
<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb550-1" title="1"><span class="co">#initial null model</span></a>
<a class="sourceLine" id="cb550-2" title="2">lodds &lt;-<span class="st"> </span><span class="kw">log</span>(<span class="kw">mean</span>(y)<span class="op">/</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">mean</span>(y)))</a>
<a class="sourceLine" id="cb550-3" title="3">beta &lt;-<span class="st"> </span><span class="kw">c</span>(lodds, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb550-4" title="4">pred_lodds &lt;-<span class="st"> </span>X<span class="op">%*%</span>beta</a>
<a class="sourceLine" id="cb550-5" title="5">p &lt;-<span class="st"> </span><span class="kw">exp</span>(pred_lodds)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(pred_lodds))</a>
<a class="sourceLine" id="cb550-6" title="6">logL &lt;-<span class="st"> </span><span class="kw">sum</span>(y<span class="op">*</span><span class="kw">log</span>(p)<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p))</a>
<a class="sourceLine" id="cb550-7" title="7">beta.prev &lt;-<span class="st"> </span>beta</a>
<a class="sourceLine" id="cb550-8" title="8"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">6</span>) {</a>
<a class="sourceLine" id="cb550-9" title="9">  w &lt;-<span class="st"> </span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p)</a>
<a class="sourceLine" id="cb550-10" title="10">  z &lt;-<span class="st"> </span>pred_lodds <span class="op">+</span><span class="st"> </span>(y<span class="op">-</span>p)<span class="op">/</span>w</a>
<a class="sourceLine" id="cb550-11" title="11">  W &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(X), <span class="kw">nrow</span>(X))</a>
<a class="sourceLine" id="cb550-12" title="12">  <span class="kw">diag</span>(W) &lt;-<span class="st"> </span>w</a>
<a class="sourceLine" id="cb550-13" title="13">  beta &lt;-<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>W<span class="op">%*%</span>X)<span class="op">%*%</span>(<span class="kw">t</span>(X)<span class="op">%*%</span>W<span class="op">%*%</span>z)</a>
<a class="sourceLine" id="cb550-14" title="14">  beta.prev &lt;-<span class="st"> </span><span class="kw">cbind</span>(beta.prev, beta)</a>
<a class="sourceLine" id="cb550-15" title="15">  pred_lodds &lt;-<span class="st"> </span>X<span class="op">%*%</span>beta</a>
<a class="sourceLine" id="cb550-16" title="16">  p &lt;-<span class="st"> </span><span class="kw">exp</span>(pred_lodds)<span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(pred_lodds))</a>
<a class="sourceLine" id="cb550-17" title="17">  logL &lt;-<span class="st"> </span><span class="kw">c</span>(logL, <span class="kw">sum</span>(y<span class="op">*</span><span class="kw">log</span>(p)<span class="op">+</span>(<span class="dv">1</span><span class="op">-</span>y)<span class="op">*</span><span class="kw">log</span>(<span class="dv">1</span><span class="op">-</span>p)))</a>
<a class="sourceLine" id="cb550-18" title="18">}</a>
<a class="sourceLine" id="cb550-19" title="19">beta.prev</a></code></pre></div>
<pre><code>##       beta.prev                                                             
## [1,] -0.4811908 -0.804907036 -0.87722472 -0.88394804 -0.88402353 -0.88402354
## [2,]  0.0000000  0.009728163  0.01218691  0.01246678  0.01247006  0.01247006
##                 
## [1,] -0.88402354
## [2,]  0.01247006</code></pre>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb552-1" title="1">logL</a></code></pre></div>
<pre><code>## [1] -870.5122 -828.9650 -827.4084 -827.3924 -827.3924 -827.3924 -827.3924</code></pre>
<p>By the sixth iteration, my intercept and slope values have stopped cahnging down to the eighth decimal place and the log-likelihood has also stabilized. Lets compare our results to the <code>glm</code> command to see how they compare:</p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb554-1" title="1">model.survival &lt;-<span class="st"> </span><span class="kw">glm</span>((survival<span class="op">==</span><span class="st">&quot;Survived&quot;</span>)<span class="op">~</span>fare, <span class="dt">data=</span>titanic, </a>
<a class="sourceLine" id="cb554-2" title="2">                      <span class="dt">family=</span><span class="kw">binomial</span>(logit),</a>
<a class="sourceLine" id="cb554-3" title="3">                      <span class="dt">control=</span><span class="kw">glm.control</span>(<span class="dt">trace=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## Deviance = 1658.387 Iterations - 1
## Deviance = 1654.791 Iterations - 2
## Deviance = 1654.785 Iterations - 3
## Deviance = 1654.785 Iterations - 4</code></pre>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb556-1" title="1"><span class="kw">coef</span>(model.survival)</a></code></pre></div>
<pre><code>## (Intercept)        fare 
## -0.88402354  0.01247006</code></pre>
<p>We get the exact same values from the <code>glm</code> command.</p>
<hr />
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Technically we also need to show that the second derivative is negative at this value of <span class="math inline">\(p\)</span>, but we will skip that step here. You can try it as an exercise at home if you are so inclined<a href="generalized-linear-model.html#fnref1" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-probability-model.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="logit-model.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
