---
title: "Building Models"
subtitle: "Sociology 312"
author: "Aaron Gullickson"
institute: "University of Oregon"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, uo, uol-fonts, lecture_slides.css]
    self_contained: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: ""
      ratio: 16:10
---

```{r child="setup_chunks.Rmd"}
```

class: inverse, center, middle

background-image: url(images/ridham-nagralawala-kuJkUTxR0z4-unsplash.jpg)
background-size: cover

# The OLS Regression Line

---

## Drawing straight lines

```{r echo=FALSE, fig.width=12}
ggplot(crimes, aes(x=Unemployment, y=Property))+
  geom_point()+
  labs(x="unemployment rate",
       y="property crimes (per 100,000)",
       title="What is the best line through these points?")+
  lims(x=c(0,13), y=c(0,5000))
```

---

## Drawing straight lines

```{r echo=FALSE, fig.width=12}
ggplot(crimes, aes(x=Unemployment, y=Property))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE, fullrange=TRUE)+
  labs(x="unemployment rate",
       y="property crimes (per 100,000)",
       title="This is the best-fitting line")+
  lims(x=c(0,13), y=c(0,5000))
```

---

## Elements of a straight line

```{r echo=FALSE, fig.width=12}
model <- lm(Property~Unemployment, data=crimes)
ypredict <- predict(model, newdata=data.frame(Unemployment=c(5,6)))
ggplot(crimes, aes(x=Unemployment, y=Property))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE, fullrange=TRUE)+
  geom_point(x=0, y=coef(model)[1], color="red", size=3)+
  annotate("label", x=1, coef(model)[1]*0.92, label=paste("y-intercept=", round(coef(model)[1], 2), sep=""))+
  annotate("segment", x = 5, xend = 6, 
           y = ypredict[1], yend = ypredict[1], 
           color="red",
           arrow=arrow(length=unit(0.1,"cm"), type = "closed"))+
  annotate("segment", x = 6, xend = 6, 
           y = ypredict[1], yend = ypredict[2], 
           color="red",
           arrow=arrow(length=unit(0.1,"cm"), type = "closed"))+
  annotate("label", x=5.5, y=ypredict[1]*0.92, label="run")+
  annotate("label", x=6.3, y=mean(ypredict), label="rise")+
  annotate("label", x=5.5, y=3000, 
           label=paste("slope=rise/run=",round(coef(model)[2],2), sep=""))+
  geom_vline(xintercept = 0, linetype=2)+
  labs(x="unemployment rate",
       y="property crimes (per 100,000)",
       title="Slope and intercept")+
  lims(x=c(0,13), y=c(0,5000))
```

---

##  The formula for a straight line

--

.pull-left[
### in high school

$$y=a+bx$$

* $a$ is the **y-intercept**: the value of $y$ when $x$ is zero.
* $b$ is the **slope**: the change in $y$ for a one-unit increase in $x$ (the rise over the run).
* In this kind of set-up the constant values of $a$ and $b$ are called **coefficients** -  a constant value that is multipled by a variable.
]

--

.pull-right[
### How we do it in statistics

$$\hat{y}_i=b_0+b_1x_i$$

* $\hat{y}_i$: The predicted value of $y$ for $i$th observation from the linear formula.
* $b_0$: The predicted value of $y$ when $x$ is zero.
* $b_1$: The predicted change in $y$ for a one-unit increase in $x$.
]

---

## How do we know which line is best?

--

.pull-left[
```{r echo=FALSE}
model <- lm(Property~Unemployment, data=crimes)
temp <- data.frame(Unemployment=crimes$Unemployment, Property=crimes$Property,
                   fitted=model$fitted.values)

ggplot(temp, aes(x=Unemployment, y=Property))+
  geom_smooth(method="lm", se=FALSE)+
  geom_segment(aes(xend=Unemployment, yend=fitted), color="red")+
  geom_point()+
  labs(x="unemployment rate",
       y="property crimes (per 100,000)",
       title="The red lines are the residuals/errors")+
  theme_bw()
```
]

--

.pull-right[

#### We choose the line that minimizes the error in our prediction

For a given observation $i$, the value $y_i-\hat{y}_i$ gives the residual or error in the prediction.

To get the total error in prediction, we can calculate the sum of squared residuals:

$$SSR=\sum_{i=1}^n (y_i-\hat{y}_i)^2=\sum_{i=1}^n (y_i-b_0-b_1x_i)^2$$

The best-fitting line is the one with the smallest possible sum of squared residuals. This is called the **Ordinary Least Squares (OLS) regression line**.
]

---

class: center, middle

<iframe src="https://aarongullickson.shinyapps.io/reducerss/">

</iframe>

---

## Formulas for the best-fitting line

$$b_1=r * \frac{s_y}{s_x}$$
$$b_0=\bar{y}-b_1*\bar{x}$$

--

We can calculate by hand in R, although we will learn an easier way later:

```{r}
slope <- cor(crimes$Property,crimes$Unemployment)*sd(crimes$Property)/sd(crimes$Unemployment)
slope
mean(crimes$Property)-slope*mean(crimes$Unemployment)
```

--

$$\hat{\texttt{property_crimes}}_i=1628.4+148.7(\texttt{unemployment_rate}_i)$$

---

## The OLS regression line as a model

.left-column[
![model plane](images/scarbor-siu-pKGvVjAp0P8-unsplash.jpg)
]

.right-column[
The OLS regression line is often called a **linear model** because we are measuring the relationship between two variables by applying a **linear function** to characterize the relationship.

the `lm` command can be used to create a model object in R:

```{r}
model <- lm(Property~Unemployment, data=crimes)
```

The tilde (~) is used to indicate the relationship between the two variables with the dependent variable on the left hand side. I can then use the `coef` command on this model to get my coefficients (i.e. intercept and slope).

```{r}
coef(model)
```
]

---

## Use `summary` for model TMI

```{r, highlight.output=c(11,12,17)}
summary(model)
```

---

## Add the best-fitting line to your scatterplot

.pull-left[
```{r add-line-example, fig.show = 'hide'}
ggplot(crimes, aes(x=Unemployment, y=Property))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE)+
  labs(x="unemployment rate",
       y="property crimes (per 100,000)")+
  theme_bw()
```
* `geom_smooth` with the argument `method="lm"` will add the OLS regression line to your scatterplot.
* `se=FALSE` will supress a confidence band which I will show later.
]

.pull-right[
```{r ref.label = 'add-line-example', echo = FALSE}
```
]

---

## Interpreting the results

$$\hat{\texttt{property_crimes}}_i=1628.4+148.7(\texttt{unemployment_rate}_i)$$

--

.pull-left[
### Intercept

**The model predicts** that states with **no unemployment** will have a property crime rate of 1628 crimes per 1000,000, **on average**.
]

--

.pull-right[
### Slope

**The model predicts** that a **one percent increase** in the unemployment rate **is associated with** an increase of 149 property crimes per 100,000, **on average**.  
]

---

##   Try interpreting these numbers

Try interpreting these numbers from a regression model where the dependent variable is box office returns (in millions of dollars) and the independent variable is the Tomato Meter (from 0 to 100). 

$$\hat{\texttt{box_office}}_i=18.32+0.56(\texttt{meter}_i)$$

--

.pull-left[
### Intercept

The model predicts that movies that receive a zero on the Tomato Meter will make $18.32 million, on average.
]

--

.pull-right[
### Slope

The model predicts that a one percentage point increase in the Tomato Meter is associated with a $560,000 increase in box office returns, on average.
]

---

## Nonsensical Intercepts

Try interpreting these numbers from a regression model where the dependent variable is sexual frequency (sexual encounters per year) and the independent variable is age in years. 

$$\hat{\texttt{sex}}_i=107.96-1.30(\texttt{age}_i)$$

--

.pull-left[
### Intercept

The model predicts that newborns will have sex 107.96 times per year, on average. 

`r emo::ji("open_mouth")` Say what??!!
]

--

.pull-right[
```{r echo=FALSE, fig.height=5}
model <- lm(sexf~age, data=sex)
ypred <- predict(model, newdata=data.frame(age=c(0,18)))
ggplot(sex, aes(x=age, y=sexf))+
  geom_vline(xintercept = 0, linetype=2)+
  geom_smooth(method="lm", se=FALSE, fullrange=TRUE)+
  geom_jitter(alpha=0.3, height=20)+
  geom_segment(x=0, y=ypred[1], xend=18, yend=ypred[2],
                arrow=arrow(length=unit(0.1, "inches"), ends = "both"), 
                color="red", size=2)+
  annotate("label", 30, 95, label="Outside scope of the data")+
  labs(x="age", y="sexual frequency (times per year)",
       title="scope problem")+
  theme_bw()
```
]

---

## Getting meaningful intercepts

.pull-left[
Lets subtract some constant $a$ from the variable $x$:

$$x^*=x-a$$

The value for zero on our new re-centered $x^*$ will be $a$ on the original scale.

In the formula of the `lm` command in R, we can do this easily by surrounding our math with `I()` which tells R to apply the function inside and treat it as a new variable:

```{r}
model <- lm(sexf~I(age-18), data=sex)
round(coef(model), 2)
```
]

--

.pull-right[
```{r echo=FALSE}
model2 <- lm(sexf~age, data=sex)
ggplot(sex, aes(x=age, y=sexf))+
  geom_segment(x=0, y=20, xend=18, yend=20,
               arrow=arrow(length=unit(0.1, "inches")), 
               color="grey40")+
  geom_segment(x=0, y=75, xend=18, yend=75,
               arrow=arrow(length=unit(0.1, "inches")), 
               color="grey40")+
  geom_segment(x=0, y=140, xend=18, yend=140,
               arrow=arrow(length=unit(0.1, "inches")), 
               color="grey40")+
  geom_vline(xintercept = c(0,18), linetype=1)+
  geom_smooth(method="lm", se=FALSE, fullrange=TRUE)+
  geom_jitter(alpha=0.3, height=20)+
  geom_point(x=0, y=coef(model2)[1], color="red", size=3)+
  geom_point(x=18, y=coef(model)[1], color="red", size=3)+
  annotate(x=5, y=coef(model2)[1]*1.06, geom="label", label=round(coef(model2)[1],2))+
  annotate(x=23, y=coef(model)[1]*1.06, geom="label", label=round(coef(model)[1],2))+
  scale_x_continuous(breaks=seq(from=0,to=90, by=18),
                     labels=paste(seq(from=0,to=90, by=18),
                                  seq(from=0,to=90, by=18)-18,
                                  sep="\n"),
                     limits = c(0,90))+
  labs(x="age", y="sexual frequency (times per year)",
       title="We effectively shift the y-axis")+
  theme_bw()
```
]

---

##  Now interpret these numbers

$$\hat{\texttt{sex}}_i=84.58-1.30(\texttt{age}_i-18)$$

--

.pull-left[
### Intercept
The model predicts that 18 year old individuals have sex 84.58 times per year, on average.
]

--

.pull-right[
### Slope
The model predicts that a one year increase in age is associated with 1.3 more sexual encounters per year, on average.
]

---

## How good is $x$ as a predictor of $y$?

I pick a random observation from the dataset and ask you to guess the value of $y$. What is your best guess?

--
.pull-left[

### Choose $\bar{y}$

* Because it is the balancing point, the mean will give you the smallest error, on average.
* If you repeat this procedure, your average error in prediction will be equal to $s_y$.


]

.pull-right[
```{r echo=FALSE, fig.height=6}
ggplot(crimes, aes(x=reorder(State, Property, median), y=Property))+
  geom_point()+
  geom_hline(yintercept = mean(crimes$Property),
             col="red", linetype=2)+
  labs(x=NULL,
       y="property crimes (per 100,000)",
       title="The mean is the least wrong number")+
  coord_flip()+
  annotate("label", x=49, y=mean(crimes$Property), 
           label="mean of property crime rate", color="red")+
  theme(axis.text.y = element_text(size=7))
```
]

---

## How good is $x$ as a predictor of $y$?

I pick a random observation from the dataset and *tell you its value of $x$*, and then ask you to guess the value of $y$. What is your best guess?

--
.pull-left[

### Choose $\hat{y}_i$ from the linear model

* Assuming that a linear model is reasonable, the predicted value from this model will be your best guess.
* The average error in your prediction will be equal to the average residual from the model, $|\hat{y}_i-y_i|$.
]

.pull-right[
```{r echo=FALSE, fig.height=5.5}
ggplot(crimes, aes(x=Unemployment, y=Property))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE)+
  geom_hline(yintercept = mean(crimes$Property),
             col="red", linetype=2)+
  labs(x="Unemployment rate",
       y="property crimes (per 100,000)",
       title="Choose the number on the blue line")+
  annotate("label", x=4, y=mean(crimes$Property), 
           label="mean of property\ncrime rate", color="red")
```
]

---

## How much did we reduce the error?


.pull-left[
```{r, echo=FALSE}
model <- lm(Property~Unemployment, data=crimes)
crimes$fitted <- model$fitted.values

ggplot(crimes, aes(x=Unemployment, y=Property))+
  geom_smooth(method="lm", se=FALSE, color="grey30")+
  geom_hline(yintercept = mean(crimes$Property), linetype=2)+
  geom_segment(data=subset(crimes, State=="South Dakota"),
               aes(x=Unemployment-0.05, xend=Unemployment-0.05), 
               yend=mean(crimes$Property), 
               color="red", size=1)+
  geom_segment(data=subset(crimes, State=="South Dakota"),
               aes(x=Unemployment+0.05, xend=Unemployment+0.05,
                   y=fitted), 
               yend=mean(crimes$Property), 
               color="goldenrod", size=1)+
  geom_segment(data=subset(crimes, State=="South Dakota"),
               aes(x=Unemployment+0.05, xend=Unemployment+0.05,
                   y=Property, yend=fitted), 
               color="darkgreen", size=1)+
  geom_point(alpha=0.2)+
  geom_point(data=subset(crimes, State=="South Dakota"), size=2)+
  annotate(geom="text", x=5, y=1750, label="South Dakota")+
  labs(x="Unemployment rate", y="property crimes per 100,000 population")+
  theme_bw()
```
]

.pull-right[

#### On average, what proportion of the red line is the green line across all observations?

Red: $\sum_{i=1}^n (y_i-\bar{y})^2$

Green: $\sum_{i=1}^n (y_i-\hat{y}_i)^2$

Proportion: $\frac{\sum_{i=1}^n (y_i-\bar{y})^2}{\sum_{i=1}^n (y_i-\hat{y}_i)^2}$

It turns out:

$$\frac{\sum_{i=1}^n (y_i-\bar{y})^2}{\sum_{i=1}^n (y_i-\hat{y}_i)^2}=r^2$$

```{r}
cor(crimes$Unemployment, crimes$Property)^2
```
 
]

---

## R-squared is a measure of goodness of fit

.pull-left[
```{r}
cor(crimes$Unemployment, crimes$Property)^2
```

About 19.7% of the variation in property crime rates across states can be accounted for by variation in unemployment rates across states.
]

--

.pull-right[
```{r highlight.output=17}
summary(lm(Property~Unemployment, data=crimes))
```
]

---

## Statistical inference for linear models


.pull-left[

The population model is: $$\hat{y}_i=\beta_0+\beta_1(x_i)$$

The null hypothesis of no relationship is given by: $$H_0: \beta_1=0$$

How do we test?

]

--

.pull-right[
```{r highlight.output=12}
summary(lm(Property~Unemployment, data=crimes))
```

Just look at a `summary` of the model! `r emo::ji("sunglasses")`
]

---


class: inverse, center, middle

background-image: url(images/jens-johnsson-DHJ71drt-ug-unsplash.jpg)
background-size: cover

# The Power of Controlling for Other Variables

---

class: inverse, center, middle

background-image: url(images/v2osk-c9OfrVeD_tQ-unsplash.jpg)
background-size: cover

# Including Categorical Variables as Predictors

---

class: inverse, center, middle

background-image: url(images/denys-nevozhai-7nrsVjvALnA-unsplash.jpg)
background-size: cover

# Interaction Terms
