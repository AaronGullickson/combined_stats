---
title: "Statistical Inference"
subtitle: "Sociology 312"
author: "Aaron Gullickson"
institute: "University of Oregon"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: [default, uo, uol-fonts, lecture_slides.css]
    self_contained: false
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: ""
      ratio: 16:10
---

```{r child="setup_chunks.Rmd"}
```

class: inverse, center, middle

background-image: url(images/tim-bennett-EoC_IuYmtug-unsplash.jpg)
background-size: cover

# The Problem of Statistical Inference

---

## What percent of Americans favor ending birthright citizenship?

--

We can look at the results from our politics dataset: 

```{r}
100*round(prop.table(table(politics$brcitizen)),3)
```

About 31.7% of respondents to the American National Election Study (ANES) favored ending birthright citizenship.

--

* Is the percent of *all* Americans also 31.7%? 

--

* The ANES is a **sample** of the US voting age **population**. How confident can we be that the sample percent is close to the true population percent?

---

## Drawing a statistical inference

```{r echo=FALSE, fig.width=10}
par(mar=c(0,0,0,0))
plot(-2,-2, xlim=c(0,10), ylim=c(0,10), xaxt="n", yaxt="n", bty="n", xlab="", ylab="")
rect(6,1,10,9, col="skyblue")
text(8,5,"US Population", cex=1.2)
text(8,4,expression(rho=="?"))
```

---

## Drawing a statistical inference

```{r echo=FALSE, fig.width=10}
par(mar=c(0,0,0,0))
plot(-2,-2, xlim=c(0,10), ylim=c(0,10), xaxt="n", yaxt="n", bty="n", xlab="", ylab="")
rect(6,1,10,9, col="skyblue")
text(8,5,"US Population", cex=1.2)
text(8,4,expression(rho=="?"))
rect(0,3,4,7, col="yellow")
text(2,5,"ANES Sample", cex=1.2)
text(2,4,expression(hat(p)==0.317))
arrows(6,4,4,4)
text(5,3.5,"Draw a sample", cex=1)
```

---

## Drawing a statistical inference

```{r echo=FALSE, fig.width=10}
par(mar=c(0,0,0,0))
plot(-2,-2, xlim=c(0,10), ylim=c(0,10), xaxt="n", yaxt="n", bty="n", xlab="", ylab="")
rect(6,1,10,9, col="skyblue")
text(8,5,"US Population", cex=1.2)
text(8,4,expression(rho%~~%0.317))
rect(0,3,4,7, col="yellow")
text(2,5,"ANES Sample", cex=1.2)
text(2,4,expression(hat(p)==0.317))
arrows(6,4,4,4)
text(5,3.5,"Draw a sample", cex=1)
arrows(4,6,6,6)
text(5,5.5,"Infer from sample", cex=1)
```

---

## Parameters and statistics

.pull-left[
### Parameters

* Parameters represent unknown measures in the population, such as the population mean or proportion
* Parameters are represented by greek letters (e.g. the population mean is $\mu$)

### Statistics

* Statistics represent known measurements from the sample that estimate the unknown population parameters.
* Statistics are represented by roman letters (e.g. the sample mean $\bar{x}$)

]

.pull-right[

| Measure             | Parameter | Statistic |
|:-------------------:|:---------:|:---------:|
| mean                | $\mu$     | $\bar{x}$ |
| proportion          | $\rho$    | $\hat{p}$ |
| standard deviation  | $\sigma$  | $s$       | 

]

---

## When samples go bad `r emo::ji("imp")`

--

.pull-left[
### Systematic Bias

.center[![systematic darts](images/systematic_darts.png)]

Something about our data collection procedure biases our results systematically.

* We made a mistake in our research design. 
* Statistical inference cannot fix this mistake.

]

--

.pull-right[
### Random Bias

.center[![random darts](images/random_darts.png)]

Just by random chance we happened to draw a sample that is very different from the population on the parameter we care about.

* We didn't do anything wrong! We just had bad luck.
* Statistical inference addresses this form of bias.
]

???

* Even though we know random bias *might* have caused our sample statistic to be different from the population mean, we can't know for sure if this happened or not because we don't know the true population parameter!
* This is what statistical inference is all about. Even though we can never say for certain that our results are close or far away from the true value, we can quantify our uncertainty about how different the sample statistic could potentially be from the population parameter.

---

class: inverse, center, middle

background-image: url(images/kai-pilger-qHfJPxOnXi4-unsplash.jpg)
background-size: cover

# The Sampling Distribution

---

## Three kinds of distributions

You draw a simple random sample of 100 people from the US population and calculate their mean years of education. There are three kinds of distributions involved in this process:

--

.pull-left[
### Population Distribution

* The distribution of years of education for the whole US poplation. 
* Its mean is given by $\mu$. 
* The population mean and distribution are unknown. 
]

--

.pull-right[
### Sample Distribution

* The distribution of years of education in your sample. 
* The mean is given by $\bar{x}$. 
* The mean and distribution are known and hopefully approximate the population distribution. 
]

--

### Sampling Distribution

* The distribution of the sample mean $\bar{x}$ in all possible samples of size 100. 
* We can't know this distribution exactly, but it turns out that we know its general shape. 

---

## Example: Height in our class

.pull-left[
* Lets treat our class of `r sum(!is.na(students$height))` students as the population. I want to estimate the average height of the class. 
* In this case, I am omnipotent - I know the population distribution because I collected data for the whole class on Canvas.
]

.pull-right[
```{r echo=FALSE}
ggplot(students, aes(x=height))+
  geom_histogram(binwidth=1, fill="grey", color="grey20")+
  geom_vline(xintercept = mean(students$height), size=2, color="red")+
  annotate("label", x=67.7, y=7, label="mu==66.52", parse=TRUE)+
  labs(x="student height in inches", 
       title="Distribution of student height in the class")
```
]

---

## How many samples of size 2 are possible?

Lets say I wanted to sample two students to estimate class height. In a class of `r sum(!is.na(students$height))` students, how many unique samples of size 2 exist?

--

* On the first draw, I have `r sum(!is.na(students$height))` possibilities.

--

* On the second draw, I have `r sum(!is.na(students$height))-1` possibilities because I am not putting my first draw back.

--

* I therefore have $42*41=1722$ possible samples. 

--

* However, half of these samples are just duplicates of the other half, but sampled in the other order. In one sample, I samples John and Then Kate and in another I sampled Kate and then John.

--

* Therefore the true number of unique samples is: $$42*41/2=861$$

--

* What if I calculated the sample mean in all 861 samples and looked at the distribution of these sample means?

---

## The sampling distribution

```{r hist-samplingd-size2, echo=FALSE, fig.width=12}
ggplot(data.frame(means2), aes(x=means2))+
  geom_histogram(fill="grey", color="grey20", breaks=seq(from=58,to=76, by=1))+
  labs(x="mean height in samples of size 2",
       title="Sampling distribution of mean student height from samples of size 2")+
  xlim(58, 76)+
  theme_bw()
```

---

## Sampling distributions for different $n$

```{r compare-density, echo=FALSE, fig.width=12}
temp <- data.frame(size=c(rep("n=2", length(means2)),
                          rep("n=3", length(means3)),
                          rep("n=4", length(means4)),
                          rep("n=5", length(means5))),
                   means=c(means2, means3, means4, means5))

ggplot(temp, aes(x=means, y=..density.., fill=size))+
  geom_density(data=subset(temp, size=="n=5"), alpha=0.6)+
  geom_density(data=subset(temp, size=="n=4"), alpha=0.6)+
  geom_density(data=subset(temp, size=="n=3"), alpha=0.6)+
  geom_density(data=subset(temp, size=="n=2"), alpha=0.6)+
  geom_vline(xintercept = mean(students$height), color="red", size=2, alpha=0.7)+
  labs(x="mean height", 
       title="The shape is becoming more bell-curved and narrower")+
  xlim(58, 76)+
  annotate("label", x=66.6, y=0.175, label="mu", parse=TRUE, size=10, color="red")
```

---

## What is the mean of the sampling distributions?

```{r echo=FALSE}
knitr::kable(cbind('Distribution'=c("Population Distribution","Sampling Distibution (n=2)",
                       "Sampling Distribution (n=3)", "Sampling Distribution (n=4)",
                       "Sampling Distribution (n=5)"),
      'Mean'=round(sapply(list(students$height, means2, means3, means4, means5), mean),2),
      'Standard Deviation'=round(sapply(list(students$height, means2, means3, means4, means5),
                                        sd), 2)),
      align=c("l","r","r"))
```

---

## Its the Central Limit Theorem!

.pull-left[
As the sample size increases, the sampling distribution of a sample mean becomes a **normal** distribution.

* The normal distribution is a bell-shaped curve with two characteristics: center and spread. 
* Centered on $\mu$, which is the true value in the population.
* With a spread (standard deviation) of $\sigma/\sqrt{n}$, where $\sigma$ is the standard deviation in the population. 
* The center of the sampling distribution is the true value of the parameter and the spread of the sampling distribution shrinks as the sample grows larger. 
]

.pull-right[
```{r echo=FALSE}
x <- seq(from=-5,to=5,by=0.1)
y <- (1/(sqrt(2*pi)))*exp(-1*(x)^2/(2^2))
par(mar=c(6,0.1,4,0.1))
plot(x,y, type="l", lwd=3, yaxt="n", xlab="",
     ylab="", xaxt="n",
     main="Sampling distribution of sample mean")
axis(1, at=c(-3,-1.96,-1,0,1,1.96,3), cex=0.8, las=3,
     labels=c(expression(mu-3*sigma/sqrt(n)),
              expression(mu-1.96*sigma/sqrt(n)),
              expression(mu-1*sigma/sqrt(n)),
              expression(mu),
              expression(mu+1*sigma/sqrt(n)),
              expression(mu+1.96*sigma/sqrt(n)),
              expression(mu+3*sigma/sqrt(n))))
polygon(c(x[21],x[21:81], x[81]),c(0,y[21:81],0), col="grey80")
polygon(c(x[31],x[31:71],x[71]),c(0,y[31:71],0), col="grey50")
polygon(c(x[41],x[41:61],x[61]),c(0,y[41:61],0), col="grey30")
abline(v=0, lwd=3, lty=2, col="red")
text(0,max(y)*0.9, expression(mu), col="red", cex=2, pos=4)
arrows(x[41],max(y)*0.5,x[61],max(y)*0.5, code=3, length=0.1)
arrows(x[31],max(y)*0.3,x[71],max(y)*0.3, code=3, length=0.1)
arrows(x[21],max(y)*0.05,x[81],max(y)*0.05, code=3, length=0.1)
text(0, max(y)*0.5, label="68%", col="white", pos=3)
text(1.5, max(y)*0.3, label="95%", col="white", pos=3)
text(2.5, max(y)*0.05, label="99.7%", col="black", pos=3)
```
]

---

## The Standard Error

There are three different kinds of standard deviations involved here, one that corresponds to each of the types of distributions. 

| Distribution   | Notation | Description | 
|----------------|----------|-------------|
| Population     | $\sigma$ | Unknown population standard deviation |
| Sample         | $s$ | Known sample standard deviation that hopefully approximates $\sigma$ |
| Sampling       | $\sigma/\sqrt{n}$ | **Standard error**: Standard deviation of the sampling distribution |

The standard error gives us an estimate of the strenght of potential random bias in our sample. 

---

## Sampling distributions are the `r emo::ji("key")` concept 

.pull-left[
* When we draw a sample and calculate the sample mean we are effectively drawing a value from the sampling distribution for the sample mean. 
* If we know what that distribution looks like then we can know the probability of drawing a sample close to or far from the true population parameter. 
]

.pull-right[
```{r echo=FALSE}
ggplot(data.frame(means5), aes(x=means5))+
  geom_histogram(fill="grey", color="grey20", breaks=seq(from=58,to=76, by=1))+
  geom_vline(xintercept = mean(means5), size=2, color="red")+
  labs(x="mean height in samples of size 5",
       title="Sampling distribution of mean height")+
  xlim(58, 76)+
  annotate("label", x=66.5, y=50000, label="most sample means\nwill be close to\nthe true value\nin the population")+
  annotate("label", x=75, y=15000, label="but a few will\nbe far away")+
  annotate("label", x=66.6, y=150000, label="mu", parse=TRUE, size=5, color="red")
```
]

---

## But there is a catch!

* The shape of the sampling distribution is determined by:

  * the population mean, $\mu$
  * the population standard deviation, $\sigma$

--

* But these values are unknown! `r emo::ji("open_mouth")` 

--

.pull-left[
### First Fix

* We can substitute the sample standard deviation $s$ from our sample for the population standard deviation $\sigma$. 
* This has consquences. Because we are using a sample value which can also be subject to random bias, this substitution creates greater uncertainty in our estimate which we will address later. 
]
  
--

.pull-right[
### Second Fix

* **Confidence Intervals**: Provide a range of values within which you feel confident that the true population mean resides. 
* **Hypothesis tests**: Play a game of make believe. If the true population mean was a given value, what is the probability that I would get the sample mean value that I actually did? 
]  
  