
# Modeling Categorical Outcomes

In this chapter, we will extend the concept of the linear model to include the case where we want to predict categorical outcomes on the left-hand side of our model equation. In order to do this, we will have to learn an extension of the OLS regression model called the **generalized linear model** which will provide a more flexible way to specify linear models, including ones with categorical outcomes. By the end of this chapter, you will be able to run full linear models with dichotomous (two categories) and polytomous (many categories) variables as outcomes. 

Slides for this module can be found [here](stat_slides/module7_slides_categorical_outcomes.html).

---

## Dichotomous Outcomes and The Binomial Distribution

```{r echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/WMxenh7t_Rs")
```

We now have a great many tools to produce more complex and realistic specifications for the right-hand side of the linear model formula. We can add dummy variables, interaction terms, transformations, splines, and polynomial terms. For outcomes, however, we are still stuck with quantitative variables, which eliminates a lot of potential outcomes that we care about in the social sciences.

Lets take the Titanic data as an example. The obvious outcome of interest is whether someone survived or died on the Titanic. We already know how to examine a bivariate relationship between survival and one other variable. We can do this using a two-way table if the other variable is categorical and mean differences if both variables are quantitative. However, we can't put this into a model framework where we can control for multiple variables simultaneously, look at interaction terms, model non-linearity, etc. 

For example, we know that women were more likely to survive the Titanic than men and we know that higher-class passengers were more likely to survive than lower-class passengers. However, we also know that women were more likely to be higher-class passengers than men. How much of the gender difference in survival might be accounted for by these underlying class differences? Alternatively, we might be interested in interacting gender and passenger class to look at how the gender difference in survival varies by passenger class. Both of these tasks would be easier if we could use the same model framework we have developed to predict survival and death. 

It turns out that we can use this model framework, but in order to do so we need to develop more understanding of the **data-generating process** that underlies the survivals and deaths on the Titanic, and by extension any dichotomous outcome in which there is a choice between two categorical possibilities. It turns out that this data-generating process is the **binomial distribution**.

### The binomial distribution

The binomial distribution is a theoretical probability distribution that arises when we have some process that follows these rules:

- We perform $n$ repeated **independent** trials where the result of each trial is either a **success** or **failure**. The language of "success"" and "failure"" here is purely aesthetic. We can use the binomial distribution in any case where there are two possible outcomes. 

- The probability of success on each trial is given by $p$. Because there are only two possible outcomes, the probability of a failure must therefore be $1-p$.

The binomial distribution governs how many successes we can expect to see in these $n$ trials. We consider that number of successes to be a **random variable** and traditionally write it as $X$. 

One of the simplest example of a binomial distribution would be to count the number of heads in a certain number of coin tosses. In this case, $p=0.5$. However that example is boring so instead we will consider the basic dice mechanic from the popular [Shadowrun tabletop role-playing game](https://en.wikipedia.org/wiki/Shadowrun). In that game, players determine whether their characters succeed at a task by rolling a "dice pool" which is a certain number of six-sided die. They then count the number of 5's or 6's that they roll on this dice. This count then determines whether they succeed and by how much. On an evenly-weighted die, the probability of rolling 5 or 6 should be two out of six, which simplifies to one out of three, so we have a binomial distribution with $n$ equal to how many dice the character has in their pool and $p=1/3$.

The binomial distribution formula below will tell us the probability that $X$ equals some number of successes $k$:

$$P(X=k)={n \choose k}p^k(1-p)^{n-k}$$

This formula may look complex, but its actually fairly intuitive if we break down into its parts. The binomial formula basically has two parts:

- $p^k(1-p)^{n-k}$ defines the probability of getting any particular sequence of $k$ successes and $n-k$ failures.
- ${n \choose k}$ tells us the number of unique ways that we can combine $k$ successes and $n-k$ into a sequence. 

Lets start with the first part. Lets assume that our Shadowrun player had a dice pool of five dice, so $n=5$. What is the probability that they rolled the following sequence: success, success, failure, failure, failure? 

One important feature of probabilities is that when events are **independent**, then the probability that they **all** happen is given by multiplying the individual probabilities together. In this case, the probability of a success is $1/3$ and the probability of a failure is $2/3$. Therefore the probability of getting that **exact** sequence is given by:

$$(1/3)(1/3)(2/3)(2/3)(2/3)=(1/3)^2(2/3)^3=0.033$$

I just multiply the probabilities together to get the probability of the exact sequence. Because I am multiplying the same number together, I can collect these terms together by using powers. The probability of getting this **exact** sequence is 0.033 or 3.3%. 

Note that this is also the probability of getting the sequence of success, failure, success, failure, failure because the order of the multiplication can be moved around and will still come out to $(1/3)^2(2/3)^3$. So, this is the probability of getting any particular sequence of two successes and three failures. 

I can generalize this to any $n$ and $k$ by just replacing the numbers with the abstract values. So:
$$p^k(1-p)^{n-k}$$
is the probability of any particular sequence of $k$ successes and $n-k$ failures in $n$ trials.

Notice that I keep saying "any particular sequence." If we are only interested in the total number of successes, we don't care what order they come in. However, the equation above, only gives us the probability of getting a particular order of $k$ successes and $n-k$ failures. To consider the total probability of $k$ successes, we have to consider all the possible ways we could get a sequence giving us $k$ successes.

For example, to continue our example of the Shadowrun dice pool of five dice, how many ways could we combine two successes and three failures. Here are all the ways:

- SSFFF
- SFSFF
- SFFSF
- SFFFS
- FSSFF
- FSFSF
- FSFFS
- FFSSF
- FFSFS
- FFFSS

There are ten possible sequences (or permutations) that would give us two successes in five trials. Each of these permutations has a probability of 0.033. To get the overall probability we need to add them up or just take $10*0.033=0.33$. So, the actual probability of rolling two successes in five trials is about 33%. Thats much better than 3.3%! The lesson here is that permutations matter.

How can I determine the number of possible permutations systematically? Thats what the ${n \choose k}$ or "$n$ choose $k$" formula answers. This formula is given by:

$${n \choose k}=\frac{n!}{k!(n-k)!}$$

If you are wondering what all the exclamations points are about, its not because I am really excited (although I am). These are called **factorials**. A factorial indicates that a number should be multiplied by all of the descending integers down to one. So, $4!$ is actually:

$$4*3*2*1$$

In practice, many of the numbers in the n choose k formula actually cancel out so it typically involves less math than you would think. Here is the n choose k formula for $n=5$ and $k=2$:

$${5 \choose 2}=\frac{5!}{2!(5-2)!}=\frac{5!}{2!3!}=\frac{5*4*3*2}{2*3*2}=5*2=10$$

When we put these two parts together, we get the full binomial formula:

$$P(X=k)={n \choose k}p^k(1-p)^{n-k}$$

Lets try it out for all the possible values in our Shadowrun dice pool:

```{r dice-pool, fig.cap="Probabilities of the number of hits (rolling a five or six) in a five dice pool"}
n <- 5
k <- 0:n
p <- 1/3
prob <- choose(n,k)*p^k*(1-p)^(n-k)
ggplot(data.frame(k,prob), aes(x=k, y=prob))+
  geom_col()+
  scale_y_continuous(labels=scales::percent)+
  labs(x="hits (number of fives or sixes) in five dice rolls",
       y="probability")+
  theme_bw()
```

We have about a one in three chance of rolling either one or two successes and about a 12% chance of getting no successes. At the other end of the spectrum, it is very unlikely to get five successes in five trials. Go ahead and pause here and play this game for yourself if you like to see how your results stack up. Al you need is five dice. Go ahead, I will wait. 

What would happen if we had a dice pool of twenty dice? Lets try it:

```{r dice-pool-gib, fig.cap="Probabilities of the number of hits (rolling a five or six) in a twenty dice pool"}
n <- 20
k <- 0:n
p <- 1/3
prob <- choose(n,k)*p^k*(1-p)^(n-k)
ggplot(data.frame(k,prob), aes(x=k, y=prob))+
  geom_col()+
  scale_y_continuous(labels=scales::percent)+
  labs(x="hits (number of fives or sixes) in twenty dice rolls",
       y="probability")+
  theme_bw()
```

Of course, the most likely number of successes is higher because we are rolling more dice. The shape is also starting to look more like a normal distribution. This is not a coincidence. As $n$ increases the shape of the binomial distribution will look more and more like a normal distribution. 

#### Expected value and variance

The **expected value** of a random variable given by $E(X)$ is the same as the mean of its probability distribution. In the case of the binomial distribution, the expected value is:

$$E(X)=np$$

If you think about it for a second, this value is completely intuitive. The expected number of successes is equal the probability of a success on any given trial multiplied by the number of trials.

We can also calculate the variance of the random variable $V(X)$. For the binomial distribution, this is given by:

$$V(X)=np(1-p)$$

This formula has an important implication. First, the variance depends on the underlying probability of success. Second, for a given $n$, this probability will be maximized at a certain value of $p$. To see what value of $p$ that is, lets go ahead and calculate the variance for our example with $n=5$ for every possible $p$ at 0.01 intervals:

```{r varbinom, fig.cap="Standard deviation for binomial distribution with five trials by different probabilities of success"}
p <- seq(from=0.001,to=0.999, by=.001)
v <- 5*p*(1-p)
ggplot(data.frame(p,v), aes(x=p, y=sqrt(v)))+
  geom_line()+
  labs(x="probability of success",
       y="standard deviation in number of successes for n=5")+
  theme_bw()
```

The variance will always be at its greatest when the probability of success is 50%. As you get closer to probabilities of 0% or 100%, you will get less variance because most trials will be failures or successes, respectively.

#### The Bernoulli distribution

The Bernoulli distribution is a special case of the binomial distribution with just a single trial $(n=1)$. Alternatively, a binomial distribution can be thought of as the sum of $n$ independent Bernoulli distributions. The Bernoulli distribution is particularly important for our purposes because each observation in our data typicallly only has one trial. The expected value of the bernoulli distribution is simply $p$ and the variance is $p(1-p)$. 

### The binomial distribution as a data-generating process

Let us now return to the Titanic example and consider the process that generated our actual data. In our actual data we only have a record of "successes" (i.e. survival) and "failures" (i.e. deaths). However, underlying this data, we can imagine that each passenger had their own very personal (and stressful) Bernoulli trial. Each passenger had some underlying probability of surviving the Titanic. We can refer to that underlying probability as $p_i$. Importantly, it is subset by $i$ because we imagine that probability was different for each passenger. Given that $p_i$, each passenger was then given one bernoulli trial and the result was either survival or death. 

Although survival and death is what we observe, what we actually want to know about is $p_i$. Furthermore, we would like to know how $p_i$ was affected by other characteristics of the passenger such as gender, passenger class, age, and fare paid. In the next section, we will make our first attempt at estimating these $p_i$ values in a model.

---

## Linear Probability Model

```{r echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/AQXIzrEJL1I")
```

In the previous section, I said that you cannot fit a linear model by OLS regression if the dependent variable is categorical. This is mostly true, but not exactly true. I can brute-force an OLS regression model by turning a dichotomous dependent variable into numeric values of 0 (for failure) and 1 (for success). In *R*, I can do this by turning my dependent variable into a boolean statement. Lets try it with a model that predicts survival on the Titanic by fare paid.

```{r lpm_survival}
model_lpm <- lm((survival=="Survived")~fare, data=titanic)
coef(model_lpm)
```

I now have a model that works, but what do the results mean? One way we can try to understand this model is by visualizing it on a scatterplot.

```{r lpm-plot, fig.cap="Scatterplot of fare paid by survival on the Titanic with linear probability model fit. Points are shown with semi-transparency to address overplotting."}
ggplot(titanic, aes(x=fare,  y=as.numeric(survival=="Survived")))+
  geom_point(alpha=0.1)+
  geom_smooth(method="lm", se=FALSE)+
  scale_y_continuous(breaks = c(0,1), labels=c("0","1"))+
  labs(x="fare paid in pounds", y="Titanic survival as numeric value")+
  theme_bw()
```

All of the points fall on two horizontal lines at $y=0$ (deaths) and $y=1$ (survivors). We can sort of see the positive relationship in that the deaths are more tightly clustered around low values of fare, while the survivors are more spread out. The blue line plots the OLS regression line I just calculated above. 

If we were to take all the passengers at any interval, we could calculate the proportion who survived by simply taking the means of the zeros and ones for the survival variable. Lets try that for all passengers who paid between ten and twenty pounds:

```{r surival-means}
mean(subset(titanic, fare>=10 & fare<=20)$survival=="Survived")
```

So about 37.9% of the passengers paying between 10 and 20 pounds survived. Figure \@ref(fig:lpm-plot-markup) below shows this point graphically. You can see that it falls pretty close to the best-fitting line. This best-fitting line is estimating the same exact thing: the predicted proportion of survivors at a given value of fare.

```{r lpm-plot-markup, fig.cap="Scatterplot of fare paid by survival on the Titanic with linear probability model fit. Red dot shows proportion of surviving passengers within the grey band."}
ggplot(titanic, aes(x=fare,  y=as.numeric(survival=="Survived")))+
  annotate("rect", xmin = 10, xmax = 20, ymin = 0, ymax = 1, alpha = 0.2)+
  geom_point(alpha=0.1)+
  geom_smooth(method="lm", se=FALSE)+
  geom_point(data=data.frame(fare=15, y=0.3793103), aes(y=y), color="red", size=2)+
  scale_y_continuous(breaks = c(0,1), labels=c("0","1"))+
  labs(x="fare paid in pounds", y="Titanic survival as numeric value")+
  theme_bw()
```

This blue line is giving us the **linear probability model**. Formally, the linear probability model in this case gives us:

$$\hat{p}_i=0.3059+0.0023(fare_i)$$

The outcome, $\hat{p}_i$ is the predicted probability of survival for the $i$th passenger. When fare paid is zero, we expect that probability to be 0.3059 of 30.59%. The model predicts that each additional pound of fare paid is associated with a 0.23 percentage point increase in the probability of survival. 

At first glance, this model seems to give us exactly what we said we wanted from the last section -- the predicted probability of survival for each passenger. However, it turns out there are two significant problems with the linear probability model that make it a less than ideal model. 

### Heteroscedasticity

The first problem is one we have seen before -- heteroscedasticity. However, we will now see it in a new form. From the previous section we know that the variance of the actual survival outcome for a passenger should be given by:

$$p_i(1-p_i)$$

The problem here is that the variance of the outcome is itself a function of the value of $p_i$ and $p_i$ in our model will be different for each passenger. So, we have unequal variance in the residuals of our outcome and violate the assumption of identical distributions. Each passenger is reaching into their own very personal Bernoulli distribution to decide whether they live or die. 

This problem is theoretically correctable, via the **iteratively reweighted least squares** approach that we used in the previous module. In this case, we need to apply weights to our results that are the inverse of the variance for each observation. In this case, the weight for each observation should be:

$$w_i=\frac{1}{\hat{p}_i(1-\hat{p}_i)}$$

Where $\hat{p}_i$ is the estimated probability from our initial model. We can then iterate through models until our estimates stop changing. Lets try it out using our initial model from above as the starting point.

```{r irls-lpm, error=TRUE}
model_next <- model_lpm
for(i in 1:10) {
phat <- model_next$fitted.values
weight <- 1/(phat*(1-phat))
model_next <- update(model_next, w=weight)
}
```

It looks like something went wrong. The problem is that some of the values for $\hat{p}_i$ are greater than one. You can look at the line in Figure \@ref(fig:lpm-plot) to confirm this issue. That causes problems in our approach because it makes some of the estimated weights negative. There is no particularly good approach to solving this problem. We could eliminate observations where $\hat{p}_i>1$ or we could truncate those values to some value like 0.99, but if we choose 0.999 rather than 0.99, we will get different results. The fundamental issue is that we are running into the second problem with linear probability models detailed below -- they can give you nonsense values for predicted probabilities outside the range of zero to one.

One solution to this problem if we still want to fit this model and deal with heteroscedastictity is to apply robust standard errors as we saw in the last module:

```{r robust-lpm}
library(lmtest)
library(sandwich)
coeftest(model_lpm, vcov=vcovHC(model_lpm, "HC1"))
```

This approach should give us correct standard errors. However, as is often the case with robust standard errors, we are getting correct standard errors for a bad model because we have not been able to address the much more important problem.

### Nonsense values

By definition, the linear probability model fits probabilities via a straight line. The thing about straight lines is they just keep going up and up (and down and down) at a constant slope with no upper or lower limits on the values that they can take. However, a probability does have a very clear theoretical upper and lower bounds. Probabilities cannot be below zero or above one. You cannot have a -20% or 150% chance of surviving the Titanic. 

In some cases, you may by happenstance get a line that fits within the theoretical range for the scope of your independent variable. However, there is no guarantee of this. Figure \@ref(fig:lpm-plot-nonsense) below clearly shows that we get predicted probabilities above one within the scope of the data for our linear probability model of survival on the Titanic by passenger class.

```{r lpm-plot-nonsense, echo=FALSE, fig.cap="Scatterplot of fare paid by survival on the Titanic with linear probability model fit. Points are shown with semi-transparency to address overplotting. Nonsense values above one and below zero are shaded in red."}
ggplot(titanic, aes(x=fare, 
                    y=as.numeric(survival=="Survived")))+
  annotate("rect",xmin=-Inf, xmax=Inf, ymin=1, ymax=Inf, alpha=0.2,
           fill="red")+
  annotate("text", x=250, y=1.2, label="Nonsense region")+
  annotate("rect",xmin=-Inf, xmax=Inf, ymin=-Inf, ymax=0, alpha=0.2,
           fill="red")+
  geom_hline(yintercept = c(0,1), linetype=2)+
  geom_point(alpha=0.1)+
  geom_smooth(method="lm", se=FALSE)+
  scale_y_continuous(breaks = c(0,1), labels=c("0","1"))+
  labs(x="fare paid in pounds", y="Titanic survival as numeric value")
```

While we can fix heteroscedasticity, there is no fix for this problem within the framework of the linear probability model. The linear probability model is not a very good model because it does not respect the underlying data generation process.

### Logit transformation to the rescue

In order to resolve this problem, we need some kind of transformation of the $p_i$ values that will cause them to be contained within the interval from zero to one. It turns out that the transformation we are looking for is the **logit** transformation.

The logit transformation converts a probability into the log-odds. Formally, 

$$logit(p)=log(\frac{p}{1-p})$$

There are really two parts to this transformation. First, we convert from probabilities to odds by taking $p/(1-p)$. We have seen odds before in this course, in the section on two-way tables. There we learned how to calculate the odds ratio. The odds is the ratio of the expected number of successes to failures. So, if $p=0.75$, we expect that three out of every four trials will produce successes, on average. In terms of the odds, we expect three successes for every one failure. To convert:

$$O=\frac{p}{1-p}=\frac{0.75}{1-0.75}=\frac{0.75}{0.25}=3$$

Why do we convert from probabilities to odds? The advantage of the odds is that it has no upper limit. As the probability gets closer and closer to onem, the odds will approach infinity, with no finite limit. Therefore, any non-negative number for the odds can be converted back into a probability that will give sensible values between zero and one. I can convert back to a probability by:

$$p=\frac{O}{1+O}$$

So, for the case of $O=3$ above:

$$p=\frac{3}{1+3}=\frac{3}{4}=0.75$$

Lets choose a really high odds like $O=100,000$. If we convert back to a probability:

$$p=\frac{100000}{1+100000}=\frac{1}{100001}=0.99999$$

We get a very high probability, but its still less than one. 

This partially helps us with our problems. If we were to look at a linear relationship between the odds of success and our independent variables we would get sensible probabilities no matter how high the predicted odds. However, it only partially helps us because it would still be possible to get negative odds from such a linear model which would be nonsensical.

The second step of logging the odds will get us all the way there. If I log a value below one, I will get a negative value and that logged value will approach negative infinity as the original value approaches zero. So a log-odds can always be converted back to a probability that will lie between zero and one. 

To convert from a log-odds $g$ to a probability, I take:

$$p=\frac{e^g}{1+e^g}$$

The value $e^g$ converts from log-odds to odds and then I just use the formula for converting from an odds to a probability. 

In essence the log-odds, or *logit*, transformation stretches out my probabilities across the entire number line. Lets see what this looks like graphically for a sequence of probabilities from 1% to 99%:

```{r logistic-curve, fig.cap="The relationship between probability and the logit transformation"}
p <- seq(from=0.01, to=0.99, by=0.001)
logit <- log(p/(1-p))
ggplot(data.frame(p, logit), aes(x=logit, y=p))+
  geom_line()+
  geom_hline(yintercept = c(0, 1), linetype=2, color="red")+
  labs(x="logit transformation", y="probability")+
  theme_bw()
```

The "S" curve shown here is often called the **logistic curve**. There are a couple of things to note about this curve:

- The curve approaches but never crosses the horizontal lines for $p=0$ and $p=1$. Thats because all finite logit values will produce probabilities within the correct theoretical range. 
- A probability of 50% corresponds to a logit of zero. Logit values below zero indicate probabilities less than 50% and logit values above zero indicate probabilities greater than 50%. 

So, it seems like we now have a potential solution to our problem with the linear probability model. If we were to transform our dependent variable from predicting probabilities to predicting log-odds, we could then get predicted log-odds for each passenger on the Titanic. When converted back to probabilities, we would be ensured that our predicted probabilities never strayed beyond zero and one. 

That all sounds great, except there is an important catch. 

```{r logit-transform-naive, error=TRUE}
titanic$survived <- titanic$survival=="Survived"
model_better <- lm(I(log(survived/(1-survived)))~fare, data=titanic)
```

We cannot actually apply this transformation directly to our dependent variable, as we have done in the past. Why not? The problem is that we want to apply this transformation directly to $p_i$ but we don't have $p_i$. We only have the actual recorded outcome as either a zero (died) or a one (survived). But there are no proper finite values of the logit at exactly zero and one. So, this approach will not work using OLS regression techniques. In order to get what we want, we need to move to a new model estimation technique -- the **Generalized Linear Model**.

---

## Generalized Linear Model

```{r echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/FbbHf4rdVgI")
```

Generalized linear models (GLM) will allow us to extend the basic idea of our linear model to incorporate more divderse outcomes and to specify more directly the **data generating process** behind our data. 

To better understand what GLMs do, I want to return to a particular set-up of the linear model. In this set-up, there are two equations. The first equation partitions the value of an actual outcome $(y_i)$ into the part accounted for by our model $(\hat{y}_i)$ and the random residual "leftover" bits $(\epsilon_i)$:

$$y_i=\hat{y}_i+\epsilon_i$$

We then have a second equation that details how the structural model part $(\hat{y}_i)$ is specified by a linear function of the independent variables:

$$\hat{y_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}$$

We should have a good sense of the second equation by now as its the basic linear model set-up. I want to focus more on the first equation right now. Its important to remember that the error terms $(\epsilon_i)$ are randomly drawn from some distribution. Its not important what that distribution is so long as all of the $\epsilon_i$ are drawn from the same distribution independently. However, for the purposes of illustration lets assume that the $\epsilon_i$ are being drawn from a normal distribution. We can then describe that distribution mathematically as:

$$\epsilon_i \sim N(0, \sigma)$$

The $\sim$ sign means "distributed as." In this case, our error terms are distributed as a normal distribution that is centered on zero and has some standard deviation $\sigma$. It doesn't really matter what that $\sigma$ is for our purposes here.

Think about this from a data-generating perspective. To get an actual value of $y$ for the $i$th observation:

1. We feed all of that observation's values for $x$ into our linear function which gives us a predicted value of $y$, $\hat{y}_i$. This is only the structural part. All observations with the same values of $x$ will get the same $\hat{y}_i$ because we have not added the random bit.
2. Reach into our normal distribution and pick out a residual that we add onto the end of our predicted value to get the actual value. This residual adjusts for all the random factors not accounted for in our model that might cause variation between observations with the exact same predicted value. 

One way of thinking about the second part is that instead of reaching into a normal distribution centered on zero for the residual, we are reaching into a normal distribution centered on $\hat{y}_i$ because all we are going to do is add the constant value of $\hat{y}_i$ to whatever we pull out of that distribution. Therefore, we can actually re-write the first equation parsing $y$ into the structural and stochastic components as:

$$y_i \sim N(\hat{y}_i, \sigma)$$

We can now have all the pieces to reformulate this linear model in the framework of the generalized linear model. 

### Generalized linear model framework

The generalized linear model requires two components: the **error distribution** and the **link function**.

The **error distribution** specifies how the outcome that we actually measure in our data is distributed. In this case, the error distribution is given by:

$$y_i \sim N(\hat{y}_i, \sigma)$$

Therefore, the error distribution is normal. The normal distribution is also sometimes called the **gaussian* distribution and that is how we will specify it in *R* below.

The **link function** specifies how the linear function of the independent variables is related to the key parameter of the error distribution. In this case the key parameter of the error distribution is $\hat{y}_i$ and our linear function is related directly to that parameter:

$$\hat{y_i}=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}$$

In this case, the link function is somewhat invisible because there is not really a link function at all. We are simply relating the linear function of the independent variables directly to the key parameter of the error distribution. In practice this si called the *identity* link.

So within the generalized linear model framework, we can express our traditional linear model as using a gaussian error distribution and an identity link. Lets go ahead and try that out. The command `glm` in *R* will estimate a generalized linear model. We will talk later in this section about how that estimation works, but for now I just want to focus on the results. 


```{r glm-example}
summary(glm(indegree~nsports, data=popularity,
            family=gaussian(identity)))$coef
```

To specify the error distribution and link function, I used the `family` argument in the `glm` command. Lets compare this result to the traditional `lm` command that is estimated via OLS regression:

```{r lm-example}
summary(lm(indegree~nsports, data=popularity))$coef
```

The results are identical. Keep in mind that these two commands used completely different estimation strategies. We know that OLS regression minimizes the sum of squared residuals. The GLM uses a technique called **maximum likelihood estimation** that we will learn about later in this section. However, the key point is that they both produced the same estimate. When we specify the data-generating process as a guassian error distribution with an identity link, we are estimating a traditional linear model.

At this point, this new framework and estimation approach hasn't really done much for us. The `lm` command will work fine and we also don't need to make the assumption that our residual terms are normally distributed for OLS regression models to be valid. So why set up this more complex framework? The answer is that by changin the error distribution and link function, we can accomodate a broad set of models that are cannot be estimated well by OLS regression techniques. Most of those models involve categorical variables. The most common model is the **logit model** (also called the logistic regression model) that can be used for dichotomous outcomes.

### A GLM for dichotomous outcomes

```{r echo=FALSE}
knitr::include_url("https://www.youtube.com/embed/2uClzwRGMuk")
```

In order to help us understand the logit model better, I want to start this in reverse. We will play god and actually generate the data. Then we can use the model to help recover the process we used to generate the data.

For this example, I want to stick with the theme of ocean liner disasters, but I want to create my own disaster. To do that, we are going to use the fictional example of the *Good Ship Lollipop*. The *Good Ship Lollipop* has a large number of 10,000 passengers. We know two things about these passengers: their gender and the amount they paid in fare. To create the passengers of the *Good Ship Lollipop* I am going to use some handy functions in R for producing random outcomes:

```{r good-ship-lollipop}
good_ship <- data.frame(gender=sample(c("Male","Female"),10000,replace=TRUE),
                        fare=rgamma(10000, 1,0.01))
summary(good_ship)
```

The data looks pretty reasonable. Unfortunately, the *Good Ship Lollipop* is going to hit an iceberg and sink on its first voyage because the crew were too busy singing and drinking spiked Shirley Temples to keep an eye out. Some passengers will survive this sinking and some will not. Since I am playing god, the first thing I need to do is figure out the underlying $p_i$ probability of survival for each passenger.

I want $p_i$ to be a function of gender and fiare paid. However, I know that its not safe or sensible to make it a direct linear function because I am end up with nonsensical values of $p_i$. If I instead make the log-odds (or logit) of survival be a linear function of gender and fare paid, then I will be assured that when the log-odds are converted to probabilities, all the probabilities will fall between zero and one. So I set up my model:

$$log(\frac{p_i}{1-p_i})=0.05-0.40(male_i)+0.005(fare_i)$$

The numbers I put in here aren't particular important and I could vary them if I wanted to change how they related to survival. Right now, I am saying that men were less likely to survive and fare was positively associated with survival. The baseline log-odds of survival for a man who paid no fare is 0.05 which works out to a probability of 0.512 or 51.2%.

Lets go ahead and feed this equation into my data to get predicted log-odds. We can then convert from those log-odds to probabilities:

```{r gsl-convertlogit}
good_ship$log_odds <- 0.05-0.4*(good_ship$gender=="Male")+0.005*good_ship$fare
good_ship$odds <- exp(good_ship$log_odds)
good_ship$probs <- good_ship$odds/(1+good_ship$odds)
```

The `probs` vector gives us the probability of survival for every passenger. In order to complete this process I now need to have every passenger make their Bernoulli trial to see if they survive the disaster. I can do this easily in *R* by using the `rbinom` function:

```{r gsl-killem}
good_ship$survived <- rbinom(10000, 1, good_ship$probs)
```

Lets use `ggplot` to visualize how this all played out:

```{r gsl-visualize, fig.cap="Life and death on the Good Ship Lollipop. The lines show the underlying probabilities of survival by gender and survival. The dots show actual outcomes after drawing a bernoulli trial for each passenger."}
ggplot(good_ship, aes(x=fare, color=gender))+
  geom_point(aes(y=survived), alpha=0.2)+
  geom_line(aes(y=probs))+
  labs(x="fare paid", y="probability of survival")+
  theme_bw()
```

We can see that women were more likely to survive. This difference in survival shrank as fare paid increased because both groups started to approach the 100% probability threshold. We can also see from the dots that more women ended up in the survivor group as we would expect and that people were more likely to survive at higher fares paid. What we are seeing in Figure \@ref(fig:gsl-visualize) is the data-generating process. The lines give us the underlying probabilities and the dots show us the realization of those probabilities into the ones and zeros we actually would have in the data. 

Can we reverse this data-generating process to recover the values I used to construct the probabilities? We can do so using a GLM approach. In this case we know the error distribution and link function.

The error distribution tells us how our dependent variable is distributed. In this case, we either have a 1 (survived) or a 0 (died). Each of these values was produced by a binomial distribution with $n=1$ and a probability equal to $p_i$. So:

$$y_i \sim binom(1, p_i)$$

The key parameter in this error distribution is $p_i$. The link function should tell us how we relate $p_i$ to the linear function of the independent variables of gender and fare. In this case, the relationship is not direct. We related the linear function of the independent variables to the log-odds or logit of the probability of survival:

$$log(\frac{p_i}{1-p_i})=\beta_0+\beta_1(male_i)+\beta_2(fare_i)$$

So to run this model we need to specify a **binomial** error distribution and a **logit** link function in a `glm` framework. Lets try it out:

```{r gsl-logit}
model.glm <- glm(survived~gender+fare, data=good_ship, 
                  family=binomial(link=logit))
summary(model.glm)$coef
```

It worked! Note that my coefficient estimates are very close to the actual values I used when playing god. They differ slightly because there is some inherent random in choosing survivors and deaths by the binomial distribution. 

We now have a framework for a model that predicts dichotomous outcomes. This is called the **logit model** or **logistic regression model**. In practice, it is one specification of the generalized linear model. 

However, we still need to understand two things. First, how are those values actually estimated in the generalized linear model? We will take that up below. Second, how do we interpret those results? We will take that up with a more thorough discussion of the logit model in the next section.

### Maximum likelihood estimation

---

## Logit Model

---

## Models Polytomous Outcomes

---
