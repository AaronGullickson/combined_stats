# The Distribution of a Variable

The **distribution** of a variable refers to how the different values of that variable are spread out across observations. This distribution gives us a sense of what the most common values are for a given variable and how much these values vary. It can also alert us to unusual patterns in the data. Intuitively, we think about distributions in our personal life any time we think about how we "measure" up relative to everyone else on something (income, number of Facebook friends, GRE scores, etc.). We want to know where we "fall" in the distribution of one of these variables. 

In this chapter we will first learn graphical techniques that allow us to visualize what the distribution of a variable looks like. For quantitative variables we will then move on to calculate important summary statistics that measure the center and spread of a distribution.  

Slides for this module can be found [here](https://aarongullickson.github.io/stat_slides/module2_slides_distribution_variable.html).

---

## Looking at Distributions

One of the best ways to understand the distribution of a variable is to visualize that distribution with a graph. However, the technique we use to graph the distribution will depend on whether we have a categorical or a quantitative variable. For categorical variables, we will use a **barplot**, while for quantitative variables, we will use a **histogram**. 

### Looking at the distribution of a categorical variable

#### Calculating the frequency

In order to graph the distribution of a categorical variable, we first need to calculate its **frequency**. The frequency is just the number of observations that fall into a given category of a categorical variable. We could, for example, count up the number of passengers who were in the various passenger classes on the Titanic. Doing so would give us the following:

```{r titanic-pclass-table, echo=FALSE}
x <- table(titanic$pclass)
x <- c(x, sum(x))
nam <- c("First","Second","Third","Total")
tab <- cbind(nam,x)
colnames(tab) <- c("Passenger class", "Frequency")
rownames(tab) <- NULL
if(is_html) {
  kable(tab, align=c("l","r"),
        caption="Passengers on the Titanic by passenger class") %>% 
    kable_styling(bootstrap_options = c("striped","hover"),
                  full_width = FALSE) %>%
    row_spec(4, bold=TRUE, color="white",background="#666")
} else {
  kable(tab, "latex", booktabs=TRUE, align=c("l","r"),
        caption="Passengers on the Titanic by passenger class") %>%
    row_spec(4, bold=TRUE)
}
```

There were 323 first class passengers, 277 second class passengers, and 709 third class passengers. Adding those numbers up gives us 1,309 total passengers. *R* will calculate these numbers for us easily using the `table` command: 

```{r table-command-pclass}
table(titanic$pclass)
```

#### Frequency, proportion, and percent

The frequency we just calculated is sometimes called the **absolute frequency** because it just counts the raw number of observations. However such raw numbers are not usually very helpful because they will vary by the overall number of observations. Instead, we typically want to calculate the **proportion** of observations that fall within each category. These proportions are sometimes also called the **relative frequency**.  We can calculate the proportion by simply dividing our absolute frequency by the total number of observations:

```{r pclass-table-props, echo=FALSE}
x <- c(paste(paste(tab[1:3,2],tab[4,2],sep="/"),round(as.numeric(tab[1:3,2])/as.numeric(tab[4,2]),3),sep=c("=")),"")
tab <- cbind(tab,x)
tab[4,3] <- "1.0"
colnames(tab) <- c("Passenger class", "Frequency","Proportion")
if(is_html) {
  kable(tab, align=c("l","r","r"),
        caption="Passengers on the Titanic by passenger class") %>% 
    kable_styling(bootstrap_options = c("striped","hover"),
                  full_width = FALSE) %>%
    row_spec(4, bold=TRUE, color="white",background="#666")
} else {
  kable(tab, "latex", booktabs=TRUE, align=c("l","r","r"),
        caption="Passengers on the Titanic by passenger class") %>% 
    row_spec(4, bold=TRUE)
}
```

*R* provides a nice shorthand function titled `prop.table` to conduct this operation. The `prop.table` command should be run on the output from a `table` command. 

```{r prop-table-command}
prop.table(table(titanic$pclass))
```

Note that I have "wrapped" the `prop.table` command around the `table` command here to do this calculation in a single line. 

We often convert proportions to percents which are more familiar to most people. To convert a proportion to a percent, just multiply by 100:

```{r pclass-table-percents, echo=FALSE}
prop <- round(as.numeric(tab[1:3,2])/as.numeric(tab[4,2]),3)
x <- c(paste(prop,"*100=",prop*100,"%",sep=""),"")
tab <- cbind(tab,x)
tab[4,3] <- "1.0"
tab[4,4] <- "100%"
colnames(tab) <- c("Passenger class", "Frequency","Proportion", "Percent")
if(is_html) {
  kable(tab, align=c("l","r","r","r"),
        caption="Passengers on the Titanic by passenger class") %>% 
    kable_styling(bootstrap_options = c("striped","hover"),
                  full_width = FALSE) %>%
    row_spec(4, bold=TRUE, color="white",background="#666")
} else {
  kable(tab, "latex", booktabs=TRUE, align=c("l","r","r","r"),
        caption="Passengers on the Titanic by passenger class") %>% 
    row_spec(4, bold=TRUE)
}
```

24.7% of passengers were first class, 21.2% of passengers were second class, and 54.2% of passengers were third class. Just over half of passengers were third class and the remaining passengers were fairly evenly split between first and second class.

#### Don't make a piechart

Now that we have proportions/percents, we can use these values to construct a graphical display of the distribution. One of the most common techniques for doing this is a **piechart**. Figure \@ref(fig:piechart) shows a piechart of the distribution of passengers on the Titanic. 

```{r piechart, echo=FALSE, fig.cap='Piechart of passenger class distribution on Titanic', fig.align='center'}
p <- prop.table(table(titanic$pclass))
pie(p)
```

You will notice that I do not show you the code for constructing this piechart. I hid this code because I don't ever want you to construct a piechart. Despite their popularity, piecharts are a poor tool for visualizing distributions. In order to judge the relative size of the slices on a piechart, your eye has to make judgments in two dimensions and with an unusual pie-shaped slice ($\pi*r^2$ anyone?). As a result, it can often be difficult to decide which slice is bigger and to properly evaluate the relative sizes of each of the slices. In this case, for example, the relative size of first and second class are quite close and it is not immediately obvious which category is larger. 

#### Make a barplot

A better way to display the distribution is by using a **barplot** in which vertical or horizontal bars give the proportion or percent. Figure \@ref(fig:barplot-passclass) shows a barplot for the distribution of passengers on the Titanic. 

```{r barplot-passclass, fig.cap='Barplot of passenger class distribution on the Titanic', fig.align='center'}
ggplot(titanic, aes(x=pclass, y=..prop.., group=1))+
  geom_bar()+
  labs(x="passenger class", y="percent")+
  scale_y_continuous(labels=scales::percent)+
  coord_flip()+
  theme_bw()
```

Figure \@ref(fig:barplot-passclass) is our first code example of the `ggplot` library that we will use to make figures in *R*. I will discuss how this code works below, but first I want to focus on the figure itself. Unlike the piechart, our eye only has to work in one dimension (in this case, horizontal). We can clearly see that third class is the largest passenger class with slightly more than 50% of all passengers. We also can see easily determine visually that third class passengers are at more than twice as common as either of the other two categories, simply by comparing the height of the bars. We can also see that slightly more passengers were in first class than second class simply by comparing the heights of those two bars. 

So how did I create this graph? The `ggplot` syntax is a little different than most of the other R syntax we will look at this term. With `ggplot`, we add several commands together with the `+` sign to create our overall plot. Each command adds a "layer" to our plot. These layers are:

1. `ggplot(titanic, aes(x=pclass, y=..prop.., group=1))`: The first command is always the `ggplot` command itself which defines the dataset we will use (in this case, `titanic`) and the "aesthetics" that are listed in the `aes` argument. In this case, we defined an x (horizontal) variable as the `pclass` variable itself and the y (vertical) variable as a proportion (which in ggplot-speak is ..prop..). The `group=1` argument is a trick for barplots that makes sure our proportions add up to 1 across all categories. 
2. `geom_bar()`: This command defines what geometric shape (in this case a bar) to actually plot. These two layers would be enough for a figure, but I also add four more layers that make for a nicer looking figure. You can try the command above with only the first two elements to see how it is different. 
3. `labs(x="passenger class", y="percent")`: The `labs` command allows me to add a variety of nice labels to my graph. In this case I labeled my x and y axes. 
4. `scale_y_continuous(labels=scales::percent)`: This is not a necessary command but it is useful to get better labels on my y-axis tickmarks. Without this command, the tickmarks would just show proportions (e.g. 0.2, 0.4). This command allows me define custom labels for those tickmarks that show them as percents rather than proportions. 
5. `coord_flip`: This does exactly what it sounds like. It flips the x and y axes of the graph. This causes the bars to display horizontally rather than vertically. This is not necessary, but is often a useful feature to avoid problems with long category names overlapping on the x-axis. Note that even though passenger class looks like it is on the y-axis, ggplot still treats it as the "x" variable for things like defining aesthetics and labeling with the `labs` layer. 
6. `theme_bw()`: This last command is for the "theme" layer that just defines a variety of characteristics for the overall look of the figure. I like the `theme_bw` (bw for black and white) theme over the default theme that comes with `ggplot`. 

Ggplot is a very flexible system that repeats these same basic layer elements in all the graphs it creates. In an appendix to this book, you can see cookbook examples of all the ways we will we use it to create figures throughout the term. 

### Looking at the distribution of a quantitative variable

Barplots won't work for quantitative variables because quantitative variables don't have categories. However, we can do something quite similar with the **histogram**. 

One way to think about the histogram is that we are imposing a set of categories on a quantitative variable by breaking our quantitative variable into a set of equally wide intervals that we call bins. The most important decision with a quantitative variable is how wide to make these bins. As an example, lets take the age variable from our politics dataset. I could break this variable into five-year intervals that go from 0-5, 5-10, 10-15, 15-20, and so on. Alternatively, I could use 10-year intervals from 0-9, 10-19, 20-29, and so on. I could also use any other interval I like such as 1-year, 3-year, and so on. Lets use 10-year intervals for this example. I then just have to count up the number of observations that fall into each 10 year interval. 

```{r age-dist-table, echo=FALSE}
x <- as.numeric(table(cut(politics$age, seq(from=0,to=90,by=10))))
nam <- (paste(seq(from=0,to=80,by=10),seq(from=9,to=89,by=10),sep="-"))
tab <- cbind(nam,x)
colnames(tab) <- c("Age Group","Frequency")
if(is_html) {
  kable(tab, align=c("l","r"),
        caption="Age distribution in politics datatset in 10 year groups") %>%
    kable_styling(bootstrap_options = c("striped","hover"),
                  full_width = FALSE)
} else {
  kable(tab, "latex", booktabs=TRUE, align=c("l","r"),
        caption="Age distribution in politics datatset in 10 year groups")
}
```

Because the survey was only administered to adults, I have zero individuals from 0-9 and only a few in the 10-19 age range.

Now that we have the frequencies for each bin, we can plot the histogram. The histogram looks much like a barplot except for two important differences. First, on the x-axis we have a numeric scale for the quantitative variable rather than categories. Second, we don't put any space between our bars. 

Below I show some *R* code for producing the histogram shown in Figure \@ref(fig:hist-agepol) using `ggplot`. This code is simpler than the barplot code. We only need to define an x aesthetic in our `ggplot` command. Instead of a `geom_bar` we use a `geom_histogram`. I can specify different bin widths in this command with the `binwidth` argument. Here I have specified a binwidth of ten years. I have also specified two different colors. The `col` argument is the color of the border for each bar and the `fill` argument is the color of the bars themselves. 

```{r hist-agepol, fig.cap='Histogram of age in the politics data', fig.align='center'}
ggplot(politics, aes(x=age))+
  geom_histogram(binwidth=10, col="black", fill="grey")+
  labs(x="age")+
  theme_bw()
```

Looking at Figure \@ref(fig:hist-agepol), I can see the peak in the distribution in the 50's (the baby boomers) with a long fat tail to the right and a steeper drop off at older ages, due to smaller cohorts and old age mortality.I can also sort of see a smaller peak in the 25-34 range which is largely the children of the baby boomers. 

What would this histogram look like if I had used 5-year bin widths instead of 10-year bins? Lets try it:

```{r hist-agepol-bin5, fig.cap='Histogram of age in the politics data with five year bins', fig.align='center'}
ggplot(politics, aes(x=age))+
  geom_histogram(binwidth=5, col="black", fill="grey")+
  labs(x="age")+
  theme_bw()
```

As Figure \@ref(fig:hist-agepol-bin5) shows, I get more or less the same overall impression but a more fine-grained view. I can more easily pick out the two distinct "humps" in the distribution that correspond to the baby boomers and their children. Sometimes adjusting bin width can reveal or hide important trends and sometimes it can just make it more difficult to visualize the distribution. As an exercise, you can play around with the interactive example below. 

```{r shiny-app-histogram, echo=FALSE}
knitr::include_app("https://aarongullickson.shinyapps.io/histogram/", height="800px")
```


What kinds of things should you be looking for in your histogram? There are four general things to be on the lookout for when you examine a histogram.

#### Center

Where is the center of the distribution? Loosely we can think of the center as the peak in the data, although we will develop some more technical terms for center in the next section. Some distributions might have more than one distinct peak. When a distribution has one peak, we call it a **unimodal** distribution. Figure \@ref(fig:hist-unimodal) shows a clear unimodal distribution for the runtime of movies. We can clearly see that the peak is somewhere between 90 and 100 minutes. When a distribution has two distinct peaks, we call it a **bimodal** distribution. Figure \@ref(fig:hist-bimodal) shows that the distribution of violent crime rates across states is bimodal. The first peak is around 2200-2700 crimes per 100,000 and the second is around 3500 crimes per 100,000. This suggests that there are two distinctive clusters of states: in the first cluster are states with a moderate crime rate and in the second cluster, states with high crime rate states. You can probably guess what we call a distribution with three peaks (and so on), but its fairly rare to see more than two distinct peaks in a distribution.

```{r hist-unimodal, message=FALSE, warning=FALSE, echo=FALSE, fig.cap='The distribution of movie runtimes is unimodal with one clear peak around 90-100 minutes', fig.align='center'}
ggplot(movies, aes(x=Runtime))+
  geom_histogram(binwidth=5, color="black", fill="lavender")+
  labs(x="Movie Runtime (in minutes)")+
  theme_bw()
```

```{r hist-bimodal, message=FALSE, warning=FALSE, echo=FALSE, fig.cap='The distribution of property crimes by states is bimodal with a two separate peaks.', fig.align='center'}
ggplot(crimes, aes(x=Property))+
  geom_histogram(binwidth=250, color="black", fill="firebrick")+
  labs(x="Property crimes per 100,000")+
  annotate(geom="text", x=4750, y=1.5, label="Washington\nDC", size=3)+
  theme_bw()
```

#### Shape

Is the shape symmetric or is one of the tails longer than the other? When the  long tail is on the right, we refer to this distribution as **right skewed**. When the long tail is on the left, we refer to the distribution as **left skewed**. Figures \@ref(fig:hist-symmetric) and \@ref(fig:hist-skewed) show examples of roughly symmetric and heavily right-skewed distributions, respectively, from the movies dataset. The Tomato Rating that movies receive (a score from 1-10) is roughly symmetric with about equal numbers of movies above and below the peak. Box office returns on the other hand are heavily right-skewed. Most movies make less than \$100 million at the box office but there are few "blockbusters" that rake in far more. Right-skewness to some degree or another is common in social science data, partially because many variables can't logically have values below zero and thus the left tail of the distribution is truncated. Left-skewed distributions are rare. In fact, they are so rare that I don't really have a very good example to show you from our datasets. 


```{r hist-symmetric, message=FALSE, warning=FALSE, echo=FALSE, fig.cap='The distribution of movie tomato ratings is roughly symmetric', fig.align='center'}
ggplot(movies, aes(x=TomatoRating))+
  geom_histogram(binwidth=0.5, color="black", fill="skyblue")+
  labs(x="Movie Tomato Rating")+
  theme_bw()
```

```{r hist-skewed, message=FALSE, warning=FALSE, echo=FALSE, fig.cap='The distribution of movie box office returns is heavily right skewed', fig.align='center'}
ggplot(movies, aes(x=BoxOffice))+
  geom_histogram(binwidth=20, color="black", fill="darkgreen")+
  labs(x="Movie Box Office Returns (millions USD)")+
  theme_bw()
```

#### Spread

How spread out are the values around the center? Do they cluster tightly around the center or are they spread out more widely? Typically this is a question that can only be asked in relative terms. We can only say that the spread of a distribution is larger or smaller than some comparable distribution. We might be interested for example in the spread of the income distribution in the US compared to Sweden, because this spread is one measure of income inequality.  

Figure \@ref(fig:hist-spread) compares the distribution of movie runtime for comedy movies and sci-fi/fantasy movies. The figure clearly shows that the spread of movie runtime is much greater for sci-fi/fantasy movies. Comedy movies are all tightly clustered between 90 and 120 minutes while longer movies are more common for sci-fi/fantasy movies leading to a longer right tail and a correspondingly higher spread. 

```{r hist-spread, message=FALSE, warning=FALSE, echo=FALSE, fig.cap='The distribution of movie runtime is much more spread out for sci-fi/fantasy films than it is for comedies.', fig.align='center'}
ggplot(movies, aes(x=Runtime, fill=Genre, y=..density..))+
  geom_histogram(data=subset(movies, Genre=="Comedy"), binwidth=10, alpha=0.5)+
  geom_histogram(data=subset(movies, Genre=="SciFi/Fantasy"), binwidth=10, alpha=0.5)+
  annotate(geom="text", x=200, y=0.002, label="Return of\nthe King", size=3)+
  labs(x="Movie runtime (in minutes)")+
  theme_bw()
```

#### Outliers

Are there extreme values which fall outside the range of the rest of the data? We want to pay attention to these values because they may have a strong influence on the statistics that we will learn to calculate to summarize a distribution. They might also influence some of the measures of association that we will learn later. Finally, extreme outliers may help identify data coding errors. 

In Figure \@ref(fig:hist-bimodal) above, we can see a clear outlier in the violent crime rate for Washington DC. Washington DC's crime rate is such an outlier relative to the other states that we will pay attention to it throughout this term as we conduct our analysis. Figure \@ref(fig:hist-spread) above shows that Peter Jackson's Return of the King was an outlier for the runtime of sci-fi/fantasy movies, although it doesn't seem quite as extreme as for the case of Washington DC. 

In large datasets, it can sometimes be very difficult to detect a single outlier on a histogram because the height of its bar will be so small. Later in this chapter, we will learn another graphical technique called the boxplot that can be more useful for detecting outliers in a quantitative variable.


---

## Measuring the Center of a Distribution

When we look at a distribution, we often can get an intuitive sense of where its "center" is. But what do we really mean by the term "center?" The notion of center often allows us to think about the value we expect a typical or "average" observation to have, but there are multiple ways of defining this center. In statistics, three different measures of center are used: the **mean**, **median**, and **mode**. 

### The mean

The mean is the measure most frequently referred to as the "average" although that term could apply to the median and mode as well. The mean is the **balancing point** of a distribution. Imagine trying to balance a distribution on your finger like a basketball. Where along the distribution would you place your finger to achieve this balance? This point is the mean. It is equivalent to the concept of "center of mass" from physics. 

The calculation of the mean is straightforward:

1. Sum up all the values of your variable across all observations. 
2. Divide this sum by the number of observations.

As an example, lets take a sub-sample of our movie data. I am going to select all the romances (not including rom-coms which are coded as comedies) produced in 2013. There were nine "pure" romances in 2013. I want to know their mean Tomato Meter rating. 

```{r romance-table-mean, echo=FALSE}
tab <- movies[movies$Genre=="Romance" & movies$Year==2013,c("Title","TomatoMeter")]
rownames(tab) <- NULL
tab <- rbind(tab, c("Sum",sum(tab$TomatoMeter)))
colnames(tab) <- c("Title","Tomato Meter")
if(is_html) {
  kable(tab, align=c("l","r"),
        caption="Tomato meter of pure romance movies released in 2013") %>%
    kable_styling(bootstrap_options = c("striped"), full_width = FALSE) %>%
    row_spec(10, bold=TRUE, color="white", background="#666")
} else {
  kable(tab, "latex", booktabs=TRUE, align=c("l","r"),
        caption="Tomato meter of pure romance movies released in 2013") %>%
    row_spec(10, bold=TRUE)
}
```

In the very last row, I show the sum of the Tomato Meter rating which simply sums up the Tomato Meter rating of each movie. To calculate the mean, I simply divide this sum by the number of movies which is 9.

$$\bar{x}=\frac{460}{9}=51.11$$

The mean Tomato Meter rating for romantic movies in 2013 was 51.11. Notice the funny $x$ with a bar over it ("x bar"). Mathematically, we often represent variables with lower-case roman letters like $x$ or $y$. Putting the bar above the letter is the way to mathematically signify the mean of that variable. 

Since we are discussing math symbols, lets talk about creating a mathematical formula for what we just did. In order to do that, I need to introduce a variety of mathematical symbols, but don't get frightened. We are just formalizing the method of calculating the mean that I just demonstrated above. 

First, as I said we represent a given variable by a lower-case roman letter, such as $x$. If we want to specify a particular observation of $x$, we use a subscript number. So $x_1$ is the value of the first observation of $x$ in our data and $x_{25}$ is the value of the 25th observation of $x$ in our data. We use the letter n to signify the number of observations so $x_n$ is always the last observation of $x$ in our data. 

Now we need some way to indicate "sum up all the values of $x$." This is given by the summation sign which looks as follows:

$$\sum_{i=1}^n x_i$$

In English, this just means "sum up all the values of $x$, starting at $x_1$ and ending at $x_n$." That gives us our sum, which we just need to divide by the number of observations, $n$. 

$$\bar{x}=\frac{\sum_{i=1}^n x_i}{n}$$

In *R*, the mean is very straightforward to calculate. Lets calculate the mean of the Tomato Meter rating for all movies in our dataset:

```{r mean-by-hand}
sum(movies$TomatoMeter)/nrow(movies)
```

The `sum` command calculates the sum and the `nrow` command calculates the number of rows of our dataset, which is equivalent to the number of observations. Or we could go even simpler and just use the `mean` command:

```{r mean-tomato-meter}
mean(movies$TomatoMeter)
```

We can see that the mean Tomato Meter of romantic movies in 2013 (51.1) were slightly above the mean for all the movies in our dataset (`r round(mean(movies$TomatoMeter),1)`), but not by much.

One last thing to note is that it only makes sense to calculate the mean of quantitative variables. You cannot add up the values for categorical variables because categorical values don't have numeric values. Rather, they have categories. Notice that you will get an "NA" value and a warning if you try to do this using the `mean` command:

```{r mean-categorical, warning=TRUE}
mean(movies$Rating)
```

### The median

The median is almost as widely used as the mean as a measure of center, for reasons I will discuss below. The median is the **midpoint** of the distribution. It is the point at which 50% of observations have lower values and 50% of the observations have higher values. 

In order to calculate the median, we need to first order our observations from lowest to highest. Lets do that with the romantic movie data above. 

```{r romance-movies-median, echo=FALSE}
tab <- movies[movies$Genre=="Romance" & movies$Year==2013,c("Title","TomatoMeter")]
tab <- tab[order(tab$TomatoMeter),]
tab$Order <- 1:nrow(tab)
colnames(tab) <- c("Title","Tomato Meter", "Order")
rownames(tab) <- NULL
if(is_html) {
  kable(tab, align=c("l","r","r"),
        caption="Tomato meter of pure romance movies released in 2013, sorted from worst to best") %>%
    kable_styling(bootstrap_options = c("striped"), full_width = FALSE) %>%
    row_spec(5, bold=TRUE, color="white", background="#666")
} else {
  kable(tab, "latex", booktabs=TRUE, align=c("l","r","r"),
        caption="Tomato meter of pure romance movies released in 2013, sorted from worst to best") %>%
    row_spec(5, bold=TRUE)
}
```

To find the median, we have to find the observation right in the middle. When we have an odd number of observations, finding the exact middle is easy. In this case, it is given by the 5th observation because it has four observations lower than it and four observations higher than it. So the median Tomato Meter rating for romantic movies in 2013 was 49. 

When you have an even number of observations, then finding the median is slightly more tricky because you have no single observation that is exactly in the middle. In this case, you find the two observations that are closest to the middle and calculate their mean (sum them up and divide by two). For example, if we had ten observations, we would take the mean of the 5th and 6th observations to calculate the median. 

The `median` command in *R* will calculate the median for you.

```{r median-command}
median(movies$TomatoMeter)
```

In this particular case, the mean (`r round(mean(movies$TomatoMeter),1)`) and median (`r round(median(movies$TomatoMeter),1)`) of the Tomato Meter produced very similar measures of center, but this isn't always the case as I will demonstrate below. 

As for the mean, it makes no sense to calculate the median of a categorical variable.

### The mode

The mode is the least used of the three measures of center. In fact, it is so infrequently used that R does not even have a built-in function to calculate it. The mode is the **high point** or **peak** of the distribution. When you look at a distribution graphically, the mode is what your eye is drawn to as a measure of center. Calculating the mode however is much trickier. Simply speaking, the mode is the most common value in the data. However, when data are recorded down to very precise decimal levels, this can be a misleading number. Furthermore, there may be multiple "peaks" in the data and so speaking of a single mode can be misleading. In the sample of 2013 romance movies, no value was repeated and so there is no legitimate value for the mode. 

In the case of the tomato meter distribution for the movies dataset, we have a tie for the most common value. 41 movies had Tomato Meter ratings of 29, 33, and 51, respectively. Figure \@ref(fig:hist-tomato-meter) shows that the distribution of tomato meter ratings lacks any real peak, with a relatively even distribution across most values.

```{r hist-tomato-meter, fig.cap='Histogram of Tomato Meter ratings from politics dataset with three candidates for the mode drawn in red', fig.align='center'}
ggplot(movies, aes(x=TomatoMeter))+
  geom_histogram(binwidth=5, col="black", fill="grey")+
  labs(x="age")+
  geom_vline(xintercept = c(29,33,51), size=2, color="red")+
  theme_bw()
```

Interestingly, while it does not make sense to think about a mean or a median for quantitative variables, we can use the idea of the mode for categorical variables. The most common category is often referred to as the **modal category**. If you want to quickly identify the modal category for a categorical variable, you can wrap a `table` command inside a `sort` command with the `decreasing=TRUE` argument to see the top category at the right.

```{r modal-category}
sort(table(movies$Rating), decreasing = TRUE)
```

R-Rated movies are the modal category for maturity rating in our movies dataset. 

### Comparing the mean and median

How can the mean and median give different results? Remember that the mean defines the **balancing point** and the median defines the **midpoint**. If you have a perfectly symmetric distribution, then these two points are the same because you would balance the distribution at the midpoint. However, when the distribution is skewed in one direction or another, the mean and the median will be different. In order to maintain balance, the mean will be pulled in the direction of the skew. When you have heavily skewed distributions, this can lead to dramatically different values for the mean and median. 

Lets look at this phenomenon for a couple of variables in the movie dataset. As the histogram above showed, the Tomato Meter variable is fairly symmetric and as a result we end up with a mean (`r round(mean(movies$TomatoMeter),1)`) and median (`r round(median(movies$TomatoMeter),1)`) that are pretty close. Figure \@ref(fig:skew-runtime) shows the distribution of movie runtime which is somewhat more right-skewed.

```{r skew-runtime, echo=FALSE, fig.cap="Distribution of movie runtime with mean (105.2) and median (102) shown as vertical lines", fig.align="center"}
ggplot(movies, aes(x=Runtime))+
  geom_histogram(binwidth=5, col="grey60", fill="grey")+
  geom_vline(data = data.frame(value=c(mean(movies$Runtime), median(movies$Runtime)),
                               center=c("mean","median")), 
             aes(xintercept=value, color=center), size=2)+
  labs(x="runtime in minutes", color="measure of center")+
  theme_bw()
```

The skew here is not too dramatic, but it pulls the mean about 3 minutes higher than the median. 

Figure \@ref(fig:skew-boxoffice) shows the distribution of box office returns to movies which is heavily right skewed. Most movies make moderate amounts of money, and then there are a few star performers that make bucket loads of cash.

```{r skew-boxoffice, echo=FALSE, fig.cap="Distribution of movie box office returns with mean (45.2) and median (21.6) shown as vertical lines", fig.align="center"}
ggplot(movies, aes(x=BoxOffice))+
  geom_histogram(binwidth=10, col="grey60", fill="grey")+
  geom_vline(data = data.frame(value=c(mean(movies$BoxOffice), median(movies$BoxOffice)),
                               center=c("mean","median")), 
             aes(xintercept=value, color=center), size=2)+
  labs(x="box office returns (in millions of dollars)", color="measure of center")+
  theme_bw()
```

As a result of this skew, the mean box office returns are about \$45.2 million, while the median box office returns are about \$21.6 million. The mean here is more than double the median!

Note that neither estimate is in some fundamental way incorrect. They are both correctly estimating what they were intended to estimate. It is up to us to understand and interpret these numbers correctly and to understand their limitations.

In many cases, we are actually more interested in the median as a measure of "average" experience than the mean, even though we think of the mean as the "average." This is, for example, why you see home prices in an area always reported in terms of medians rather than means. Mean home prices tend to be much higher than median home prices because of the relatively few very expensive homes in a given area. 

---

## Percentiles and the Five Number Summary

In this section, we will learn about the concept of **percentiles**. Percentiles will allow us to calculate a **five number summary** of a distribution and introduce a new kind of graph for describing a distribution called the **boxplot**.

### Percentiles

We have already seen one example of a percentile. The median is the 50th percentile of the distribution. It is the point at which 50% of the observations are lower and 50% are higher. We can actually use this same logic to calculate other percentiles. We could calculate the 25th percentile of the distribution by finding the point where 25% of the observations are below and 75% are above. We could even calculate something like the 43rd percentile if we were so inclined. 

We calculate percentiles in a fashion similar to the median. First, sort the data from lowest to highest. Then, find the exact observation where X% of the observations fall below to find the Xth percentile. In some cases, there might not be an exact observation that fits this description and so you may have to take the mean across the two closest numbers. 

The `quantile` command in R will calculate percentiles for us in this fashion (quantile is a synonym for percentile). In addition to telling the quantile command which variable we want the percentiles of, we need to tell it which percentiles we want.  In the command below, I ask for the 27th and 57th percentile of age in our sexual frequency data. 

```{r quantile-example}
quantile(sex$age, p=c(.27,.57))
```

27% of the sample were younger than 32 years of age and 57% of the sample were younger than 46 years of age. 

### The five number summary

We can split our distribution into quarters by calculating the minimum(0th percentile), the 25th percentile, the 50th percentile (the median), the 75th percentile, and the maximum (100th percentile). Collectively, these percentiles are known at the **quartiles** of the distribution (not to be confused with quantile) and are also described as the **five number summary** of the distribution. 

 We can calculate these quartiles with the `quantile` command. If I don't enter in specific percentiles, the `quantile` command will give me the quartiles by default:

```{r quantile-age-sex}
quantile(sex$age)
```

The bottom 25% of respondents are between the ages of 18-31. The next 25% are between the ages of 31-43. The next 25% are between the ages of 43-56. The top 25% are between the ages of 56-89.

We can also use this five number summary to calculate the **interquartile range** (IQR) which is just the difference between the 25th and 75th percentile. This gives us a sense of how spread out observations are. In this data:

$$IQR=56-31=25$$

So, the 25th and 75th percentile of age are separated by 25 years. 

### Boxplots

We can also use this five number summary to create another graphical representation of the distribution called the boxplot. Figure \@ref(fig:boxplot-age-sex) below shows a boxplot for the age variable from the sexual frequency data.

```{r boxplot-age-sex, echo=FALSE, fig.cap="Boxplot of respondent's age in sexual frequency data"}
ggplot(sex, aes(x="", y=age))+
  geom_boxplot(fill="skyblue")+
  labs(x=NULL)+
  theme_bw()
```

The "box" in the boxplot is drawn from the 25th to the 75th percentile. The height of this box is equal to the interquartile range. The median is drawn as a thick bar within the box. Finally, "whiskers" are then drawn to the minimum and maximum of the data. Sometimes, the whiskers are drawn to less than the minimum and maximum if these values are very extreme and instead the whiskers are drawn out to 1.5xIQR in length and then individual points are plotted. In this case there were no extreme values, so the whiskers were drawn all the way out to the actual maximum and minimum. 

The boxplot provides many pieces of information. It shows the center of the distribution as measured by the median. It also gives a sense of the spread of the distribution and extreme values by the height of the box and whiskers. It can also show skewness in the distribution depending on where the median is drawn within the box and the size of the whiskers. If the median is in the center of the box, then that indicates a symmetric distribution. If the median is towards the bottom of the box, then the distribution is right-skewed. If the median is towards the top of the box, then the distribution is left-skewed.  

I have not yet shown you how to make a boxplot using ggplot. The code for Figure \@ref(fig:boxplot-age-sex) is shown below. 

```{r boxplot-code, eval=FALSE}
ggplot(sex, aes(x="", y=age))+
  geom_boxplot(fill="skyblue")+
  labs(x=NULL)+
  theme_bw()
```

Most of this code is straightforward. We use the `y` aesthetic to indicate the variable we want the boxplot for and we use the `geom_boxplot` command to graph the boxplot (in this case the `fill` argument can be used to specify a color choice for the box of the boxplot). The only unusual thing here is the use of `x=""` in the top-level aesthetics and the use of `x=NULL` in the `labs` command. These additions are not strictly necessary but they do cause the horizontal x-scale on the graph to be suppressed. Otherwise we would see some non-intuitive numbers here.

The exercise below allows you to adjust a slider to see different percentiles on both a histogram and a boxplot. 

```{r shiny-app-percentiles, echo=FALSE}
knitr::include_app("https://aarongullickson.shinyapps.io/percentiles/", height="800px")
```

In general, boxplots for a single variable do not contain as much information as a histogram and so are generally inferior for understanding the full shape of the distribution. The real advantage of boxplots will come in the next module when we learn to use comparative boxplots to make comparisons of the distribution of a quantitative variable across different categories of a categorical variable. 

---

## Measuring the Spread of a Distribution

The second most important measure of a distribution is its spread. Spread indicates how far individual values tend to fall from the center of the distribution. As Figure \@ref(fig:spread-compare) below shows, two distributions can have the same center and general shape (in this case, a bell curve) but have very different spreads.

```{r spread-compare, echo=FALSE, fig.cap="Two different distributions with the same mean but very different spreads, based on simulated data."}
#par(mfrow=c(1,2), mar=c(1,1,1,1))
temp <- data.frame(x=c(rnorm(10000,10,1),
                       rnorm(10000,10,0.5)),
                   type=c(rep("wide",10000),
                          rep("narrow",10000)))
ggplot(temp, aes(x=x, fill=type))+
  geom_histogram(data=subset(temp, type=="narrow"), alpha=0.7, binwidth=0.2)+
  geom_histogram(data=subset(temp, type=="wide"), alpha=0.7, binwidth=0.2)+
  guides(fill=FALSE)+
  labs(x="some interesting variable")+
  theme_bw()
```

### Range and interquartile range

One of the simplest measures of spread is to calculate the **range**. The range is the distance between the highest and lowest value. Lets take a look at the range in the fare paid (in British pounds) for tickets on the Titanic. The `summary` command, will give us the information we need:

```{r calculate-range}
summary(titanic$fare)
```

Note that at least one person made it on the Titanic for free. The highest fare paid was 512.3 pounds. So the range is easy to calculate 512.3 - 0 = 512.3. The difference between the highest and lowest paying passenger was about 512 pounds. 

This example also reveals the shortcoming of the range as a measure of spread. If there are any outliers in the data, they are going to show up in the range and so the range may give you a misleading idea of how spread out the values are. Note that the 75th percentile here is only 31.28 pounds, which would suggest that the 512.3 maximum is a pretty high outlier. We can check this by graphing a boxplot, as I have done in Figure \@ref(fig:boxplot-fare). 

```{r boxplot-fare, fig.cap="Boxplot of fare paid on the Titanic"}
ggplot(titanic, aes(x="", y=fare))+
  geom_boxplot(fill="seagreen", outlier.color = "red")+
  labs(x=NULL, y="fare paid in British pounds")+
  theme_bw()
```

The maximum value is such an outlier that the rest of the boxplot has to be "scrunched" in order to fit it all into the graph. Clearly this is not a good indicator of spread. However, we have already seen a better measure of spread using a similar idea: the **interquartile range** or **IQR**. The IQR is just the range between the 25th and 75th percentile. We already have these numbers from the output above, so the IQR = 31.28-7.90=23.38. So, the difference in fare between the 25th and 75th percentile (the middle 50% of the data) was 23.4 pounds. That result suggests a much smaller spread of fares. 

You can also use the `IQR` command in *R* to directly calculate the IQR. 

```{r iqr-command}
IQR(titanic$fare)
```

### Variance and standard deviation

The most common measure of spread is the **variance** and its derivative measurement, the **standard deviation**. It is so common in fact, that most people simply refer to the concept of "spread" as "variance." The variance can be defined as the "average squared distance to the mean." Of course, "squared distance" is a bit hard to think about, so we more commonly take the square root of the variance to get the standard deviation which gives us the "average distance to the mean." Imagine if you were to randomly pick one observation from your dataset and guess how far it would be from the mean. Your best guess would be the standard deviation. 

The calculation for the variance and standard deviation is a bit intimidating but we will break it down into steps to show it is not that hard. At the same time, I will show you to calculate the parts in R using the fare variable from the Titanic data. 

The overall formula for the variance (which is represented as $s^2$) is:

$$s^2=\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n-1}$$

That looks tough, but lets break it down. The first step is this:

$$(x_i-\bar{x})$$

You take each value of your variable $x$ and subtract the mean from it. This can be done in *R* easily:

```{r}
diffx <- titanic$fare-mean(titanic$fare)
```

This measure gives us a description of how far each observation is from the mean which is already kind of a measure of spread, but we can't do much with it yet because some differences are positive (higher than the mean) and some are negative (lower than the mean). In fact, if we take the mean of these differences, it will be zero by definition because this is what it means for the mean to be the balancing point of the distribution.

```{r}
round(mean(diffx),5)
```

The next step is:

$$(x_i-\bar{x})^2$$
  
We just need to square the differences. This will get rid of our negative/positive problem, because the squared values will all be positive.

```{r}
diffx.squared <- diffx^2
```

The next step is:

$$\sum_{i=1}^n (x_i-\bar{x})^2$$

We need to sum up all of our values. This value is sometimes  called the **sum of squared X** or **SSX** for short. It is already pretty close to a measure of variance already. 

```{r}
ssx <- sum(diffx.squared)
```

The more distance there is from the mean on average, the larger this value will be. However, it also gets larger when we have more values because we are just taking a sum. To get a number that is comparable across different number of observations, we need to do the final step:

$$s^2=\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n-1}$$

We are going to divide our SSX value by the number of observations minus one. The "minus one" thing is a bit tricky and I don't want to get into the details of why we do it here. When n is large, this will have little effect and basically you are taking an average of the squared distance from the mean.

```{r}
variance <- ssx/(length(diffx)-1)
variance
```

So the average squared distance from the mean fare is 2677.39 pounds squared. Of course, this isn't a very interpretable number, so its probably better to square root it and get the standard deviation:

```{r}
sqrt(variance)
```

So, the average distance from the mean fare is 51.74 pounds. 

Note that I could have used the power of *R* to do this entire calculation in one line:

```{r}
sqrt(sum((titanic$fare-mean(titanic$fare))^2)/(length(titanic$fare)-1))
```

Alternatively, I could have just used the `sd` command to have *R* do all the heavy lifting:

```{r}
sd(titanic$fare)
```