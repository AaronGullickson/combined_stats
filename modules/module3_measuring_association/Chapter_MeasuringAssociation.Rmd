#Measuring Association

------

##The Two-Way Table

The **two-way table** (also known as a **cross-tabulation** or **crosstab**) gives the joint distribution of two categorical variables. Lets use our politics dataset to construct a two-way table of belief in anthropogenic climate change by political party:

```{r}
tab <- table(politics$party, politics$globalwarm)
tab
```

The two-way table gives us the **joint distribution** of the two variables, which is the number of respondents who fell into both categories. For example, we can see that 429 democrats did not believe in anthropogenic climate change while 1932 did. 

From this table, we can also calculate the **marginal distribution** of each of the variables, which are just the distributions of each of the variables separately. We can do that by adding up across the rows and down the columns:

```{r echo=FALSE}
row.margin <- paste(tab[,1],"+",tab[,2],"=",tab[,1]+tab[,2],sep="")
col.margin <- c(paste(tab[1,],"+",tab[2,],"+",tab[3,],"+",tab[4,],"=",tab[1,]+tab[2,]+tab[3,]+tab[4,],sep=""), sum(tab))
tab.full <- cbind(tab, row.margin)
tab.full <- rbind(tab.full, col.margin)
colnames(tab.full) <- c("Deniers", "Believers", "Total")
rownames(tab.full) <- c(rownames(tab)[1:4], "Total")
set.alignment(c("right","right","right"))
emphasize.italics.rows(5)
emphasize.italics.cols(3)
pander(tab.full)
```

The marginal distribution of party affiliation is given by the Total column on the right and the marginal distribution of climate change belief is given by the Total row at the bottom. Looking at the column marginal, we can see that there were a total of 2361 Democrats, 1389 Republicans, and so on. Looking at the row marginal, we can see that there were 11814 anthropogenic climate change deniers and 4100 anthropogenic climate change believers. The final number (5914) in the far right corner is the total number of respondents altogether. You can get this number by summing up the column marginals (1814+4100) or row marginals (2361+1389+2003+161). 

The `margin.table` command in *R* will also calculate marginals for us. I can use the `margin.table` command on the table I created and saved above as *tab*  to calculate the same marginals as above. Note that you need to indicate which marginal you want by a number, where 1=row and 2=column, as the second option to `margin.table`:

```{r}
margin.table(tab,1)
margin.table(tab,2)
```

The two-way table provides us with evidence about the association between two categorical variables. To understand what the association looks like, we will learn how to calculate **conditional distributions**. 

###Conditional distributions

To this point, we have learned about the **joint** and **marginal** distributions in a two-way table. In order to look at the relationship between two categorical variables, we need to understand a third kind of distribution: the **conditional distribution**. The conditional distribution is the distribution of one variable conditional on being in a certain category of the other variable. In a two-way table, there are always two ways to calculate a conditional distribution. In our case, we could look at the distribution of climate change belief conditional on party affiliation, or we could look at the distribution of party affiliation conditional on climate change belief. Both of these distributions really give us the same information about the association, but sometimes one way is more intuitive to understand. In this case, I am going to start with the former case and calculate the distribution of climate change belief conditional on party affiliation.

This conditional distribution is basically given by the rows of our two-way table, which give the number of individuals of a given party who fall into each belief category. For example, the distribution of denial/belief among Democrats is 429 and 1932, while among Republicans, this distribution is 708 and 681. However, these two rows are not directly comparable as they are because Republicans are a much smaller group than Democrats. Thus, even if the shares were very different between the two groups, the absolute numbers for Republicans would probably be smaller for both categories. In order to make these rows comparable, we need the proportion of each party that falls into each belief category. In order to do that, we need to divide our rows through by the marginal distribution of party affiliation, like so: 

```{r echo=FALSE}
row.margin <- margin.table(tab,1)
row.margin <- t(matrix(rep(row.margin, each=2),2,4))
tab.cond <- matrix(paste(tab, row.margin, sep="/"), dim(tab))
tab.cond <- cbind(tab.cond, row.margin[,1])
rownames(tab.cond) <- rownames(tab)
colnames(tab.cond) <- c("Deniers","Believers","Total")
set.alignment(c("right","right","right"))
pander(tab.cond)
```

Note that each row gets divided by its row marginal. If we do the math here, we will come out with the following proportions:

```{r echo=FALSE}
tab.prop <- round(prop.table(tab,1),4)
tab.prop <- cbind(tab.prop,rep(1,4))
colnames(tab.prop) <- c("Deniers","Believers","Total")
set.alignment(c("right","right","right"))
pander(tab.prop)
```

Note that the proportions should add up to 1 within each row because we are basically calculating the share of each row that belongs to each column category. **To understand these conditional distributions, you need to look at the numbers within each row.** For example, the first row tells us that 18% of Democrats are deniers and 82% of Democrats are believers. The second row tells us that 51% of Republicans are deniers and 49% of Republicans are believers.

We can tell if there is an association between the row and column variable if these conditional distributions are different across rows. In this case, they are clearly very different. About 82% of Democrats are believers while only about half (49%) of Republicans are believers. About 70% of Independents are believers, while the proportions of other parties are very similar to those for Republicans. 

R provides a function called `prop.table` that can estimate these conditional distributions. The syntax of `prop.table` is very similar to `margin.table`:

```{r}
prop.table(tab,1)
```

Its important to remember which way you did the conditional distribution and get the interpretation correct. If you are not sure, just note which way the proportions add up to one - this is the direction you should be looking (i.e. within row or column). In this case, I am looking at the distribution of variables within rows, so the proportions refer to the proportion of respondents from a given political party who hold a given belief. But, I could have done my conditional distribution the other way:

```{r}
prop.table(tab,2)
```

Note that this table looks deceptively similar to the table above. But look again. The numbers now don't add up to one within each row. They do however add up to one within each column. In order to read this table properly, we have to understand that it is giving us the distribution within each column: the proportion of respondents who have a given belief who belong to a given political party. So we can see in the first number that 23.6% of deniers are Democrats, 39.0% are Republicans, 32.9% are Independents, and 4.5% belong to other parties. This distribution is very different from the party affiliation distribution of believers in the second column which tells us that there is an association. However, the large party cleavages on the issue are not as immediately obvious here as they were with the previous conditional distribution. Always think carefully about which conditional distribution is more sensible to interpret and always make sure that you are interpreting them in the correct way. 

It is also possible to graph the conditional distribution as a set of barplots. First, lets save the output of our prop.table into a new object.

```{r}
distBeliefByParty <- prop.table(tab,1)
```

We can then use the `barplot` command to graph these distributions. However, there is one "gotcha" that we need to be aware of when running this command. When looking at conditional distributions across rows, barplot will misinterpret our data because it expects it to be oriented so things sum to one down the columns. We can however easily fix this with the `t` command ("t"" for "transpose"") which rotates our results:

```{r barplot_climatebelief, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
barplot(t(distBeliefByParty), legend.text=c("Denial", "Belief"), las=1, cex.names=0.8)
```

Note that when we use barplot like this, it gives us each party in one bar and then shades the bar to show the share of each response. I have used the legend.text option here to indicate which shading belongs to which category. 

Another option is to graph the bars for each category next to each other and grouped by party. You can do this with the `beside` option which you set to TRUE: 

```{r braplot_climatebelief_beside, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
barplot(t(distBeliefByParty), beside=TRUE, las=1, col=c("darkgreen","yellow"), cex.names=0.8)
legend(4, 0.8, legend=c("Denial","Belief"), fill=c("darkgreen","yellow"), bty="n")
```

In this case, I have also added custom colors for the two categories and designed my own legend that I can place better than the automated one. 

------

##Odds ratio (Advanced)

We can also use the **odds ratio** to measure the association between two categorical variables.  The odds ratio is not a term that is common in everyday speech but it is a critical concept in all kinds of scientific research. 

Lets take the different distributions of climate change belief for Democrats and Republicans. About 82% of Democrats were believers, but only 49% of Republicans were believers. How can we talk about how different these two numbers are from one another? We could subtract one from the other or we could take the ratio by dividing one by the other. However, both of these approaches have a major problem. Because the percents (and proportions) have minimum and maximum values of 0 and 100, as you approach those boundaries the differences necessarily have to shrink because one group is hitting its upper or lower limit. This makes it difficult to compare percentage or proportional differences across different groups because the overall average proportion across groups will affect the differences in proportion.

Odds ratios are a way to get around this problem. To understand odds ratios, you first have to understand what odds are. All probabilities (or proportions) can be converted to a corresponding odds. If you have a $p$ probability of success on a task, then your odds $O$ of success are given by:

$$O=\frac{p}{1-p}$$

The odds are basically the ratio of the probability of success to the probability of failure. This tells you how many successes you expect to get for every one failure. Lets say a baseball player gets a hit 25% of the time that the player comes up to bat (an average of 250). The odds are then given by:

$$O=\frac{0.25}{1-0.25}=\frac{0.25}{0.75}=0.33333$$

The hitter will get on average 0.33 hits for every one out. Alternatively, you could say that the hitter will get one hit for every three outs. 

Re-calculating probabilities in this way is useful because unlike the probability, the odds has no upper limit. As the probability of success approaches one, the odds will just get larger and larger.

We can use this same logic to construct the odds that a Democratic and Republican respondent, respectively, will be climate change believers. For the Democrat, the probability is 0.82, so the odds are:

$$O=\frac{0.82}{1-0.82}=\frac{0.82}{0.18}=4.56$$

Among Democrats, there are 4.56 believers for every one denier. Among Republicans, the probability is 0.49, so the odds are:

$$O=\frac{0.49}{1-0.49}=\frac{0.49}{0.51}=0.96$$

Among Republicans, there are 0.96 believers for every one denier. This number is very close to "even" odds of 1, which happen when the probability is 50%. 

The final step here is to compare those two odds. We do this by taking their **ratio**, which means we divide one number by the other:

$$\frac{4.56}{0.95}=4.75$$

This is our odds ratio. How do we interpret it? This odds ratio tells us how much more or less likely climate change belief is among Democrats relative to Republicans. In this case, I would say that "the odds of belief in anthropogenic climate change are 4.75 times higher among Democrats than Republicans." Note the "times" here. This 4.75 is a multiplicative factor because we are taking a ratio of the two numbers. 

You can calculate odds ratios from conditional distributions just as I have done above, but there is also a short cut technique called the **cross-product**. Lets look at the two-way table of party affiliation but this time just for Democrats and Republicans. For reasons I will explain below, I am going to reverse the ordering of the columns so that believers come first. 

```{r echo=FALSE}
tab <- cbind(c(1887,672),c(413,698))
colnames(tab) <- c("Believer","Denier")
rownames(tab) <- c("Democrat","Republican")
set.alignment(c("right","right"))
emphasize.strong.cells(rbind(c(1,1),c(2,2)))
emphasize.italics.cells(rbind(c(1,2),c(2,1)))
pander(tab)
```

The two bolded numbers are called the **diagonal** and the two italicized numbers are the **reverse diagonal**. The cross-product is calculated by multiplying the two numbers in the diagonal by each other and multiplying the two numbers in the reverse diagonal together and then dividing the former product by the latter:

$$\frac{1887*698}{672*413}=4.75$$

I get the exact same odds ratio as above, without having to calculate the proportions and odds themselves. This is a useful shortcut for calculating odds ratios. There is one thing to keep in mind, however. The odds ratio that you calculate is always the odds of the first row being in the first column relative to those odds for the second row. Its easy to show how this would be different if I had kept the original ordering of believers and deniers:

```{r echo=FALSE}
tab <- cbind(c(413,698),c(1887,672))
colnames(tab) <- c("Denier","Believer")
rownames(tab) <- c("Democrat","Republican")
set.alignment(c("right","right"))
emphasize.strong.cells(rbind(c(1,1),c(2,2)))
emphasize.italics.cells(rbind(c(1,2),c(2,1)))
pander(tab)
```

$$\frac{672*413}{1887*698}=0.21$$

I get a very different odds ratio, but that is because I am calculating something different. I am now calculating the odds ratio of being a denier rather than a believer. So I would say that the "the odds of denial of anthropogenic climate change among Democrats are only 21% of the odds for Republicans." In other words, the odds of being a denier are much lower among Democrats. 

Note, however, that the information here is the same because the 0.21 here is exactly equal to 1/4.75. In other words, the odds ratio of denial is just the inverted mirror image of the odds ratio of belief. Its just important that you remember that when you calculate the cross-product, you are always calculating the odds ratio of being in the category of the first column, whatever category that may be. 

------

##Mean Differences

Measuring association between a quantitative and categorical variable is fairly straightforward. We want to look for differences in the distribution of the quantitative variable at different categories of the categorical variables. For example, if we were interested in the gender wage gap, we would want to compare the distribution of wages for women to the distribution of wages for men. There are two ways we can do this. First, we can graphically examine the distributions using the techniques we have already developed, particularly the boxplot. Second, we can compare summary measures like the mean across categories.

###Graphically examining differences in distributions

We could compare entire histograms of the quantitative variable across different categories of the categorical variable, but this is often too much information. A cleaner method is to use **comparative boxplots**. Comparative boxplots construct boxplots of the quantitative variable across all categories of the categorical variable and plot them next to each other for easier comparison. Here is an example looking at differences in movie runtime across different movie genres.

```{r compar_box_runtime_genre, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
boxplot(Runtime~Genre, data=movies, col="seagreen", ylab="movie runtime in minutes", las=2)
```

This command introduces a new syntax. The tilde ("~") in the syntax above allows us to relate one variable to another. In this context, it tells the boxplot command to plot separate boxplots of runtime by the categories of genre. 

Comparative boxplots are useful because they allow us to easily compare differences in both the center and spread of distributions at the same time.  The thick bars give us an indication of the median movie runtime for each genre. Here we can clearly see that animated and horror movies have the lowest median runtimes and that mysteries have the longest median runtimes. We can also see that while there is significant variation, some genres like romance, scifi/fantasy, and thriller all have similar median runtimes. 

However, the boxplot also reveals additional information on the spread of runtimes for each genre. This is easiest to see by comparing the height of the boxes (the interquartile range) across genres. In particular, we can see that scifi/fantasy, action, and dramas have the largest variation in movie runtime, while horror and animated movies have very small variation. Clearly, there is more consensus about the appropriate length of horror and animated movies, than there is about action, drama, and scifi/fantasy movies. 

###Comparing differences in the mean

We can also establish a relationship by looking at differences in summary measures. Implicitly, we are already doing this in the comparative boxplot when we look at the median bars across categories. However, in practice it is more common to compare the mean of the quantitative variable across different categories of the categorical variable. In *R*, you can get the mean of one variable at different levels of a categorical variable using the `tapply` command like so:

```{r}
tapply(movies$Runtime, movies$Genre, mean)
```

The `tapply` command takes three options. The first option is the quantitative variable for which we want means. The second option is the categorical variable. The third option is the method we want to run on the quantitative variable, in this case the mean. The output is the mean movie runtime by genre.

We can also display these results graphically using barplots. To make this prettier, I am first going to sort the output from largest to smallest mean runtime:

```{r mean_barplot_runtime_genre, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
mruntime <- tapply(movies$Runtime, movies$Genre, mean)
mruntime <- sort(round(mruntime,1), decreasing=TRUE)
par(mar=c(7,4,4,2),las=2)
b <- barplot(mruntime, col="skyblue", ylab="movie runtime in minutes", ylim=c(0,125))
text(b[,1],mruntime+4,label=paste(mruntime), cex=0.7)
```

Now we can see very clearly how mean movie runtimes compare across different genres. We can also easily calculate the mean difference in movie length across any two genres. For example, lets say we are interested in the difference between action movies (111.9 minutes, on average) and horror movies (97.4 minutes, on average).

$$111.9-97.4=14.5$$

Action movies are, on average, 15 minutes longer than horror movies.

------

##Scatterplot and Correlation Coefficient

The techniques for looking at the association between two quantitative variables are more developed than the other two cases, so we will spend more time on this topic. Additionally, the major approach here of ordinary least squares regression turns out to be a very flexible, extendable method that we will build on later in the term.

When examining the association between two quantitative variables, we usually distinguish the two variables by referring to one variable as the **dependent variable** and the other variable as the **independent variable**. The dependent variable is the variable whose outcome we are interested in predicting. The independent variable is the variable that we treat as the predictor of the dependent variable. For example, lets say we were interested in the relationship between income inequality and life expectancy. We are interested in predicting life expectancy by income inequality, so the dependent variable is life expectancy and the independent variable is income inequality. 

The language of dependent vs. independent variable is causal, but its important to remember that we are only measuring the association. That association is the same regardless of which variable we set as the dependent and which we set as the independent. Thus, the selection of the dependent and independent variable is more about which way it more intuitively makes sense to interpret our results.

###The scatterplot

We can examine the relationship between two quantitative variables by constructing a **scatterplot**. A scatterplot is a two-dimensional graph. We put the independent variable on the x-axis and the dependent variable on the y-axis. For this reason, we often refer generically to the independent variable as x and the dependent variable generically as y. 

To construct the scatterplot, we plot each observation as a point, based on the value of its independent and dependent variable. For example, lets say we are interested in the relationship between the median age of the state population and violent crime in our crime data. Our first observation, Alabama, has a median age of 37.8 and a violent crime rate of 378 crimes per 100,000. We can plot this point on our graph as follows:

```{r scatter_onepoint, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(crimes$MedianAge[1],crimes$Violent[1],
     xlim=c(min(crimes$MedianAge),max(crimes$MedianAge)),
     ylim=c(min(crimes$Violent),max(crimes$Violent)),
     xlab="Median Age",
     ylab="Violent crimes (per 100,000)",
     main="Scatterplot of Median Age and Violent Crime",
     pch=21, bg="red",las=1)
text(crimes$MedianAge[1],crimes$Violent[1],
     label=paste(crimes$State[1],"\n(",
                 round(crimes$MedianAge[1],1),",",
                 round(crimes$Violent[1],0),")",sep=""),
                 pos=4, cex=0.8)
```

If I repeat that process for all of my observations, I will get a scatterplot that looks like:

```{r scatter_age_violent, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(crimes$MedianAge,crimes$Violent,
     xlim=c(min(crimes$MedianAge),max(crimes$MedianAge)),
     ylim=c(min(crimes$Violent),max(crimes$Violent)),
     xlab="Median Age",
     ylab="Violent crimes (per 100,000)",
     main="Scatterplot of Median Age and Violent Crime",
     pch=21, bg="red",las=1)
text(crimes$MedianAge,crimes$Violent,
     label=paste(crimes$State),
                 pos=4, cex=0.8)
```

It is not necessary to show the names of states here, but I have done so here in order to identify individual observations. 

What are we looking for when we look at a scatterplot? There are four important questions we can ask of the scatterplot. First, what is the **direction** of the relationship. We refer to a relationship as **positive** if both variables move in the same direction. if y tends to be higher when x is higher and y tends to be lower when x is lower, then we have a positive relationship. On the other hand, if the variables move in opposite directions, then we have a **negative** relationship.  If y tends to be lower when x is higher and y tends to be higher when x is lower, then we have a negative relationship. In the case above, it seems like we have a generally negative relationship. States with higher median age tend to have lower violent crime rates.

Second, is the relationship **linear**? I don't mean here that the points fall exactly on a straight line (which is part of the next question) but rather does the general shape of the points appear to have any "curve" to it. If it has a curve to it, then the relationship would be non-linear. This issue will become important later, because our two primary measures of association are based on the assumption of a linear relationship. In this case, there is no evidence that the relationship is non-linear.

Third, what is the **strength** of the relationship. If all the points fall exactly on a straight line, then we have a very strong relationship. On the other hand, if the points form a broad elliptical cloud, then we have a weak relationship. In practice, in the social sciences, we never expect our data to conform very closely to a straight line. Judging the strength of a relationship often takes practice. I would say the relationship above is of moderate strength.

Fourth, are there **outliers**? We are particularly concerned about outliers that go against the general trend of the data, because these may exert a strong influence on our later measurements of association. In this case, there are two clear outliers, Washington DC and Utah. Washington DC is an outlier because it has an extremely high level of violent crime relative to the rest of the data. Its median age tends to be on the younger side, so its placement is not inconsistent with the general trend. Utah is an outlier that goes directly against the general trend because it has one of the lowest violent crime rates and the youngest populations. This is, of course, driven by Utah's heavily Mormon population, who both have high rates of fertility (leading to a young population) and whose church communities are able to exert a remarkable degree of social control over these young populations.

####Constructing scatterplots in *R*

You can construct scatterplots in *R* with the `plot` command. At a very basic level, you can just feed in the independent and dependent variables:

```{r scatter_basic1, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(crimes$Poverty, crimes$Property)
```

We can improve on this command with our usual bells and whistles to label x and y axes, titles, etc. We can also use the `pch` option to define different dots for our points. If you pull up the help command `?points`, it will show a list of codes for all the different points you can use. I like to use `pch=21` because it will draw circles where you can color in both the center and the border. For example:

```{r scatter_basic2, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(crimes$Poverty, crimes$Property, 
     xlab="Poverty rate", ylab="Property crime rate", 
     main="scatterplot of poverty rate and property crime", 
     las=1, pch=21, bg="orange", col="blue")
```

Sometimes with large datasets, scatterplots can be difficult to read because of the problem of **overplotting**. This happens when many data points overlap, so that its difficult to see how many points are showing. For example:

```{r scatter_overplot, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(movies$Runtime, movies$TomatoMeter, pch=21, bg="grey", col="grey", las=1,
     xlab="movie runtime", ylab="Tomato Meter")
```

Because so many movies are in that 90-120 minute range it is difficult to distinguish them and thus a little tricky to summarize the relationship. There are more advanced ways we can address this issue, but these are beyond our introductory class. 

Overplotting can also be a problem with discrete variables because these variables can only take on certain values which will then exactly overlap with one another. For example:

```{r scatter_overplot2, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(sex$educ, sex$sexf,
     xlab="years of education", ylab="sexual frequency")
```

Both of these variables are discrete and thus only take certain values, so all we see on the graph is more or less every possible point based on the possible values of education and sexual frequency. Multiple points are plotted directly on top of each other. We can fix this issue more easily by applying the `jitter` command which just adds a random amount to each observation so they don't exactly overlap. You often have to experiment with `jitter` to find just how much randomness (the second option) you have to apply. 

```{r scatter_jitter, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(jitter(sex$educ,5), jitter(sex$sexf,50), 
     xlab="years of education", ylab="sexual frequency",
     pch=21, bg="grey", col="grey")
```

In this case, because of the large number of observations we still have a lot of overplotting. 

###The correlation coefficient

We can measure the association between two quantitative variables with the correlation coefficient, *r*. The formula for the correlation coefficient is:

$$r=\frac{1}{n-1}\sum_{i=1}^n (\frac{x_i-\bar{x}}{s_x}*\frac{y_i-\bar{y}}{s_y})$$

That looks complicated, but lets break it down step by step. We will use the association between median age and violent crimes as our example.

The first step is to subtract the means from each of our x and y variables. This will give us the distance above or below the mean for each variable.

```{r}
diffx <- crimes$MedianAge-mean(crimes$MedianAge)
diffy <- crimes$Violent-mean(crimes$Violent)
```

The second step is to divide these differences from the mean of x and y by the standard deviation of x and y, respectively. 

```{r}
diffx.sd <- diffx/sd(crimes$MedianAge)
diffy.sd <- diffy/sd(crimes$Violent)
```

Now each of your x and y values have been converted from their original form into the **number of standard deviations above or below the mean**. This is often called **standarization**. By doing this, we have put both variables on the same scale and have removed whatever original units they were measured in (in our case, years of age and crimes per 100,000). 

The third step is to to multiply each converted value of x by each converted value of y. 

```{r}
product <- diffx.sd*diffy.sd
```

Why do we do this? First consider this scatterplot of our standardized x and y:

```{r scatter_r_standardized, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
cols <- c("red","blue")
par(mar=c(4,4,0.1,2))
plot(diffx.sd,diffy.sd,
     xlab="SDs from mean of x",  ylab="SDs from mean of y",
     pch=21, bg=cols[(product>0)+1],las=1)
abline(h=0,lty=2)
abline(v=0,lty=2)
text(c(-1.5,1.5),c(-1.5,1.5),labels="Positive",col="blue")
text(c(-1.5,1.5),c(1.5,-1.5),labels="Negative",col="red")
```

Points shown in blue have either both positive or both negative x and y values. When you take the product of these two numbers, you will get a positive product. This is evidence of a positive relationship. Points shown in red have one positive and one negative x and y value. When you take the product of these two numbers, you will get a negative product. This is evidence of a negative relationship.

The final step is to add up all this evidence of a positive and negative relationship and divide by the number of observations (minus one). 

```{r}
sum(product)/(length(product)-1)
```

This final value is our correlation coefficient. We could have also calculated it by using the `cor` command:

```{r}
cor(crimes$MedianAge, crimes$Violent)
```

How do we interpret this correlation coefficient? It turns out the correlation coefficient *r* has some really nice properties. First, the **sign** of *r* indicates the direction of the relationship. If *r* is positive, the association is positive. If *r* is negative, the association is negative. if *r* is zero, there is no association. 

Second, *r* has a **maximum** value of 1 and a **minimum** value of -1. These cases will only happen if the points line up exactly on a straight line, which never happens with social science data. However, it gives us some benchmark to measure the strength of our relationship.  Here are some simulated scatterplots with different *r* in order to help you get a sense of the strength of association for different values of *r*.

```{r correlations_stength, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
library(MASS)
par(mfrow=c(4,3), xaxt="n", yaxt="n", mar=c(1,1,1,1))
r <- seq(from=0.0, length=12, by=0.09)
for(i in 1:length(r)) {
  plot(mvrnorm(300,c(0,0),Sigma=cbind(c(1,r[i]),c(r[i],1))),
       pch=21, bg="grey", main=paste("r =",r[i],sep=" "))
} 
```

Third, *r* is a **unitless** measure of association. It can be compared across different variables and different datasets in order to make a comparison of the strength of association. For example, the correlation coefficient between unemployment and violent crimes is 0.45. Thus, violent crimes are more strongly correlated with unemployment than with median age (0.44>0.30).  The association between median age and property crimes is -0.36, so median age is more strongly related to property crimes than violent crimes (0.36>0.30).  

There are some important cautions when using the correlation coefficient. First, the correlation coefficient will only give a proper measure of association when the underlying relationship is **linear**. if there is a non-linear (curved) relationship, then *r* will not correctly estimate the association. Second, the correlation coefficient can be affected by **outliers**. We will explore this issue of outliers and influential points more in later sections.  

------

##The OLS Regression Line

Lets take another look at the scatterplot we constructed in the previous section that showed the relationship between median age and violent crime rates:

```{r scatter_age_violent2, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(crimes$MedianAge,crimes$Violent,
     xlim=c(min(crimes$MedianAge),max(crimes$MedianAge)),
     ylim=c(min(crimes$Violent),max(crimes$Violent)),
     xlab="median age in state",
     ylab="violent crimes (per 100,000)",
     main="Scatterplot of median age and violent crime rates\nacross US States",
     pch=21, bg="red",las=1)
abline(lm(Violent~MedianAge, data=crimes))
```

Notice that I now have a line plotted through those points. When you were trying to determine the direction of the relationship many of you were probably imagining a line going through the points already. Of course, if we just tried to "eyeball" the best line, we would get many different results. The line I have graphed above, however, is the best fitting line, at least according to some reasonable criteria. It is the best-fitting line because it minimizes the total distance from all of the points collectively to the line. This line is called the **ordinary least squares regression line** ( or OLS regression line, for short) and it is the basis for the most used statistical technique in the social sciences. This fairly simply concept of fitting the best line to a set of points on a scatterplot is the workhorse of social science statistics.

###The Formula for a Line

 Remember the basic formula for a line in two-dimensional space? In algebra, you probably learned something like this:

$$y=a+bx$$

The two numbers that relate $x$ to $y$ are $a$ and $b$. The number $a$ gives the **y-intercept**. This is the value of $y$ when $x$ is zero. The number $b$ gives the **slope** of the line, sometimes referred to as the "rise over the run." The slope indicates the change in $y$ for a one-unit increase in $x$. 

The OLS regression line above also has a slope and a y-intercept. But we use a slightly different syntax to describe this line than the equation above. The equation for an OLS regression line is:

$$\hat{y}_i=b_0+b_1x_i$$

On the right-hand side, we have a linear equation (or function) into which we feed a particular value of $x$ ($x_i$). On the left-hand side, we get not the actual value of $y$ for the $i$th observation, but rather a **predicted value** of $y$. The little symbol above the $y$ is called a "hat" and it indicates the "predicted value of $y$." We use this terminology to distinguish the actual value of $y$ ($y_i$) from the value predicted by the OLS regression line ($\hat{y}_i$). 

The y-intercept is given by the symbol $b_0$. The y-intercept tells us the predicted value of $y$ when $x$ is zero. The slope is given by the symbol $b_1$. The slope tells us the predicted change in $y$ for a one-unit increase in $x$. In practice, the slope is the more important number because it tells us about the association between $x$ and $y$. Unlike the correlation coefficient, this measure of association is not unitless. We get an estimate of how much we expect $y$ to change in terms of its units for a one-unit increase in $x$. 

In the case above, the slope is -25.6 and the y-intercept is 1343.9. We could therefore write the equation like so:

$$\hat{crimerate}_i=1343.9-25.6(medianage_i)$$

We would interpret our numbers as follows. The model predicts that a one-year increase in age within a state is associated with 25.6 fewer violent crimes per 100,000 population, on average. The model predicts that in a state where the median age is zero, the violent crime rate will be 1343.9 crimes per 100,000 population, on average.

There is a lot to digest in these interpretations and I want to return to them in detail, but first I want to address a more basic question. How did I know that these are the right numbers for the best-fitting line? 

###Calculating the Best-Fitting Line

The slope and intercept of the OLS regression line are determined based on addressing one simple criteria: minimize the distance between the actual points and the line. More formally, we choose the slope and intercept that produce the **minimum sum of squared residuals (SSR)**.

A **residual** is the vertical distance between an actual value of y for an observation and its predicted value:

$$residual_i=y_i-\hat{y}_i$$

These residuals are also sometimes called **error terms**, because the larger they are in absolute value, the worse is our prediction. Take a look at the same scatterplot as above but this time with the residuals drawn in red:


```{r scatter_reside, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(crimes$MedianAge,crimes$Violent,
     xlim=c(min(crimes$MedianAge),max(crimes$MedianAge)),
     ylim=c(min(crimes$Violent),max(crimes$Violent)),
     xlab="median age in state",
     ylab="violent crimes (per 100,000)",
     main="Scatterplot of median age and violent crime rates\nacross US States",
     pch=21, bg="grey",las=1)
model <- lm(Violent~MedianAge, data=crimes)
abline(model)
segments(crimes$MedianAge, model$fitted.values, crimes$MedianAge, crimes$Violent,
        col="red")
```

Unless the points all fall along an exact straight line, there is no way for me to eliminate these residuals altogether, but some lines will produce higher residuals than others. What I am aiming to do is minimize the sum of squared residuals which is given by:

$$SSR = \sum_{i=1}^n(y_i-\hat{y}_i)^2$$

I square each residual and then sum them up. By squaring, I eliminate the problem of some residuals being negative and some positive. 

To see how this all works try [this exercise](https://aarongullickson.shinyapps.io/reducerss/.) where you can experiment with trying to find the best-fitting line to minimize the sum of squared residuals for another scatterplot. 

Fortunately, we don't have to figure out the best slope and intercept by trial and error, as in the exercise above. There are relatively straightforward formulas for calculating the slope and intercept. They are:

$$b_1=r\frac{s_y}{s_x}$$

$$b_0=\bar{y}-b_1*\bar{x}$$

The *r* here is the correlation coefficient. The slope is really just a re-scaled version of the correlation coefficient. We can calculate this with the example above like so:

```{r}
slope <- cor(crimes$MedianAge, crimes$Violent)*sd(crimes$Violent)/sd(crimes$MedianAge)
slope
```

I can then use that slope value to get the y-intercept:

```{r}
mean(crimes$Violent)-slope*mean(crimes$MedianAge)
```

We can also estimate the OLS regression line using the `lm` command, which I describe below, but first lets turn to the issue of interpretation. 

###Interpeting Slopes and Intercepts

Learning to properly interpret slopes and intercepts (especially slopes) is the number one most important thing you will learn all term, because of how common the use of OLS regression is in social science statistics. You simply cannot pass the class unless you can interpret these numbers. So take the time to be careful in interpretation here. 

####Interpreting Slopes

In abstract terms, the slope is always the predicted change in $y$ for a one unit increase in $x$. However, this abstract definition will simply not do when you are dealing with specific cases. You need to think about the units of $x$ and $y$ and interpret the slope in concrete terms. There are also a few other caveats to consider.

Take the interpretation I used above for the -25.6 slope of median age as a predictor of violent crime rates. My interpretation was:

> The **model predicts** that a **one year increase in age** within a state **is associated** with **25.6 fewer violent crimes per 100,000 population**, **on average**.

There are multiple things going on in this sentence that need to be addressed. First, lets address the phrase "model predicts." The idea of a model is something we will explore more later, but for now I will say that when we fit a line to a set of points to predict $x$ by $y$, we are applying a model to the data. In this case, we are applying a model that relates $y$ to $x$ by a simple linear function. All of our conclusions are dependent on this being a good model. Prefacing your interpretation with "the model predicts..." highlights this point. 

Second, a "one year increase in age" indicates the meaning of a one unit increase in $x$. Never literally say a "one unit increase in $x$." Think about the units of $x$ and describe the change in $x$ in these terms. 

Third, I use "is associated with" to indicate the relationship. This phrase is intentionally passive. We want to avoid causal language when we describe the relationship. Saying something like "when $x$ increases by one $y$ goes up by $b_1$" may sound more intuitive, but it also implies causation. The use of "is associated with" here indicates that the two variables are related without implicitly implying that one causes the other. Using causal language is the most common mistake in describing the slope. 

Fourth, "25.6 fewer violent crimes per 100,000 population" is the expected change in $y$. Again, you always have to consider the unit scale of your variables. In this case, $y$ is measured as the number of crimes per 100,000 population, so a decrease of 25.6 means 25.6 fewer violent crimes per 100,000 population. 

Fifth, I append the term "on average" to the end of my interpretation. This is because we know that our points don't fall on a straight line and so we don't expect a deterministic relationship between median age and violent crime. Rather, we think that if we were to take a group of states that had one year higher median age than another group of states, the average difference between the groups would be -25.6. 

Lets try a couple of other examples to see how this works. I will use the lm command in R to calculate the slopes and intercepts, which I explain in the section below. First, lets look at the association between age and sexual frequency (I will explain the code I use here later in this section).

```{r}
coef(lm(sexf~educ, data=sex))
```

The slope here is 0.03. Education is measured in years and sexual frequency is measured as the number of sexual encounters per year. So, the interpretation of the slope should be: 

> The model predicts that a one year increase in education is associated with 0.03 more sexual encounters per year, on average. 

There is a tiny positive effect here, but in real terms the relationship is basically zero. It would take you about 100 years more education to get laid 3 more times. Just think of the student loan debt. 

Now, lets take the relationship between movie runtimes and tomato meter ratings:

```{r}
coef(lm(TomatoMeter~Runtime, data=movies))
```


The slope is 0.41. Runtime is measured in minutes. The tomato meter is the percent of reviews that were judged to be positive. 

> The model predicts that a one minute increase in movie runtime length is associated with a 0.38 percentage point increase in the movie's Tomato Meter rating, on average.

Longer movies tend to have higher ratings. We may rightfully question the assumption of linearity for this relationship however. It seems likely that if a movie can become too long, so its possible the relationship here may be non-linear. We will explore ways of modeling that potential non-linearity later in the term.

####Interpreting Intercepts

Intercepts give the predicted value of $y$ when $x$ is zero. Again you should never interpret an intercept in these abstract terms but rather in concrete terms based on the unit scale of the variables involved. What does it mean to be zero on the $x$ variable? 

In our example of the relationship of median age to violent crime rates, the intercept was 1343.9. Our independent variable is median age and the dependent variable is violent crime rates, so:

> The model predicts that in a state where the median age is zero, the violent crime rate would be 1343.9 crimes per 100,000 population, on average. 

Note that I use the same "model predicts" and "on average" prefix and suffix for the intercept as I used for the slope. Beyond that I am just stating the predicted value of $y$ (crime rates) when $x$ is zero in the concrete terms of those variables. 

Is it realistic to have a state with a median age of zero? No, its not. You will never observe a US state with a median age of zero. This is a common situation that often confuses students. In cases when zero falls outside the range of the independent variable, the intercept is not a particular useful number because it does not tell us about a realistic situation. The intercept's only "job" is to give a number that allows the line to go through the points on the scatterplot at the right level. You can see this in the interactive exercise above if you select the right slope of 148 and then vary the intercept. 

In general making predictions for values of $x$ that fall outside the range of $x$ in the observed data is problematic. This is ofen leads to intercepts which don't make a lot of sense. This problem with zero being outside the range of data is also evident in the other two examples of slopes from the previous section. When looking at the relationship between education and sexual frequency, no respondents are actually at zero years of education and no movies are at zero minutes of runtime. 

In truth, to fit the line correctly, we only need the slope and one point along the line. It is convenient to choose the point where $x=0$ but there is no reason why we could not choose a different point. It is actually quite easy to calculate a different predicted value along the line by **re-centering** the independent variable.

To re-center the independent variable $x$, we just need to to subtract some constant value $a$ from all the values of $x$, like so:

$$x^*=x-a$$
The zero value on our new variable $x^*$ will indicates that we are at the value of $a$ on the original variable $x$. If we then use $x^*$ in the OLS regression line rather than $x$, the intercept will give us the predicted value of $y$ when $x$ is equal to $a$. 

Lets try this out on the model predicting violent crimes by median age. We will create a new variable where we subtract 35 from the median age variable and use that in the regression model.

```{r}
crimes$MedianAge.ctr <- crimes$MedianAge-35
coef(lm(Violent~MedianAge.ctr, data=crimes))
```

The intercept now gives me the predicted violent crime rate in a state with a median age of 35. In effect, I have moved my y-intercept from zero to thirty-five as is shown in the figure below. 

```{r ols_move_intercept, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
par(mar=c(4,4,1,1))
plot(crimes$MedianAge, crimes$Violent, pch=21, bg="grey90", col=NULL, cex=0.7,
     xlim=c(0,45), ylim=c(200,1400), xaxt="n", bty="n", las=1,
     xlab="Median Age (original and re-centered)", ylab="Violent crime rate (crimes/100,000)")
model <- lm(Violent~MedianAge, data=crimes)
abline(model, lwd=2)
axis(1,at=seq(from=0,to=45, by=5))
axis(1,at=seq(from=0,to=45, by=5), labels=paste(seq(from=0,to=45, by=5)-35), 
     tick=FALSE, line=1)
abline(v=0, lty=3, col="grey", lwd=2)
abline(v=35, lty=3, col="grey", lwd=2)
MedianAge <- c(0,35)
predicted <- predict(model,data.frame(MedianAge))
points(MedianAge,predicted, pch=21, bg="red")
text(MedianAge,predicted,labels=paste(round(predicted,1)), pos=4, cex=0.7)
arrows(rep(0,3),c(400,800,1200),rep(35,3),c(400,800,1200),length=0.1)
```

Its also possible to re-center an independent variable in the `lm` command without creating a whole new variable. If you surround the re-centering in the `I()` function within the formula, R will interpret the result of whatever is inside the `I()` function as a new variable. Here is an example based on the previous example:

```{r}
coef(lm(Violent~I(MedianAge-35), data=crimes))
```

###How good is $x$ as a predictor of $y$?

If I selected a random observation from the dataset and asked you to predict the value of $y$ for this observation, what value would you guess? Your best guess would be to guess the mean of y because this is the case where your average error would be smallest. This error is defined by the distance between the mean of y and the selected value, $y_i-\bar{y}$. 

Now, lets say instead of making you guess randomly I first told you the value of another variable $x$ and gave you the slope and intercept predicting $y$ from $x$. What is your best guess now? You should guess the predicted value of $\hat{y}_i$ from the regression line because now you have some additional information. There is no way that having this information could make your guess worse than just guessing the mean. The question is how much better do you do than guessing the mean. Answering this question will give us some idea of how good $x$ is as a predictor of $y$. 

We can do this by separating, or *partitioning* the total possible error in our first case when we guessed the mean, into the part accounted for by $x$ and the part that is unaccounted for by $x$. 

I can demonstrate this partitioning for one observation in our crime data (the state of Vermont) with this scatterplot:

```{r scatter_partition_variance, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
model <- lm(Violent~MedianAge, data=crimes)
plot(crimes$MedianAge,crimes$Violent,
     xlim=c(min(crimes$MedianAge),max(crimes$MedianAge)),
     ylim=c(min(crimes$Violent),max(crimes$Violent)),
     xlab="Median Age",
     ylab="Violent crimes (per 100,000)",
     main="Scatterplot of Median Age and Violent Crime",
     pch=21, bg="grey90",col="white", las=1)
abline(model, lwd=2)
abline(h=mean(crimes$Violent), lwd=2, lty=2)
points(crimes$Violent[46],crimes$MedianAge[46], pch=21,
       bg="black")
segments(crimes$MedianAge[46]-.08,crimes$Violent[46],
         crimes$MedianAge[46]-.08,mean(crimes$Violent),
         col="red", lwd=2)
segments(crimes$MedianAge[46]+.08,crimes$Violent[46],
         crimes$MedianAge[46]+.08,
         model$coef[1]+model$coef[2]*crimes$MedianAge[46],
         col="blue", lwd=2)
segments(crimes$MedianAge[46]+.08,mean(crimes$Violent),
         crimes$MedianAge[46]+.08,
         model$coef[1]+model$coef[2]*crimes$MedianAge[46],
         col="green", lwd=2)
legend(4,4500, legend=c("Total distance","Explainable by model", "Model residual"), lty=1, lwd=2, col=c("red","green","blue"))
```

The distance in red is the total distance between the observed property violent rate in the state of Vermont and the mean violent crime rate across all states (given by the dotted line). If I were instead to use the regression line predicting the violent crime rate by median age, I would predict a lower violent crime rate than average for Vermont because of its relatively high median age, but I would still predict a value that is too high. This red line is then partitioned into th green line which is the improvement in my estimate and the blue line which is the error that remains in my prediction from the regression line. If I could then repeat this process for all of the states, I could calculate the percentage of the total red lines that the green lines cover. This would give me an estimate of how much I reduce the error in my prediction by using the regression line rather than the mean to predict Vermont's violent crime rate. 

In practice, we actually need to square those vertical distances because some are negative and some are positive and then we can sum them up over all the observations. So we get the following formulas:

- Total variation: $SSY=\sum_{i=1}^n (y_i-\bar{y})^2$
- Explained by model: $SSM=\sum_{i=1}^n (\hat{y}_i-\bar{y})^2$
- Unexplained by model: $SSR=\sum_{i=1}^n (y_i-\hat{y}_i)^2$

The proportion of the variation in $y$ that is explainable or accountable by variation in $x$ is given by $SSM/SSY$. 

This looks like a kind of nasty calculation, but it turns out there is a much simpler way to calculate this proportion. If we just take our correlation coefficient $r$ and square it. We will get this proportion. This measure is often called "r squared" and can be interpreted as the proportion of the variation in $y$ that is explainable or accountable by variation in $x$. 

In the example above, we can calculate R squared:

```{r}
cor(crimes$MedianAge, crimes$Violent)^2
```
About 9% of the variation in violent crime rates across states can be accounted for by variation in the median age across states.  

###Using the `lm` command to calculate OLS regression lines in *R*

I want to say a few words about using the `lm` (for "linear model") command to calculate the OLS regression line. We could just use the given formulas to calculate the slope and intercept in *R*, as I showed above with the crime example. However, the `lm` command will become particularly useful later in the term when we extend this basic OLS regression line to more advanced "models."

In order to run the `lm` command, you need to input a formula. The structure of this formula looks like "dependent~independent" where "dependent" and "independent" should be replaced by your specific variables. The tilde (~) sign indicates the relationship. So, if we wanted to look at the association between unemployment and property crime, we would do:

```{r}
model1 <- lm(crimes$Property~crimes$Unemployment)
```

**The dependent variable always goes on the left-hand side of this equation.**

In this case, I have entered in the variable names using the `data$variable` syntax, but `lm` also offers you a more streamlined way of specifying variables, by including a `data` option separately so that you only have to put the variable names in the formula, like so:

```{r}
model1 <- lm(Property~Unemployment, data=crimes)
```

Because I have specified "crimes" as the dataset, *R* knows that the variables "Property" and "Unemployment" refer to variables within this dataset. The result will be the same as the previous command, but this approach makes it easier to read the formula itself. 

I have saved the output of the `lm` command into a new object that I have called "model1". You can call this object whatever you like.  This is out first real example of the "object-oriented" nature of *R*. I can apply a variety of functions to this object in order to extract information about the relationship. If I want to get the most information, I can run a `summary` on this model. 

```{r}
summary(model1)
```

There is a lot information here and we actually don't know what most of it means yet. All we want is the intercept and slope. These numbers are given by the two numbers in the "Estimate" column of the "Coefficients" section. The intercept is 1628.35 and the slope is 148.68. 

We could also run the `coef` command which will give us just the slope and intercept of the model. 

```{r}
coef(model1)
```

This result is much more compact and will do for our purposes at the moment. 

Finally, we can also plot the line to a scatterplot as shown in the very first figure of this module by using the `abline` command on our model. 

```{r scatter_abline, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(crimes$Unemployment, crimes$Property, 
     pch=21, bg="red", 
     xlab="Unemployment Rate", ylab="Property crime rate per 100,000", 
     main="scatterplot of unemployment\nand property crime rates")
abline(model1)
```

###Regression Line Cautions

OLS regression models can be very useful for understanding relationships, but they do have some important limitations that you should be aware of when you are doing statistical analysis. 

There are three major limitations/cautions to be aware of when using OLS regression:

1. OLS regression only works for linear relationships.
2. Outliers can sometimes exert heavy influence on estimates of the relationship
4. Don't extrapolate beyond the scope of the data.

####Linearity

By definition, an OLS regression line is a straight line. If the underlying relationship between x and y is non-linear, then the OLS regression line will do a poor job of measuring that relationship.

One common case of non-linearity is the case of diminishing returns in which the slope gets weaker at higher values of x. That is the case with this data:

```{r nonlinearity, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
par(mar=c(4,4,0.1,1))
plot(preston$inc, preston$lifeexp,
     xlab="national income, 1960",
     ylab="life expectancy, 1960", pch=21, bg="grey", las=1)
```


The data here plot the national income of countries in 1960 to the life expectancy of countries in 1960. The relationship is clearly a strongly positive one, but also one of diminishing returns where the positive effect seems to plateau at higher levels of national income and life expectancy. This makes sense because the same absolute increase in national income at low levels of life expectancy can be used to reduce the incidence of well-understood infectious and parasitic diseases, whereas the same absolute increase in national income at high levels of life expectancy must try to reduce the risk of less understood and treatable diseases like cancer. You get more bang for your buck when life expectancy is low. 

If we try to fit a line to this data, we will get this:

```{r nonlinearity2, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
par(mar=c(4,4,0.1,1))
plot(preston$inc, preston$lifeexp,
     xlab="national income, 1960",
     ylab="life expectancy, 1960", pch=21, bg="grey", las=1)
abline(lm(lifeexp~inc,data=preston), lwd=2, col="blue")
text(800,68,label="Underestimate", col="red")
text(200,40,label="Over\nestimate", col="red")
text(2300,68,label="Overestimate", col="red")
```

Clearly a straight line is a poor fit. We systematically overestimate life expectancy at low and high national income and underestimate life expectancy in the middle. 

Its possible, in some circumstances, to correct for this problem of non-linearity but we will not explore those options in this class. For now, its just important to be aware of the problem and if you see clear non-linearity then you should question the use of an OLS regression line. 

####Outliers and Influential Points

An outlier is an **influential point** if removing this observation from the dataset substantially changes the slope of the OLS regression line. You can try this out in [this exercise](https://aarongullickson.shinyapps.io/influentialpoints) on the crime data by clicking on points to remove them. Note how much the line changes when you remove a point. (You can click on a point a second time to add it back again).

For the case of median age, Utah and DC both have fairly strong influences on the shape of the line. Removing DC makes the relationship weaker, while removing Utah makes the relationship stronger. Outliers will tend to have the strongest influence when their placement is inconsistent with the general pattern. In this case, Utah is very inconsistent with the overall negative effect because it has both low median age and low crime rates. 

Lets say that you have identified an influential point. What then? In truth there is only so much you can do. You cannot remove a valid data point just because it is an influential point. There are two cases where it would be legitimate to exclude the point. First, if you have reason to believe that the observation is an outlier because of a data error, then it would be acceptable to remove it. Second, if you have a strong argument that the observation does not belong with the rest of the cases, because it is logically different, then it might be OK to remove it.

In our case, there is no legitimate reason to remove Utah, but there probably is a legitimate reason to remove DC. Washington DC is really a city and the rest of our observations are states that contain a mix of urban and rural population. Because crime rates are higher in urban areas, DC's crime rates look very exaggerated compared to states. Because of this "apples and oranges" problem, it is probably better to remove DC. If our unit of analysis was cities, on the other hand, then DC should remain. 

In large datasets (1000+ observations), its unusual that a single point or even a small cluster of points will exert much influence on the shape of the line. The concern about influential points is mostly a concern in small datasets like the crime dataset. 

####Thou Doth Extrapolate Too Much

Its dangerous enough to assume that a linear relationship holds for your data (see the first point in this module). Its doubly dangerous to assume that this linear relationship holds beyond the scope of your data. Lets take the relationship between sexual frequency and age. We saw in the previous module that the slope here is -1.3 and the intercept is 108. The intercept itself is outside the scope of the data because we only have data on the population 18 years and older. It would be problematic to make predictions about the sexual frequency of 12 year olds, let alone zero-year olds. 

Another trivial example would be to look at the growth rate of children 5-15 years of age by correlating age with height. It would be acceptable to use this model to predict the height of a 14 year old, but not a 40 year old.  We expect that this growth will eventually end sometime outside the range of our data when individuals reach their final adult height. If we extrapolated the data, we would predict that 40 year olds would be very tall. 


------