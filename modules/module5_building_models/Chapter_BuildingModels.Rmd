#Building Better Models

------

##What is a Model?

In this module, we will build on our understanding of the OLS regression line. It might be worthwhile to review that module in its entirety, but I will also give a brief review here. 

Lets use the OLS regression line to look at the relationship between sexual frequency and years of education. We can calculate this line in R:

```{r}
model <- lm(sexf~educ, data=sex)
coef(model)
```

The OLS regression line has an intercept of 49.7 and a slope of 0.03. The intercept is the predicted value of the dependent variable when the independent variable is zero and the slope is the predicted change in the dependent variable for a one-unit increase in the independent variable. The dependent variable is sexual frequency measured in the number of sexual encounters per year and the independent variable is years of education measured in years. So, my interpretation of the slope is:

> The model predicts that a one year increase in years of education is associated with 0.03 more instances of sex per year, on average. 

There is a very small positive association between years of education and sexual frequency. The effect of getting a bachelor's degree (lets call it four years of education) is approximately 0.03*4=0.12, or an additional 0.12 sexual encounters per year, on average.  Hopefully, you are all here for the sake of pure knowledge and human capital, and not for the future sex rewards.

My interpretation of the intercept is:

> The model predicts that individuals with no education have sex 49.7 times per year, on average. 

Note that zero education ("no education") is actually outside the scope of my data:

```{r}
summary(sex$educ)
```

The lowest value of years of education is one year and there are only a few respondents whose education is that low. So, I don't put much stock in this intercept to tell me something important. 

###The OLS regression line as a model

You will note that I saved the output of my `lm` command above as model and I have also referred to this line in class as a "model." When I asked you to interpret slopes and intercepts, I have insisted that you preface your interpretation with the term "The model predicts." What do I mean by this term "model?" When we talk about "models" in statistics, we are talking about modeling the relationship between two or more variables in a formal mathematical way. In the case of the OLS regression line, we are predicting the dependent variable as a **linear function** of the independent variable. 

Just as the general term model is used to describe something that is not realistic but rather an idealized representation, the same is true of our statistical models. We certainly don't believe that our linear function provides a correct interpretation of the exact relationship between our two variables. Instead we are trying to abstract from the details and fuzziness of the relationship to get a "big picture" of what the relationship looks like. 

However, we always have to consider that our model is not a very good representation of the relationship. The most obvious potential problem is if the relationship is non-linear and yet we fit the relationship by a linear model, but there can be other problems as well.

In this section, we will learn a variety of ways to extend our basic bivariate (two-variable) OLS regression model to produce more realistic models of association and the underlying social processes we seek to uncover. We will focus on two particular techniques:

1. We will learn how to include **multiple independent variables simultaneously** in a single OLS regression model, thus moving from bivariate to multivariate OLS regression models. By doing this, we will be able to control for the effects of potentially confounding variables. 
2. We will learn how to **include categorical variables as independent variables**. This will allow us to predict quantitative dependent variables by a combination of categorical and quantitative independent variables.

###Additional examples and readings for this section

In addition to our usual datasets, we will be looking at two examples of published research articles in this section. The first article is titled "The rewards of authority in the workplace: Do gender and age matter" by Schieman, Schafer, and McIvor. The second article is titled "Generational Status and Academic Achievement among Latino High School Students: Evaluating the Segmented Assimilation Theory" by Demetra Kalogrides. Both articles were published in the journal Sociological Perspectives. You are expected to read both articles in their entirety and become familiar with their general research question, research methodology, and conclusions. By the end of this section you should understand every number presented in both articles, particularly the regression model output.  

------

##The Power of Controlling for Other Variables

In the previous module, I showed that the OLS regression line predicting sexual frequency by years of education was 0.03. So in my dataset, there is a very small positive association between sexual frequency and years of education. 

Its possible that this is a causal effect. We could even spin stories about why we think such a positive association (a very small one) might exist. Maybe more educated people appear sexier to the opposite sex. Maybe more educated people take better care of themselves and thus are healthier and more able to have sex. Maybe more educated people are just more sexually liberated. 

Before we get carried away however its important to consider whether our results might be **spurious**. Its possible that the positive association between years of education and sexual frequency is driven by a third variable that we haven't accounted for. This is a common problem in research using observational data. Association does not necessarily mean causation because of the potential for other variables to account for our observed association (and because of the possibility of reverse causation). We refer to such variables as **lurking** or **confounding** variables. 

In this case, the potentially confounding variable that we need to consider is age. Lets look at the association between age and each of our other variables (sexual frequency and education). 

```{r}
cor(sex$sexf,sex$age)
cor(sex$educ,sex$age)
```

Age is negatively correlated with sexual frequency. We have observed this relationship before and it is not terribly surprising. Older people have less sex, on average. The negative correlation between age and years of education is perhaps a little more surprising. Older people have less education than younger people, on average. This may seem surprising to you because as you get older you have more opportunity to complete more education. However, you have to remember that the data we have are a snapshot in time. We are not tracking individuals over time as they age, but rather looking at differences between older and younger people at a single point in time. This kind of data is often called a **cross-sectional** dataset. Because we are looking at a single point in time, the age differences really reflect differences in **birth cohorts** or what people often loosely call "generations." Remember that this dataset is from 2004. The difference between a 20 year old and a 60 year old is that the 20 year old was born in 1984 and the 60 year old was born in 1944. 

Because we are comparing birth cohorts, the differences in educational attainment reflect history more than life cycle. Older cohorts were less educated than younger birth cohorts. On average, you will be more educated than your parents and your parents were more educated than your grandparents. Thus, the correlation between age and education is negative. 

These two negative correlations suggest a **spurious** reason why we might observe a positive association between sexual frequency and education. Younger people have more education and younger people have more sex. Thus, when we look at the relationship between sexual frequency and education, we see a positive association but that positive association is indirectly driven by youth and the association of youth with both education and sex. 

How can we examine whether this potential spurious explanation is accurate? It turns out that we can add more than one independent variable to an OLS regression model at the same time. The mathematical structure of such a model would be:

$$\hat{frequency}_i=b_0+b_1(education_i)+b_2(age_i)$$

We now have two different "slopes", $b_1$ and $b_2$. These two slopes give the association of education and age, respectively, on sexual frequency, while **controlling for the other independent variable**. We now have what is called a **multivariate** OLS regression model. 

This "controlling" concept is a key point that I will return to below, but first I want to try to think graphically about what this model is doing. In the case of bivariate regression, we thought of fitting a line to a scatterplot in two-dimensional space. We are doing something similar here, but since we now have three variables, our scatterplot is in three dimensions. 

```{r scatter_3d, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
library(scatterplot3d)
s3d <- scatterplot3d(jitter(sex$educ,5),jitter(sex$age,5),jitter(sex$sexf,5), pch=21, cex.symbols =0.5, bg="grey", xlab="years of education", ylab="age", zlab="sexual frequency", main="3d scatterplot", cex.main=1.8)
fit <- lm(sexf~educ+age, data=sex)
x1 <- c(0,25)
x2 <- c(0,100)
y <- fit$coef[1]+fit$coef[2]*x1+fit$coef[3]*x2
#s3d$points3d(x1,x2,y, type="l", col="red", lwd=3)
s3d$plane3d(fit, col="red")
```

The dependent variable is shown on the vertical (z) axis and the two independent variables are shown on the "width" and "depth" axes (x and y). The red dashes show a flat plane across the data. The OLS regression model equation above defines this plane, rather than a single line. So, rather than fitting a straight line through the data, I am fitting a plane. Note however that if I could rotate the 3d scatterplot to hide the age "depth" dimension and in that case it would look like a two-dimensional scatterplot and the edge of the plane would look like a line that describes the relationship between education and sexual frequency. Similarly, I could rotate it the other way to look at the relationship between age and sexual frequency. 

How do I know what are the best values for b0, b1, and b2 that define my plane? The logic is the same as for bivariate OLS regression: I choose values that minimize the sum of squared residuals (SSR):

$$SSR=\sum_{i=1}^n (\hat{y}_i-y_i)^2$$

SSR is a measure of how far the predicted values of the dependent variable are from the actual values, so we want the intercept and slopes that minimizes this error. Unlike the bivariate case, however, there is no simple formula that I can give you for the slope and intercept, without some knowledge of matrix algebra. However, R can calculate the correct numbers for you easily. I am not concerned with your technical ability to calculate these numbers by hand, but I do want you to understand why those are the "best" numbers. **They are the best  numbers because they minimize the sum of the squared residuals for the model**. 

We can calculate this model in *R* just by adding another variable to our model in the `lm` command:

```{r}
model <- lm(sexf~educ+I(age-18), data=sex)
coef(model)
```

Note that as I did in the previous module, I am re-centering age on 18 years so that I have reasonable value for the interpretation of the intercept. In equation form, our model will look like:

$$\hat{frequency}_i=91.06-0.43(education_i)-1.30(age_i-18)$$

###Interpreting results in a multivariate OLS regression models

How do we interpret the results?

- **Intercept**: The model predicts that **18-year old individuals** at **with no education** will have 91.06 sexual encounters per year, on average.
- **Education Slope**: The model predicts that, **holding age constant**, an additional year of education is associated with 0.43 fewer sexual encounters per year, on average. 
- **Age Slope**: The model predicts that, **holding education constant**, an additional year of age is associated with 1.3 fewer sexual encounters per year, on average. 

The intercept is now the predicted value **when all independent variables are zero**. My interpretation of the slopes is almost identical to the bivariate case, except for one very important addition.  I am now estimating the effect of each independent variable on the dependent variable while **holding constant all other independent variables**. You could also say "controlling for all other independent variables."

What does it mean to "hold other variables constant?" It means that when we look at the effect of one independent variable, we are looking at how the predicted value of the dependent variable changes while keeping all the other variables the same. For instance, the education effect above is the effect of a one year increase in education *among individuals of the same age*. Because we are looking at the effect of education among individuals of the same age, age should no longer have a confounding effect on our estimate of the effect of education. Thus holding constant/controlling for other variables helps to remove the potential spurious effect of those variables as confounders.

Note how the effect of education on sexual frequency changed once I included age as a control variable.  Before controls, I estimated a slightly positive slope (0.03) but now I am estimating a substantial negative slope (-0.43). So my understanding of the relationship between education and sexual frequency is completely reversed. *When you compare individuals of the same age*, more educated individuals have less sex, on average, than less educated individuals. 

####Crime example

Lets build a regression model where we predict the property crime rate in a state by the percent of adults in the state without a high school diploma and the median age of the state's residents.

```{r}
summary(lm(Property~PctLessHS+MedianAge, data=crimes))
```

Note that I am giving you the full output of summary now, but we can find the slopes and intercept by looking at the Estimate column of the "Coefficients" table. "Coefficients" is another term for slopes and intercepts because that it the technical term for these values in the regression model equation. 

The model is:

$$\hat{crime}_i=5137+69(pctlesshs_i)-83(medianage_i)$$

The model predicts that, comparing two states with the same median age of residents, a one percent increase in the percent of the state with less than a high school diploma is associated with an increase of 69 property crimes per 100,000, on average. The model predicts that, comparing two states with the same percentage of adults without a high school diploma, a one year increase in the median age of a state's residents is associated with a decrease of 83 property crimes per 100,000, on average. 

Note that we also get the $R^2$ value from the summary command. In multivariate models, the $R^2$ value always tells you what proportion of the variation in the dependent variable is accountable for by variation in all of the independent variables combined. In this case $R^2$ is 0.2495. About 25% of the variation in property crime rates across states is accountable for by variation in the percent of adults without a high school diploma and the median age of residents across states. 

###Including more than two independent variables

If we can include two independent variables in a regression model, why stop there? Why not include three or four or more? The number of independent variables you can include is only limited by the sample size (you can never have more independent variables than the sample size minus one), although in practice we generally stop well short of this limit for pragmatic reasons. 

Lets take the model above predicting property crime rates by percent of adults with less than a high school diploma and the median age of residents. Lets add the poverty rate as another predictor:

```{r}
summary(lm(Property~PctLessHS+MedianAge+Poverty, data=crimes))
```

The model predicts:

- A one percent increase in the percent of adults in a state without a high school diploma is associated with 0.05 more property crimes per 100,000, on average, **holding constant the median age of residents and the poverty rate in a state**. This result is about as close to zero as you will find.  
- A one year increase in the median age of a state's residents is associated with 73 fewer property crimes per 100,000, on average, **holding constant the percent of adults without a high school diploma and the poverty rate in a state**. 
- A one percent increase in a state's poverty rate is associated with 98 more crimes per 100,000, on average, **holding constant the percent of adults without a high school diploma and the median age of residents in a state**. 
- 34% of the variation in property crime rates across states can be accounted for by variation in the percent of adults without a high school diploma, residents' median age, and the poverty rates across states.

When I interpret the models now, I am holding constant the other two variables when I estimate the effect of each. Note that controlling for the poverty rate has a huge effect on the education variable whose effect goes from a substantial positive effect to basically zero effect. What does this tell us? Poverty rates and high school dropout rates are positively correlated and so when you don't control for poverty rates, it looks like the high school dropout rate predicts crime because states with high high school dropout rates have high poverty rates and high poverty rates predict property crime rates. Once you control for the poverty rate, you see that it is economic deprivation not educational deprivation that is driving the crime rate. 

In general, the form of the multivariate regression model is:

$$\hat{y}_i=b_0+b_1x_{i1}+b_2x_{i2}+b_3x_{i3}+\ldots+b_px_{ip}$$

The intercept is given by $b_0$. This is the predicted value of $y$ when all of the independent variables are zero. The remaining $b$'s give the slopes for all of the variables up through the $p$th variable. Each of these gives the predicted change in $y$ for a unit increase in that independent variable, **holding all other independent variables constant**. 

###How to read a table of regression results

In academic journal articles and books, the results of OLS regression models are represented in a fairly standard way. In order to understand how to read these articles, you need to understand this presentation style. Its not immediately intuitive for everyone. The table below shows the typical style. In this table, I am reporting three regression models with the property crime rates as the dependent variable and three different independent variables.  

```{r echo=FALSE, results="asis", warning=FALSE, message=FALSE}
library(stargazer, quietly=TRUE, verbose=FALSE)
m1 <- lm(Property~PctLessHS, data=crimes)
m2 <- lm(Property~PctLessHS+MedianAge, data=crimes)
m3 <- lm(Property~PctLessHS+MedianAge+Poverty, data=crimes)
stargazer(m1,m2,m3, type="html",
          omit.stat=c("adj.rsq","ser","f"), 
          digits=2, 
          dep.var.labels="Property crime rate",
          covariate.labels = c("Percent Less than HS", 
                               "Median Age",
                               "Poverty Rate"),
          notes=c("Standard errors in parenthesis"),
          notes.append=TRUE,
          title="OLS regression models predicting violent crime rates for US states",
          dep.var.labels.include=FALSE,
          dep.var.caption="",
          header=FALSE)
```

When reading this table and others like it, keep the following issues in mind:

1. The first question you should ask is "what is the dependent variable?" This is the outcome that we are trying to predict. Typically, the dependent variable will be listed in the title of the table. In this case, the title tells you that the dependent variable is property crime rates and the unit of analysis is US states.
2. The independent variables are listed on the rows of the table. In this case, I have independent variables of percent less than HS, median age, and the poverty rate. As I will explain below, just because an independent variable is listed here does not mean that it is actually included in all models.
3. The term "constant" is a synonym for "intercept."
4. Models are listed in each column of the table. If numbers are listed for the row of a particular independent variable then that variable is included in that particular model. In this case, I have three different models. The first model only has numbers listed for Percent less than HS, so that is the only independent variable in the first model. The second model has numbers listed for Percent less than HS and Median Age, so both of these variables are included in the model. The third model includes all three variables in the model. Remember that in each case the dependent variable is the property crime rate. 
5. Within each cell with numbers listed there is a lot going on. However, at this point we are only interested in the main number listed at the top. This number is the slope (or intercept in the case of the "Constant" row). We will learn about the number in parenthesis and all those asterisks in the next module. 
6. At the bottom, you typically get a number of summary measures of the model. The only two we care about are the number of observations and the $R^2$ of the model.

------

##Including Categorical Variables as Predictors

To this point, we only know how to include quantitative variables into OLS regression models. However, it turns out you can use a fairly easy trick to include categorical variables as independent variables in OLS regression models. By including categorical variables as independent variables, we expand considerably the range of things that we can do with OLS regression models.  The most difficult part of this trick is correctly interpreting your results. 

###Indicator variables

As an example, I am going to look at the relationship between religious affiliation and sexual frequency. To keep our example simple I am going to **dichotomize** the religious affiliation variable, which means I am going to collapse it into two categories, rather than the six categories in the dataset. I will use a simply dichotomy of "Not Religious/Religious." In R, I can create this variable like so:

```{r}
sex$norelig <- sex$relig=="None"
```

This is technically a **boolean** variable, which means it takes a TRUE or FALSE value. For our purposes, TRUE is a non-religious person. 

We already know how to look at the relationship between sexual frequency and this dichotomized religious affiliation variable. We can look at the mean differences in sexual frequency across our two categories:

```{r}
tapply(sex$sexf, sex$norelig, mean)
59.84862-48.33671
```

The non-religious have sex 11.5 more times per year than the religious, on average. Hallelujah?

We can represent this same mean difference in a regression model framework by using an indicator variable. An indicator variable is a variable that only takes a value of zero or one. It takes a value of one when the observation is in the indicated category and a zero otherwise. Mathematically, we would say:

$$nonrelig_i=\begin{cases}
  1 & \text{if non-religious}\\
  0 & \text{otherwise}
  \end{cases}$$
  
The **indicated category** is the category which gets a one on the indicator variable. In this case the indicated category is non-religious. The **reference category** is the category that gets no indicator variable. In this case, that is just the religious group. Later on, we will see that this can become slightly more complicated. You can think of the indicator variable as an on/off switch where 1 indicates that it is "on" (i.e. the observation belongs to the indicated category) and 0 indicates that it is "off" (i.e. the observation does not belong to the indicated category).

What would happen if we put this indicator variable into a regression model predicting sexual frequency like so:

$$\hat{frequency}_i=b_0+b_1(nonrelig_i)$$

How would we interpret the slope and intercept for such a model? It might help to look at a scatterplot of this relationship.

```{r scatter_dichotomous, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
par(mar=c(4,4,0,2))
plot(sex$norelig, sex$sexf, pch=21, bg="seagreen",
     xlab="religious indicator variable",
     ylab="sexual frequency", xaxt="n", las=1)
axis(1, at=c(0,1), labels=c("0=religious","1=non-religious"))
points(c(0,1),tapply(sex$sexf, sex$norelig, mean),
       pch=21, col="red", bg="grey80", cex=2)
text(c(0,1),tapply(sex$sexf, sex$norelig, mean)*1.1,
     c("Religious\nmean", "Non-religious\nmean"), pos=c(4,2))
abline(lm(sexf~norelig, data=sex), lty=2, col="red")
text(0.5, 30, "Slope is 11.5,\nthe mean difference between\nthe groups")
```

Notice that all of the points align vertically either at the 0 or 1 on the x-axis. This is because the indicator variable can only take these two values. In addition, there is a lot of overplotting of these points right on top of one another so it is hard to see the trend. To simplify things I have plotted the means of the two groups in grey dots and the OLS regression line for the scatterplot in red. In order to be the best-fitting line, this OLS regression line must connect those two dots that represent the mean of each group. 

What will the slope of this line be? If we go up "one unit" on the non-religious indicator variable we have gone from a religious person to a non-religious person and the change in predicted sexual frequency is equal to the mean difference of 11.5 between the groups. The intercept is given by the value at zero which is just given by the mean sexual frequency among the religious of 48.3. So, the OLS regression line should look like:

$$\hat{frequency}_i=48.3+11.5(nonrelig_i)$$

I can calculate these same numbers in *R* with the `lm` command:

```{r}
coef(lm(sexf~norelig, data=sex))
```

The numbers are the same. More important than the numbers, however, is the interpretation of the numbers. The intercept is the mean of the dependent variable for the reference category. The slope is the mean difference between the reference category and the indicated category. In this case, I would say:

- Religious individuals have sex 48.3 times per year, on average.
- Non-religious individuals have sex 11.5 times more per year than non-religious individuals, on average.

Note that I can derive the sexual frequency of the non-religious from these two numbers by taking the value for the non-religious and adding the mean difference to find out that non-religious individuals have sex 59.8 times per year, on average. 

####Reversing the indicator variable

What if I switched my indicator variable so that the religious were indicated and the non-religious were the reference category? 

$$relig_i=\begin{cases}
  1 & \text{if religious}\\
  0 & \text{otherwise}
  \end{cases}$$

Lets try it out in *R* and see (the `!=` below is computer lingo for "not equal to"):

```{r}
sex$religious <- sex$relig!="None"
coef(lm(sexf~religious, data=sex))
```

Lets compare the two models:

$$\hat{frequency}_i=48.3+11.5(nonrelig_i)$$
$$\hat{frequency}_i=59.8-11.5(relig_i)$$

Both models give me the exact same information, but from the perspective of a different reference group. The first model tells me the mean sexual frequency of the religious (48.3) and how much *more* sex the non-religious have on average (11.5). The second model tells me the mean sexual frequency of the non-religious (59.8) and how much *less* sex the religious have (-11.5). I can easily derive one model from the other, without actually having to calculate it in R. Therefore, which category you set as the reference category is really a matter of taste, rather than one of consequence. The results are the same either way. 

###Categorical variables with more than two categories

What if I have a categorical variable that has more than two categories? Lets expand the religious variable that I dichotomized back to its original scale. There are six different categories: Fundamentalist Protestant, Mainline Protestant, Catholic, Jewish, Other, and None:

```{r}
summary(sex$relig)
```

Lets look at the mean sexual frequency for each of these groups.

```{r}
round(tapply(sex$sexf, sex$relig, mean),1)
```

We could plot up these means on a number line to get a visual display of the differences:

```{r compare_relig_means, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
mdiffs <- round(tapply(sex$sexf, sex$relig, mean),1)
par(mar=c(0.1,4,0,0.1))
plot(rep(1,length(mdiffs)), mdiffs, cex=2, pch=21, bg="skyblue", bty="n", xaxt="n",
     yaxt="n", xlab="", ylab="mean sexual frequency (per year)", ylim=c(35,65), xlim=c(0.8,2))
axis(2, at=seq(from=35,to=65, by=5))
text(rep(1,length(mdiffs)), c(mdiffs[1]*1.01,mdiffs[2:6]), pos=4,
     paste(names(mdiffs),"(",mdiffs,")"))
arrows(0.85, mdiffs[1], 0.85, mdiffs[2], length=0.05, col="red")
arrows(0.875, mdiffs[1], 0.875, mdiffs[3], length=0.05, col="red")
arrows(0.9, mdiffs[1], 0.9, mdiffs[4], length=0.05, col="red")
arrows(0.925, mdiffs[1], 0.925, mdiffs[5], length=0.05, col="red")
arrows(0.95, mdiffs[1], 0.95, mdiffs[6], length=0.05, col="red")
abline(h=mdiffs[1], lty=2, col="red")
```

Nones and others clearly have much higher mean sexual frequency than the remaining religious groups and Jews have much lower mean sexual frequency. The three Christian groups cluster in the middle, although mainline protestants have a lower mean sexual frequency than the other two. 

This plot also shows the mean differences between the groups, with fundamentalist Protestants set as the reference category. The vertical distances from the dotted red line (the mean of fundamentalist Protestants) give the mean differences between each religious group and fundamentalist Protestants. So we can see that "Nones" have sex 10.2 more times per year than fundamentalist Protestants, on average, and mainline Protestants have sex 5.4 fewer times per year, on average, than fundamentalist Protestants. 

We can use the same logic of indicator variables we developed above to represent the mean differences between groups observed here in a regression model framework. However, because we now have six categories, we will need five indicator variables. You always need **one less indicator variable than the number of categories**.  The category which doesn't get an indicator variable is your reference category. As per the graph above, I will make Fundamentalist Protestants my reference category. Therefore, I need one indicator variable for each of the other five categories:

$$main_i=\begin{cases}
  1 & \text{if main}\\
  0 & \text{otherwise}
  \end{cases}$$
  
$$catholic_i=\begin{cases}
  1 & \text{if catholic}\\
  0 & \text{otherwise}
  \end{cases}$$
  
$$jewish_i=\begin{cases}
  1 & \text{if jewish}\\
  0 & \text{otherwise}
  \end{cases}$$
  
$$other_i=\begin{cases}
  1 & \text{if other religion}\\
  0 & \text{otherwise}
  \end{cases}$$

$$none_i=\begin{cases}
  1 & \text{if no religion}\\
  0 & \text{otherwise}
  \end{cases}$$

 Now lets put these variables into an OLS regression model:

$$\hat{frequency}_i=b_0+b_1(main_i)+b_2(catholic_i)+b_3(jewish_i)+b_4(other_i)+b_5(none_i)$$

We can figure out how all this works by getting the predicted value for the member of a specific group. That respondent should get a 1 for the variable where they are a member and a zero on all other variables. For example, a fundamentalist protestant should get a zero on all of these variables:

$$\hat{frequency}_i=b_0+b_1(0)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0$$

So, the intercept is the predicted value for fundamentalist Protestants. Similarly we could calculate the predicted value for mainline Protestants:

$$\hat{frequency}_i=b_0+b_1(1)+b_2(0)+b_3(0)+b_4(0)+b_5(0)=b_0+b_1$$

The difference between the two is $b_1$, so this "slope" gives the mean difference between mainline and fundamentalist Protestants. We could do the same thing for Catholics:

$$\hat{frequency}_i=b_0+b_1(0)+b_2(1)+b_3(0)+b_4(0)+b_5(0)=b_0+b_2$$

The mean difference between Catholics and fundamentalist Protestants is given by $b_2$.

In general, each of the "slopes" is the mean difference between the indicated category and the **reference category**. In this case, the reference category is fundamentalist Protestants so each of the slopes gives the mean difference between that religious category and fundamentalist Protestant, just like the graph above. 

R is fairly intelligent about handling all of these indicator variables and you don't actually have to create these five different variables. If you put a categorical variable into your regression formula, R will know to treat it as a set of indicator categories. The only catch is that R will already have a default category set as the reference. It just so happens that in our GSS data, fundamentalist Protestants are already set as the reference. So I can run this model by:

```{r}
model <- lm(sexf~relig, data=sex)
round(coef(model),2)
```

You can tell which category is the reference by which category is left out here. Note how the coefficients (given by the estimates column) match the mean differences I calculated above in the graph. We are simply reproducing these mean differences in a regression model framework.

###Categorical and quantitative variables combined in a single model

If all we are doing is reproducing mean differences between categories, what good is this method? After all, we already know how to do that. The major advantage of putting these mean differences into a regression model framework is that we can **control for other potentially confounding variables**. 

These sexual frequency differences by religious affiliation are a prime example. Lets take a look at the age differences between religious affiliations:

```{r}
round(tapply(sex$age, sex$relig, mean),1)
```

Notice how closely these age differences mirror the differences in sexual frequency. Others and nones are the youngest, while Jews are the oldest. Among Christians, mainline Protestants are older than fundamentalist Protestants and Catholics. We also know from prior work that age has a negative effect on sexual frequency. This should make us suspicious that some (or all) of the observed differences in sexual frequency between religious groups simply reflect age differences between those groups. 

We can easily address this issue by simply including age as a control variable in our model:

```{r}
model <- lm(sexf~relig+age, data=sex)
round(coef(model),2)
```

We now interpret those slopes as the mean difference in sexual frequency between fundamentalist Protestants and the indicated category, **among individuals of the same age**. So for example, we would interpret the 2.69 on "None" as:

> The model predicts that, among individuals of the same age, those with no religious preference have sex 2.69 more times per year than fundamentalist protestants, on average.

We would also interpret the age effect controlling for religious affiliation like so:

> The model predicts, that holding religious affiliation constant, a one year increase in age is associated with 1.28 fewer instances of sex per year, on average. 

The table below helps to highlight the change in the effects once age is controlled.

```{r echo=FALSE, results="asis", warning=FALSE, message=FALSE}
m1 <- lm(sexf~relig, data=sex)
m2 <- lm(sexf~relig+age, data=sex)
stargazer(m1,m2, type="html",
          omit.stat=c("adj.rsq","ser","f"), 
          digits=2, 
          dep.var.labels="sexual frequency",
          covariate.labels = c("Mainline Protestant",
                               "Catholic",
                               "Jewish",
                               "Other",
                               "None",
                               "Age"),
          notes=c("SE's in parenthesis. Reference category is fund. Protestant"),
          notes.append=TRUE,
          title="OLS regression models predicting sexual frequency",
          dep.var.labels.include=FALSE,
          dep.var.caption="",
          header=FALSE)
```

All of the coefficients (except for Catholic, which was tiny anyway) have declined substantially in size. The mean differences from fundamentalist Protestants for both Jews and other religions have basically disappeared and the "None" effect has been severely reduced. In other words, almost all or all of the observed differences in sexual frequency by religious affiliation were indirectly a product of underlying age differences between religious affiliations. If you were just about ready to convert to a different religion to get laid more often (or less depending on your preferences), you may want to hold off for the moment. 

------

##Interaction Terms

By definition, a linear model is an **additive** model. As you increase or decrease the value of one independent variable you increase or decrease the predicted value of the dependent variable by a set amount, **regardless of the other values of the independent variable**. This is an assumption built into the linear model by its additive form, and it may misrepresent some relationships where independent variables **interact** with one another to produce more complicated effects. In particular, in this section, we want to know whether the effect (i.e. the slope) of one independent variable varies by the value of another independent variable. 

###The nature of additive models

As an example for this section, I am going to look at the relationship between movie genre, runtime, and tomato meter ratings. To simplify things, I am going to only look at these relationships for two genres: action and comedy. I can limit my movies dataset to these two genres with the following command:

```{r}
movies.short <- subset(movies, Genre=="Comedy" | movies$Genre=="Action")
```

Now lets look at a simple model where genre and runtime both predict Tomato Meter ratings.

```{r}
round(coef(lm(TomatoMeter~Genre+Runtime, data=movies.short)),2)
```

Genre is a categorical variable and action movies are set as the reference category. In equation form, the model looks like: 

$$\hat{meter}_i=-1.75+4.43(comedy_i)+0.31(runtime_i)$$

I can interpret my slopes as follows:

- The model predicts that when comparing movies of the same runtime, comedies have Tomato Meter ratings 4.43 percentage points higher than action movies, on average.
- The model predicts that, holding constant movie genre, a one minute increase in movie runtime is associated with a 0.31 percentage point increase in the Tomato Meter rating, on average. 

This is an additive model. If we move from an action movie to a comedy of the same runtime, our predicted Tomato Meter rating goes up by 4.43, **regardless of the actual value of runtime**. If we increase movie runtime by one minute while keeping genre the same, our predicted Tomato Meter rating goes up by 0.41, **regardless of whether that genre is action or comedy**. 

It may help to graphically visualize the nature of this additive relationship. We can do this by plotting lines showing the relationship between runtime and Tomato Meter ratings separately for our two different genres of action and comedy. The line for action movies is given by:

$$\hat{meter}_i=-1.75+4.43(0)+0.41(runtime_i)=-1.75+0.41(runtime_i)$$

The line for comedy movies is given by:

$$\hat{meter}_i=-1.75+4.43(1)+0.41(runtime_i)=2.68+0.41(runtime_i)$$

Each line has an intercept and a slope. Notice that the intercepts are different but the slopes are the same. That means we have two parallel lines at different levels. You can see this easily by graphing the lines out:

```{r plot_nointeraction_twogenre, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(-1,-1, ylim=c(20,80), xlim=c(70,190),
     xlab="runtime in minutes", ylab="Predicted tomato meter")
abline(-1.74,0.41, lwd=2, col="red")
abline(2.68, 0.41, lwd=2, col="blue")
legend(80,80, lty=1, col=c("red","blue"), legend=c("Action","Comedy"))
```

The parallel lines are an assumption of the OLS regression model structure we have used. There are two consequences of this assumption. First, At every single level of runtime, the predicted Tomato Meter difference between comedy and action movies is exactly 4.62. This can be seen on the graph by the consistent gap between the blue and red line. Second, the effect of runtime on the Tomato Meter rating is assumed to be the same for action and comedy movies. This can be seen on the graph by the fact that both lines have the exact same slope. 

Although these may seem like two different issues, they are really the same issue from different perspectives. If we were to allow the slopes of the blue and red line to be different, then the gap between them would not be static. The questions is how can we allow the slopes of the two lines to be different. This is where the concept of the **interaction term** comes in. 

###The interaction term

An interaction term is a variable that is constructed from two other variables by multiplying those two variables together. In our case, we can easily construct an interaction term as follows:

```{r}
movies.short$comedy <- movies.short$Genre=="Comedy"
movies.short$interaction <- movies.short$Runtime*movies.short$comedy
```

In this case, I had to create a real indicator variable for comedy before I could multiply them, but then I just multiply this indicator variable by movie runtime. Now lets add this interaction term to the model:

```{r}
model <- lm(TomatoMeter~Runtime+comedy+interaction, data=movies.short)
round(coef(model), 2)
```

We now have an additional "slope" for the interaction term. Lets write this model out in equation form to try to figure out what is going on here. 

$$\hat{meter}_i=-14.45+24.36(comedy_i)+0.52(runtime_i)-0.19(runtime_i*comedy_i)$$


Remember that the interaction term is just a literal multiplication of the two other variables. To figure out how this all works, lets once again separate this into two lines predicting Tomato Meter by runtime, for comedies and action movies separately.

For action movies, the equation is:

$$\hat{meter}_i=-14.45+24.36(0)+0.52(runtime_i)-0.19(runtime_i*0)=-14.45+0.52(runtime_i)$$

For comedy movies, the equation is:

$$\hat{meter}_i=-14.45+24.36(1)+0.52(runtime_i)-0.19(runtime_i*1)=(-14.45+24.36)+(0.52-0.19)(runtime_i)=9.91+0.33(run_i)$$

We now have two lines with different intercept and **different slopes**. The interaction term has allowed the effect of runtime on the Tomato Meter to vary by type of genre. In this case, the interaction term tells us how much smaller the slope is for comedy movies than for action movies. We can also just plot the lines to see how it looks:

```{r plot_interaction_twogenre, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(-1,-1, ylim=c(20,80), xlim=c(70,190),
     xlab="runtime in minutes", ylab="Predicted tomato meter")
abline(-14.45,0.52, lwd=2, col="red")
abline(9.91, 0.33, lwd=2, col="blue")
legend(80,80, lty=1, col=c("red","blue"), legend=c("Action","Comedy"))
```

The pattern here is fairly clear. Short comedies get better ratings than short action movies, while long comedies get worse ratings than long action movies. Put another way, comedies get less "return" in terms of their ratings when increasing their length than do action movies. This can be seen by the much steeper slope for action movies. 

###Interpreting interaction terms

Interpreting interaction terms can be tricky, because the inclusion of an interaction term **also changes the meaning of other slopes in the model**. The slopes for the two variables that make up the interaction term are called the **main effects**. In our example, those two variables are runtime and the comedy indicator variable and the main effects of these variables are 0.52 and 24.36, respectively. The most important rule to remember is that when an interaction term is in a model, the main effects are only the expected effects **when the other variable involved in the interaction is zero**. This is because the interaction implies that the effects of the two variables are not constant but rather change depending on the value of the other variable in the interaction term. Therefore, we can only interpret effects at a particular value of the other variable. So I would interpret these main effects as follows:

- The model predicts that **among action movies**, a one minute increase in movie runtime is associated with a 0.52 point increase in the Tomato Meter rating, on average. 
- The model predicts that **among movies with zero minutes of runtime** (outside the scope of data of course), comedies are predicted to have Tomato Meter ratings 24.36 points higher than action movies, on average.

Notice that I did not have to say I was controlling for the other variable. I am doing more than controlling when I include an interaction term. I am conditioning the effect of one variable on the value of another. That is why I instead use the phrase "among observations that are zero on the other variable." Note that you could also include other non-interacted variables in this model as well, like maturity rating, in which case you would also need to indicate that you controlled for those variables. 

Interpreting interaction terms themselves can also be tricky because they are the difference in the effect of on variable depending on the value of another. One approach is to interpret this difference in effect directly. In this case, we would say:

- The model predicts that the predicted increase in Tomato Meter ratings for a one minute increase in movie runtime is 0.19 points smaller for comedy movies than for action movies, on average.

You have to be careful with this type of interpretation. In this case, both slopes were still positive so I can talk about how the effect was smaller. However, in some cases, the slopes may end up in different directions entirely which would require a somewhat different interpretation. 
Another approach is to actually calculate the slope for the indicated category (comedies) and interpret it directly:

- The model predicts that **among comedy movies**, a one minute increase in movie runtime is associated with a 0.33 increase in the Tomato Meter rating, on average (which is lower than for action movies). 

In short, you have to be careful and thoughtful when thinking about how to interpret interaction terms. 

###Interaction terms in *R*

In the example above, I created the interaction term manually, but I didn't actually need to do this. *R* has a shortcut method for calculating interaction terms:

```{r}
model <- lm(TomatoMeter~Runtime*Genre, data=movies.short) 
round(coef(model),2)
```

The results are exactly the same as before. To include an interaction term between two variables I just have to connect them with a `*` rather than a `+` in the `lm` formula. By default, *R* will include each variable separately as well as their interaction. 

###Interaction terms with multiple categories

In the above example, I only compared comedy and action movies in order to keep the comparison simple, but it is possible to run the same analysis on the full movie dataset to see how runtime varies across all genres. 

```{r}
model <- lm(TomatoMeter~Runtime*Genre, data=movies)
round(coef(model),2)
```

Thats a lot of numbers! There is a slope for each genre except action (10 in all) and an interaction between runtime and each genre except action (another 10 in all). What we are estimating here are 11 different lines (on for each genre) for the relationship between runtime and Tomato Meter rating. Because action movies are the reference, the main effect of runtime is the slope for action movies (0.52). The interaction terms show us how much larger or smaller the effect of runtime is for each given genre. So the effect is 0.39 smaller for dramas for a total effect of 0.13 (0.52-0.39). It is 0.32 larger for family movies for a total effect of 0.84 (0.52+0.32), and so forth. Similarly, the intercept is the intercept only for action movies. To get the intercept for other genres, we take the intercept value itself and add the main effect of genre. So for dramas the intercept is -14.45+59.5=45.05 and for family movies it is -14.45-29.06=-43.51. If we put all these slopes and intercepts together, we will get 11 lines like so:

```{r plot_fullinteraction_genre, echo=FALSE, fig.width=5, fig.height=4, out.width='500px', out.height='400px', dpi=300}
plot(-1,-1, ylim=c(0,120), xlim=c(70,190),
     xlab="runtime in minutes", ylab="Predicted tomato meter")
model <- lm(TomatoMeter~Runtime*Genre, data=movies)
intercepts <- coef(model)[1]+c(0,coef(model)[3:12])
slopes <- coef(model)[2]+c(0,coef(model)[13:22])
cols <- c("black","seagreen","yellow","brown","red","orange",
          "blue","skyblue","violet","slategray","indianred")
for(i in 1:length(intercepts)) {
  abline(intercepts[i],slopes[i], lwd=2, col=cols[i])
}
legend(80,20, legend=levels(movies$Genre),lty=1, col=cols, ncol=4, cex=0.5)
```

There is a lot going on here, but we can detect some interesting patterns. Almost all of the lines are positive indicating that longer movies tend to generally get better ratings. This is not true of Romances however, where there is a slight negative relationship between movie runtime and Tomato Meter ratings. Dramas also have a fairly flat slope and a high intercept, so they tend to outperform most other short movies but don't fare as well compared to other genres when they are longer. The steepest slope is for family movies, which apparently are horrible when short (think "Beethoven 6: Beethoven saves Christmas, again" or something), but do much better when longer (although I can't think of many examples here). SciFi/Fantasy movies have the highest ratings at every single runtime, although they don't get quite as much return from runtime as family movies.  

###Interaction terms with two categorical variables

The examples so far have involved interacting a quantitative variable with a categorical variable which gives you a different line for each category of your categorical variable. However, we can also create an interaction term between two categorical variables. 

As an example, lets look at differences in income in the politics dataset by race and education. To simplify things, I am going to dichotimize race into white/non-white and education into less than college/college or more, as follows:

```{r}
politics$nwhite <- politics$race!="White"
politics$college <- as.numeric(politics$educ)>3
```

Lets look at the mean income across these combination of categories:

```{r}
tapply(politics$income, politics[,c("nwhite","college")], mean)
```

White college graduates make \$94K, on average, while non-white college graduates make \$76K, on average. Whites without a college degree make \$52K, on average, while non-whites without a college degree make \$36K, on average. If we put this in a table, I can show that there are four different ways to make comparisons between these numbers. 

```{r echo=FALSE}
tab <- round(tapply(politics$income, politics[,c("nwhite","college")], mean),1)
tab <- cbind(tab, tab[,2]-tab[,1])
tab <- rbind(tab, tab[2,]-tab[1,])
colnames(tab) <- c("No degree", "College degree", "Difference")
rownames(tab) <- c("White","Non-white","Difference")
set.alignment(c("right","right","right"))
pander(tab)
```

If we look at the two differences along the far-right column, we are seeing the "returns" in terms of income for a college degree separately for whites and non-whites. The return for whites is \$42.8K and the return for non-whites is slightly lower at \$40.1K. if we look at the differences along the bottom row, we are seeing the racial inequality in income separately for those with no degree and those with a college degree. Among those with no college degree, non-whites make \$15.5K less than whites. Among those with a college degree, non-whites make \$18.2K less than whites. The racial gap in income gets slightly larger at the higher level of education. 

Now lets look at the difference in the differences. For the racial gap in income this is given by -15.5-(-18.2)=2.7. For the returns to a college degree this is given by 42.8-40.1=2.7. The difference in the differences is the same! This is because we are looking at the same relationship in two different ways. If non-whites get a weaker return to college than whites, then the racial gap in income must get larger among the college-educated. Similarly, if the racial gap in income gets larger at the college level, it tells us that non-whites must get a weaker return on their college education. 

This 2.7 number is basically an interaction term. We can interpret the number as the difference in returns to income from a college degree between whites and non-whites. Alternatively, we can interpret the number as the difference in the racial income gap between those with no degree and those with a college degree. Either way, we have the same information, with the same somewhat depressing finding: greater educational attainment does not necessarily reduce racial inequality because minorities get less return on their college degrees. 

Lets try modeling this relationship with an OLS regression model. First lets try a model without interaction terms:

```{r}
model <- lm(income~nwhite+college, data=politics)
coef(model)
```

Lets put this into an equation framework:

$$\hat{income}_i=52.0-16.2(nwhite_i)+41.9(college_i)$$

We can use this equation to fill in the predicted valued of the same table we calculated by hand above:

```{r echo=FALSE}
tab <- cbind(c(52.0,"52.0-16.2=35.8",-16.2),
      c("52.0+41.9=93.9","52.0-16.2+41.9=77.7",-16.2),
      c(41.9,41.9,0))
colnames(tab) <- c("No degree", "College degree", "Difference")
rownames(tab) <- c("White","Non-white","Difference")
set.alignment(c("right","right","right"))
pander(tab)
```

The predicted values do not match the exact values above. More importantly, if you look at the differences, you can see that the returns to education are assumed to be identical for whites and non-whites (41.9) and the racial gap is assumed to be the same for those with no degree and those with a college degree (-16.2). This is the limitation of the additive model. We assume that the effects of race and college completion are not affected by each other. If we want to determine whether returns to college are different by race, we need to model the interaction term, as follows:

```{r}
model <- lm(income~nwhite*college, data=politics)
coef(model)
```

In equation form:

$$\hat{income}_i=51.7-15.5(nwhite_i)+42.8(college_i)-2.6(nwhite_i*college_i)$$

Lets use this model to get predicted values in our table:

```{r echo=FALSE}
tab <- cbind(c(51.7,"51.7-15.5=36.2",-15.5),
      c("51.7+42.8=94.5","51.7-15.5+42.8-2.6=76.4",-18.1),
      c(42.8,40.2,-2.6))
colnames(tab) <- c("No degree", "College degree", "Difference")
rownames(tab) <- c("White","Non-white","Difference")
set.alignment(c("right","right","right"))
pander(tab)
```

Our model now fits the data exactly (except for some rounding error) and the differences are allowed to vary by the other category, so that we can see the differences in returns to college by race and the differences in the racial gap by education level. The interaction term itself of -2.6 is basically the same to what we calculated by hand (adjusted for some rounding error). 

If we were to interpret the intercept and slopes from the model above, we would say:

- Whites with no college degree had a mean income of $51,700. 
- Among those with no college degree, non-whites earn $15,500 less than whites, on average. 
- Among whites, those with a college degree have incomes $42,800 higher on average than those without a college degree.
- The returns to income from a college degree are $2,600 smaller for non-whites than they are for whites, on average. 

------

##Transformations (Advanced)

You **transform** your data when you apply a mathematical function to a variable to transform its values into different values. There are a variety of different transformations that are commonly used in statistics, but for this class we will focus on the one transformation that is most common in the social sciences: the log transformation.

Why would you want to transform your data? There are two important potential benefits that transformations can provide. First, a transformation can often resolve the problem of non-linear relationships. If the relationship between $x$ and $y$ is non-linear, then by transforming one or both variables, you may be able to recover a linear relationship. Second, transformation can reduce skewness in a variable and will pull in extreme outliers so that they are less influential. For these reasons, transformations can be useful in regression models. However, it is important to also remember that a transformation changes the way in which x and y relate to one another and thus requires us to adjust our interpretation of results. 

###The natural log transformation

When I talk about the "log" transformation, I am talking about what you probably learned as the "natural log" transformation. This transformation is given to you by the "ln" button on your calculator. For our purposes, this is the only "log" we care about. Just remember that in Eugene, we go natural. 

Any positive number can be logged. For example, I can calculate the log of the number 7:

```{r}
log(7)
```
OK, the natural log of 7 is `r log(7)`. But what does that mean? The natural log of a number is defined as the value you would have to raise the constant $e$ (2.718282) to in order to get back the original number. To raise $e$ by some number, you can use the `exp` function in R ("exp" for "exponential"):

```{r}
exp(log(7))
```

Ta-da! If I raise $e$ to the log of 7, I get back 7. The number $e$ is a very special number like $\pi$ having to do with what happens when you compound interest continually over time, but none of that matters for our purposes. For our purposes, what matters is that by logging a number you can make a multiplicative relationship into an additive relationship. This is because of a basic mathematical relationship where:

$$e^a*a^b=e^{a+b}$$
$$log(x+y) = log(x)+log(y)$$
You can try this out in R to see that it works:

```{r}
exp(2)*exp(3)
exp(2+3)
log(5*4)
log(5)+log(4)
```
This is really all you need to know about the log transformation to understand this section.

###Log-transformations allow us to estimate multiplicative models

Take a look at the histogram of the income variable from our politics dataset. 

```{r echo=FALSE}
hist(politics$income, breaks=seq(from=0,to=250,by=10), col="red", las=1, 
     xlab="income in 1000s", main="Histogram of income")
```

This variable is heavily right skewed, with a few very high earners at the top end of the distribution, and the vast majority of individuals making less than \$100,000 per year. The heavy skew makes this distribution is an ideal candidate for the log transformation. Lets go ahead and log-transform it and save our log-transformation as another variable called "lincome."

```{r}
politics$lincome = log(politics$income)
```

Now lets look at the distribution of this log-transformed income data:

```{r echo=FALSE}
hist(politics$lincome, col="red", las=1, breaks=20, 
     xlab="income in 1000s (log-scale)", main="Histogram of income (logged)")
```

Now we are getting a slight outlier in the opposite direction, but in general the tail ends of our distribution are pulled in considerably. This suggests that we have to worry less about how outliers might affect our results. 

Now lets try putting this log-transformed income variable in as the dependent variable and lets predict it by the age of the respondent. 

```{r}
summary(lm(lincome~age, data=politics))$coef
```
OK, so what does that mean? It might be tempting to interpret the results here as you normally would. We can see age has a positive effect. So, a one year increase in age is associated with a 0.005 increase in ... what? Remember that our dependent variable here is log-income. We could literally say that it is a 0.005 increase in log-income, but that is not a very helpful or intuitive way to think about the result. Similarly the intercept gives us the predicted log-income when age is zero. Thats not helpful for two reasons: its outside the scope of the data, and we don't really know how to think about a log-income of 3.29. 

In order to translate this into something meaningful, lets try looking at this in our equation format. Here is what we have:

$$\log(\hat{y}_i)=3.29+0.005*x_i$$

What we really want is to be able to understand this equation back on the original scale of the dependent variable, which in this case is income. Remember that taking $e^{\log(y)}$ just gives us back $y$. We can use that logic here. If we "exponentiate" (take $e$ to the power of the values) the left-hand side of the equation, then we can get back to $\hat{y}_i$. However, remember from algebra, that what we do to one side of the equation, we have to do to both sides. That means:

$$e^{\log(\hat{y}_i)}=e^{3.29+0.005*x_i}$$
$$\hat{y}_i=(e^{3.29})*(e^{0.005})^{x_i}$$
The good news is that we now just have our predicted income on the left-hand side. The bad news is that the right hand side looks a bit complex. Since $e$ is just a number we can go ahead and calculate the values in those parentheses (called "exponentiating"):

```{r}
exp(3.29)
exp(0.005)
```

That means:

$$\hat{y}_i=(26.8)*(1.005)^{x_i}$$
What we have here is a **multiplicative** relationship rather than an additive relationship. How does this changes things? Well, to see lets plug in some values for $x_i$ and see how it changes our predicted income value. An age of zero is outside the scope of our data, but lets plug it in for instructional purposes anyway:

$$\hat{y}_i=(26.8)*(1.005)^{0}=(26.8)(1)=26.8$$
So, the predicted income when $x$ is zero is just given by exponentiating the intercept. Lets try increasing age by one year:

$$\hat{y}_i=(26.8)*(1.005)^{1}=(26.8)(1.005)$$
I could go ahead and finish that multiplication, but I want to leave it here to better show the change. A one year increase increases income by a **multiplicative** factor of 1.005. In other words, a one year increase in age is associated with a 0.5% increase in income, on average. What happens if I add another year?

$$\hat{y}_i=(26.8)*(1.005)^{2}=(26.8)(1.005)(1.005)$$
Each additional year leads to a 0.5% increase in predicted income. This is what I mean by a multiplicative increase. We are no longer talking about the predicted change in income in terms of **absolute** numbers of dollars, but rather in **relative** terms of percentage increase. 

### General form and interpretation

In general, you have the following equation when you transform your dependent variable:

$$\log(\hat{y}_i)=b_0+b_1(x_{i1})+b_2(x_{i2})+\ldots+b_p(x_{ip})$$

In order to properly interpret your results, you must exponentiate all of your slopes and the intercept. Your slopes can be interpreted as:

> The model predicts that a one-unit increase in $x_j$ is associated with a $e^{b_j}$ multiplicative increase in $y$, on average while holding all other independent variables constant. 
> The model predicts that $y$ will be $e^{b_0}$ on average when all independent variables are zero. 

Of course, just like all of our prior examples, you are responsible for converting this into sensible English. 

Lets try a fuller example where we predict log income by age, education and race at the same time:

```{r}
round(summary(lm(lincome~age+educ+race, data=politics))$coef,3)
```

Lets start by interpreting the intercept. Two of our variables, education and race, are categorical with reference categories of less than high school and white, respectively and age is a quantitative variable. In order to interpret our intercept of 2.73, we first need to exponentiate it:

```{r}
exp(2.73)
```

So we would say:

> The model predicts that a zero-year old white person with less than a high school degree will make \$15,333, on average. 

Of course, because an age of zero is outside the scope of our data, we don't put much any stock in this prediction. The more interesting numbers are the various slopes. 

Lets interpret, the age slope of 0.004. First, exponentiate:

```{r}
exp(0.004)
```

So, we would say:

> The model predicts that a one year increase in age is associated with a 0.4% increase in income, on average, among individuals of the same race and educational level. 

What about the effect for a BA degree? This is a categorical variable with less than high school as the reference, but again we need to exponentiate to get a meaningful number:

```{r}
exp(1.19)
```

This is a pretty big increase! We might say:

> The model predicts that the incomes of college graduates are 3.28 times higher than the incomes of those with less than a high school diploma, on average, holding constant race and age. 

How would we describe negative effects. Lets look at the effect of -0.23 for Hispanics. First, lets exponentiate it:

```{r}
exp(-0.23)
```

There are two ways we could describe this. Let me try both ways:

> The model predicts that the incomes of Hispanics will be 79% as high as the incomes of whites, on average, holding constant age and education.

Keep in mind that 79% as high means you are making less. If your boss comes and offers you to change your salary to 90% of what it is now, don't be fooled -- thats not a good deal. Alternatively, I could have taken 100-79=21 and said:

> The model predicts that the income of Hispanics is 21% less than the income of whites, on average, holding constant age and education. 

Always keep in mind that regardless of the type of variable involved and the direction of the relationship, we are always talking about a relative, multiplicative change in the dependent variable. In some cases, it may make more sense to describe this as a percentage gain or loss (as in the age and Hispanic case), while in other cases it may make sense to describe it in terms of how many "times more or less" (as in the case of the BA degree).

*Note: from here on out, I am throwing together information discussed in class, so the descriptions may be a bit rough. Apologies in advance*

### Logging the independent variable 

Lets revisit the data from Preston that we looked at earlier in the term. 

```{r preston2, echo=FALSE}
par(mar=c(4,4,0.1,1))
plot(preston$inc, preston$lifeexp,
     xlab="national income, 1960",
     ylab="life expectancy, 1960", pch=21, bg="grey", las=1)
```
These data show a clearly non-linear relationship. More specifically, this is a "diminishing returns" relationship where a positive effect gets smaller in magnitude at higher levels of the independent variable. Logging the dependent variable will not help us make this relationship look more linear, but logging the independent variable will:

```{r preston3, echo=FALSE}
par(mar=c(4,4,0.1,1))
plot(log(preston$inc), preston$lifeexp,
     xlab="national income, 1960",
     ylab="life expectancy, 1960", pch=21, bg="grey", las=1)
```

How does this transformation of the independent variable affect how we interpret the results? Lets run the model:

```{r prestonmodel}
summary(lm(lifeexp~log(inc), data=preston))
```

Interpreting the slope here can be tricky. The basic point is that by logging the independent variable, the change in the independent variable is now relative as measured by a 1% increase in the independent variable (national income per capita here). The change in the dependent variable is still in absolute terms (years of life expectancy here), but in order to interpret the slope correctly you must divide it by 100 (i.e. move the decimal place two places to the left). In this case, I would say:

> The model predicts that a 1% increase in national income per capita in a country is associated with a 0.075 year increase in life expectancy on average. 

Why did I have to move the decimal place two to the left? Lets use the model to compare the predicted values of two cases, where one case has exactly 1% higher national income per capita, to see how this works. I will start the lower country at a national income per capita of \$20,000, but the results here apply regardless of what number I choose here. To see the change I subtract one predicted value from the other. 

$$
\begin{aligned} 
((14.93+7.54*\log(20200))-(14.93+7.54*\log(20000))&=7.54(\log(20200)-\log(20000))\\
&=7.54*\log(20200/20000)\\ 
&=7.54*\log(1.01)
\end{aligned}
$$

Now, it turns out that $log(1.01)$ almost exactly equals 0.01, so this is roughly equivalent to $7.54*0.01=0.0754$. This same math will be true regardless of the starting value of income chosen, so roughly speaking a 1% increase in $x$ is associated with a $b_1/100$ change in $y$. 

### Logging both independent and dependent variables: The elasticity model

So, now we have seen that logging the dependent variable will make change in the dependent relative, and logging the independent variable will make change in the independent variable relative. It makes sense to think that if you log them both, you would get relative change in $x$ predicting relative change in $y$. Correct! This is what is called an "elasticity" model because the predicted slopes are equivalent to the concept of elasticity in economics: how much does of a percent change in $y$ results from a 1% increase in $x$. 

To show you how this works, lets try to predict movie box office returns by tomato ratings where we apply all three types of models.

```{r}
model.logy <- lm(log(BoxOffice)~TomatoRating, data=movies)
model.logx <- lm(BoxOffice~log(TomatoRating), data=movies)
model.logboth <- lm(log(BoxOffice)~log(TomatoRating), data=movies)
coef(model.logy)[2]
coef(model.logx)[2]
coef(model.logboth)[2]
```

In the first model, I am logging $y$ so I nee to exponentiate the result to interpret it. 

```{r}
exp(0.2012)
```

So, I would say:

> The model predicts that a one point increase in a movie's tomato rating is associated with a 22% increase in box office returns on average. 

In the second model, I need to move over the decimal place to the left and then say:

> The model predicts that a 1% increase in a movie's tomato rating is associated with a \$500,000 increase in box office returns. 

The third model (the elasticity model) is the easiest to interpret. It turns out that the number can be interpreted directly as the percentage change in $y$ expected for a 1 percent increase in $x$, so:

> The model predicts that a 1% increase in a movie's tomato rating is associated with a 0.85% increase in box office returns. 

Why is this the case? We can work this out from a similar mathematical exercise above. On the independent variable side a 1% increase in $x$ is still associated with a $b_1/100$ increase in the dependent variable, but that independent variable is still the log of $y$. Thus, technically you should get the result by taking $e^{b_1/100}$. However, since you are dividing by 100 that number will almost always be small enough that you can use the approximation that $b_1$ itself is the percentage increase in $y$ for the given change in $x$. 

### The square root transformation

The log transformation is very flexible and solves multiple problems at once (non-linearity, outliers, skewness), which explains its popularity. But it breaks down in one important situation: you cannot log a variable that has zero or negative values. The negative case is not as important because generally the log transformation fixes things for variables that only take non-negative values. However, there are numerous cases where a quantitative variable can be zero as well as a positive. Lets run the same elasticity model as above on box office returns, but this time lets predict returns by the Tomato Meter rather than the Tomato Rating.

```{r error=TRUE}
summary(lm(log(BoxOffice)~log(TomatoMeter), data=movies))
```
Oh no! We got an error. The problem is that the Tomato Meter has a few cases of zero values (when a movie received zero positive reviews). The log of zero is negative infinity and that simply won't work when fitting a linear model. What can you do?

Well it turns out that the square root transformation can do much the same work as the natural logarithm. It will pull in skewness and can make non-linear relationships more linear. Since the square root of zero is a real number (zero to be precise), it will also work on variables that have legitimate zeroes. So,

```{r}
summary(lm(log(BoxOffice)~sqrt(TomatoMeter), data=movies))
```

Now we can get a result. The downside, however, is that there is [no clear and easy interpretation of how to intepret this effect](http://stats.stackexchange.com/questions/35982/how-to-interpret-regression-coefficients-when-response-was-transformed-by-the-4t).  

### Other methods to deal with non-linearity

Transformations are one way to handle non-linearity in the relationship between $x$ and $y$. There are other common ways that this non-linearity can also be handled. We will cover the case of smoothing and polynomial regression here. Another case that we will not cover here is the use of splines, although some of the smoothing techniques covered here will use splines implicitly. 

#### Smoothing

Smoothing is primarily a graphical technique that can be used to diagnostically detect non-linearity in a relationship. Lets plot the relationship between movie tomato rating and box office returns. 

```{r echo=FALSE}
plot(movies$TomatoRating, movies$BoxOffice, pch=21, bg="grey", col=NULL,
     las=1, xlab="Tomato Rating", ylab="Box Office Returns (millions)")
```
There is so much overplotting and box office returns are so skewed here that it is quite difficult to visually see the relationship. Smoothing will help us do that. There are numerous ways that one can smooth data, but the basic idea is that you replace the $y$ value for an observation with a substitute value that incorporates information about the adjacent neighbors (in terms of $x$) for this observation. 

To see how this works, the figure below picks out one movie that had particularly anomalous box office returns given its tomato rating (shown in red) and the two movies that were most immediately adjacent to this value in terms of their tomato rating. It then takes the mean box office returns between the three movies and plots this as the smoothed mean value for the selected movie in green. 

```{r echo=FALSE}
movies2 <- movies[order(movies$TomatoRating),]
plot(movies$TomatoRating, movies$BoxOffice, pch=21, bg="grey90", col=NULL,
     las=1, xlab="Tomato Rating", ylab="Box Office Returns (millions)")
points(movies2$TomatoRating[450], movies2$BoxOffice[450], pch=21, bg="red")
text(movies2$TomatoRating[450], movies2$BoxOffice[450], movies2$Title[450], cex=0.5, pos=1)
points(movies2$TomatoRating[c(449,451)], movies2$BoxOffice[c(449,451)], pch=21, bg="grey10")
text(movies2$TomatoRating[c(449,451)], movies2$BoxOffice[c(449,451)], movies2$Title[c(449,451)], cex=0.5, pos=2)
points(movies2$TomatoRating[450], mean(movies2$BoxOffice[449:451]), pch=21, bg="green")
```

In practice this kind of mean smoothing (also called a "running average") or median smoothing only works well for time series values where there is only one unique value of $x$ for each observation, whereas we have many movies with the exact same tomato rating. In practice, a better smoothing approach for data like this is to use more complex methods for smoothing that involve splines and local polynomial regression to get predicted values. Three methods that will do this in R are shown below. For the `loess` and `smooth.spline` functions, you have to declare how many adjacent observations you want to consider as a proportion of the total dataset. I have chosen 10% and 75%. The wider you make this span, the smoother the line will get at the cost of potentially losing important spikes and dips. The "supersmoother" function `supsmu` is simpler and tries to determine the best span internally. 

```{r}
#for loess its important to first order the movies by x
movies <- movies[order(movies$TomatoRating),]
plot(movies$TomatoRating, movies$BoxOffice, pch=21, bg="grey90", col=NULL,
     las=1, xlab="Tomato Rating", ylab="Box Office Returns (millions)")
smooth.loess <- loess(BoxOffice~TomatoRating, data=movies, span=0.1)
lines(smooth.loess$x, smooth.loess$fitted, col="red", lwd=3)
smooth.loess <- loess(BoxOffice~TomatoRating, data=movies, span=0.75)
lines(smooth.loess$x, smooth.loess$fitted, col="orange", lwd=3)
lines(smooth.spline(movies$TomatoRating, movies$BoxOffice, spar=0.1), lwd=2, col="blue")
lines(smooth.spline(movies$TomatoRating, movies$BoxOffice, spar=0.75), lwd=2, col="darkgreen")
lines(supsmu(movies$TomatoRating, movies$BoxOffice), col="yellow", lwd=2)
legend(2, 700, legend=c("Lowess, 10% span", "Lowess, 75% span", "Spline smoothing, 10%", 
                        "Spline smoothing, 75%", "Supersmoother"),
       lwd=1, lty=1, col=c("red","orange","blue","darkgreen","yellow"), cex=0.6)
```

All the smoothers here indicate an exponential type relationship that would be better fit by logging the dependent variable. 

#### Polynomial Regression

A final method that can be used to fit non-linear relationships is to fit *polynomial* terms. Recall, for example, the formula:

$$y=a+bx+cx^2$$

This function defines a parabola which fits not a straight line but a curve with one point of inflection. We can fit this sort of curve in an OLS regression model by simply including the square of a variable as an additional term in the model. Before we do this it is usually a good idea to center the variable to be squared somewhere around the mean because this will reduce collinearity between the original term and its square. 

For example, I could fit a polynomial term to a model that predicts tomato meter by movie runtime. like so:

```{r}
summary(lm(TomatoMeter~I(Runtime-90)+I((Runtime-90)^2), data=movies))
```

Interpreting these numbers directly can be quite tricky. The easiest approach is often to simply graph the resulting parabola for reasonable values of $x$. I can do that here for movies:

```{r}
x <- 70:620-90
fitted <- 42.256+0.406*x-0.0004635*x^2
plot(x, fitted, type="l", lwd=2, xaxt="n", las=1,
     xlab="runtime (in minutes)", ylab="Predicted tomato meter")
ticks <- seq(from=70, to=620, by=50)-90
axis(1, at=ticks, labels=ticks+90)
abline(v=528-90, lty=2)
```

Note that we now get a curvilinear relationship where the positive effect of runtime gets smaller at higher values of runtime much like a diminishing returns relationship. In this case, the effect of runtime can even reverse direction and become negative which is not possible with a log transformation on $x$. However, its worth noting that the effect of runtime doesn't become negative until we are well outside the range of real movie values. 

You can actually mathematically figure out the exact inflection point based on the two "slopes" for the original term and its square. Given the following model:

$$y=b_0+b_1*x+b_2*x^2$$

The inflection point is given by:

$$b_1/(-2 * b_2)$$

In this case, that gives us an inflection point of:

$$0.4060329/(2*0.0004635)=438$$

However, because we subtracted 90 from each value, the actual runtime minutes for the inflection is equal to $438+90=528$. Note that this value is show on the graph above. 
