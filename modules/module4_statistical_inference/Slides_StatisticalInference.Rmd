---
title: "Statistical Inference"
author: "Prof. Gullickson, University of Oregon, Winter 2019"
resource_files:
output:
  ioslides_presentation:
    css: lecture_slides.css
    fig_caption: yes
    fig_height: 4
    fig_width: 6
    logo: images/slides/logo.png
    widescreen: yes
subtitle: Sociology 312, Statistical Analysis
runtime: shiny
---

<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

```{r echo=FALSE}
load("example_datasets/movies/movies.RData")
load("example_datasets/sex/sex.RData")
load("example_datasets/crimes/crimes.RData")
load("example_datasets/titanic/titanic.RData")
load("example_datasets/politics/politics.RData")
load("example_datasets/students/studentsCurrent.RData")
load("example_datasets/students/studentsamplingdist.RData")
```

# The problem of statistical inference
Statistical Inference

## Statistical Inference
<div class="footer">
<body>Sociology 312, Statistical Inference: The problem of statistical inference</body>
</div>

>- The data that we work with often constitutes a **sample** from a larger **population**. Our interest is not in the sample itself, but rather in the population. 

>- We use our sample statistics to **infer** what the true values are in the population, but we have to recognize that our sampling statistics may not exactly match the population numbers because of random chance.

>- Statistical inference is the technique of quantifying our uncertainty about the true values in the population. 

>- Lets take the example of the slope between sexual frequency and age from our sexual frequency data. The data came from the General Social Survey (GSS), a **representative** sample of the US population.  

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The problem of statistical inference</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=6, out.width='800px', out.height='600px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
par(mar=c(0,0,0,0))
plot(-2,-2, xlim=c(0,10), ylim=c(0,10), xaxt="n", yaxt="n", bty="n", xlab="", ylab="")
rect(6,1,10,9, col="skyblue")
text(8,5,"US Population", cex=1.2)
text(8,4,expression(beta[1]%~~%"?"))
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The problem of statistical inference</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=6, out.width='800px', out.height='600px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
par(mar=c(0,0,0,0))
plot(-2,-2, xlim=c(0,10), ylim=c(0,10), xaxt="n", yaxt="n", bty="n", xlab="", ylab="")
rect(6,1,10,9, col="skyblue")
text(8,5,"US Population", cex=1.2)
text(8,4,expression(beta[1]%~~%"?"))
rect(0,3,4,7, col="yellow")
text(2,5,"GSS Sample", cex=1.2)
text(2,4,expression(b[1]==-1.3))
arrows(6,4,4,4)
text(5,3.5,"Draw a sample", cex=0.7)
```


----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The problem of statistical inference</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=6, out.width='800px', out.height='600px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
par(mar=c(0,0,0,0))
plot(-2,-2, xlim=c(0,10), ylim=c(0,10), xaxt="n", yaxt="n", bty="n", xlab="", ylab="")
rect(6,1,10,9, col="skyblue")
text(8,5,"US Population", cex=1.2)
text(8,4,expression(beta[1]%~~%-1.3))
rect(0,3,4,7, col="yellow")
text(2,5,"GSS Sample", cex=1.2)
text(2,4,expression(b[1]==-1.3))
arrows(6,4,4,4)
text(5,3.5,"Draw a sample", cex=0.7)
arrows(4,6,6,6)
text(5,5.5,"Infer from sample", cex=0.7)
```

## Populations and Samples
<div class="footer">
<body>Sociology 312, Statistical Inference: The problem of statistical inference</body>
</div>

>- We refer to the unknown value in the poplation that we want to know about as a **parameter**. We represent this value with a greek letter (e.g. $\beta_1$)
>- We refer to the value in the sample that is an estimate of the population value as a **statistic**. We represent this value with a roman letter (e.g. $b_1$). 

## How do we know if our sample is any good?
<div class="footer">
<body>Sociology 312, Statistical Inference: The problem of statistical inference</body>
</div>

>- There are two sources of potential **bias** that may cause our sample statistic to be different from the population parameter. 
>- **Systematic bias** means that something about our sampling procedure biases our results systematically. Perhaps we are unable to sample really wealthy people or perhaps the question wording predisposes people to a certain answer. Systematic bias is a real problem but can be minimized in well-designed and executed surveys. 
>- **Random bias** means that *just by random chance* we happened to draw a sample that is very different from the population on the parameter we care about. There is nothing we can do to prevent this from happening regardless of how well our study is conducted. The only way to minimize random bias is to draw a bigger sample. 

## The real kicker
<div class="footer">
<body>Sociology 312, Statistical Inference: The problem of statistical inference</body>
</div>

>- Even though we know random bias *might* have caused our sample statistic to be different from the population mean, we can't know for sure if this happened or not because we don't know the true population parameter!
>- This is what statistical inference is all about. Even though we can never say for certain that our results are close or far away from the true value, we can quantify our uncertainty about how different the sample statistic could potentially be from the population parameter.

# The concept of the sampling distribution
Statistical Inference

## Three kinds of distributions {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

Lets say you want to know the mean years of education in the United States. You draw a simple random sample of 100 people. There are three kinds of distributions involved in this process.

>- **Population Distribution** - The distribution of years of education for the whole US poplation. Its mean is given by $\mu$. The population mean and distribution are unknown. 
>- **Sample Distribution** - The distribution of years of education in your sample. The mean is given by $\bar{x}$. The mean and distribution are known because we have this sample. Hopefully, this mean and distribution should approximate the population distribution. 
>- **Sampling Distribution** - The distribution of all possible samples of size 100 that you could have drawn. We don't know this distribution exactly, but it turns out that we know its general shape. 

## Example: Height in our class
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

>- Lets treat our class of `r sum(!is.na(students$height))` students as the population. I want to estimate the average height of the class. 
>- In this case, I am omnipotent - I know the population distribution because I collected data for the whole class on Canvas.
>- What do the sampling distributions look like for different sample sizes? 

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
minHeight <- min(students$height, na.rm=TRUE)
maxHeight <- max(students$height, na.rm=TRUE)
hist(students$height,breaks=seq(from=minHeight,to=maxHeight,by=1), 
     col="red", main="population distribution of class height", xlab="height", las=1, cex.main=1.5)
```

## Lets draw samples of size 2
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

>- Each sample has two values that give its **sample distribution**.
>- For each sample, I can calculate mean height.
>- If I keep drawing samples of size two until I have all possible samples, the means from each sample would constitute the **sampling distribution**. 
>- How many samples of size 2 can I draw from a population of size `r sum(!is.na(students$height))`?
>- (`r sum(!is.na(students$height))`*`r (sum(!is.na(students$height))-1)`)/2=`r sum(!is.na(students$height))*(sum(!is.na(students$height))-1)/2` possible samples. 

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
hist(means2,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="The sampling distribution of class height\nfor samples of size 2", xlab="mean height", las=1, cex.main=1.5)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
hist(means3,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="The sampling distribution of class height\nfor samples of size 3", xlab="mean height", las=1, cex.main=1.5)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
hist(means4,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="The sampling distribution of class height\nfor samples of size 4", xlab="mean height", las=1, cex.main=1.5)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
hist(means5,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="The sampling distribution of class height\nfor samples of size 5", xlab="mean height", las=1, cex.main=1.5)
```

## The shape becomes more bell-curved
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
par(mfrow=c(2,2))
hist(means2,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="Sampling distribution (n=2)", xlab="mean height", las=1)
hist(means3,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="Sampling distribution (n=3)", xlab="mean height", las=1)
hist(means4,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="Sampling distribution (n=4)", xlab="mean height", las=1)
hist(means5,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="Sampling distribution (n=5)", xlab="mean height", las=1)
```

## The means of the sampling distributions are the same but the standard deviation gets smaller
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE}
knitr::kable(cbind('Distribution'=c("Population Distribution","Sampling Distibution (n=2)",
                       "Sampling Distribution (n=3)", "Sampling Distribution (n=4)",
                       "Sampling Distribution (n=5)"),
      'Mean'=round(sapply(list(students$height, means2, means3, means4, means5), mean),2),
      'Standard Deviation'=round(sapply(list(students$height, means2, means3, means4, means5),
                                        sd), 2)),
      align=c("l","r","r"))
```

## Its the central limit theorem!
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

As the sample size increases, the sampling distribution of a sample mean becomes a **normal** distribution.

>- The normal distribution is a bell-shaped curve with two characteristics: center and spread. 
>- Centered on $\mu$, which is the true value in the population.
>- With a spread (standard deviation) of $\sigma/\sqrt{n}$, where $\sigma$ is the standard deviation in the population. 
>- The center of the sampling distribution is the true value of the parameter and the spread of the sampling distribution shrinks as the sample grows larger. 

## Three different standard deviations
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

There are three different kinds of standard deviations involved here, one that corresponds to each of the types of distributions. 

>- $\sigma$: population standard deviation
>- $s$: sample standard deviation, approximates $\sigma$
>- $\sigma/\sqrt{n}$: standard deviation of the sampling distribution for the mean. We often refer to the standard deviation of the sampling distribution as the **standard error**.

## The Normal Distribution
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-5,to=5,by=0.1)
y <- (1/(sqrt(2*pi)))*exp(-1*(x)^2/(2^2))
par(mar=c(4,0.1,0.1,0.1))
plot(x,y, type="l", lwd=3, yaxt="n", xlab="",
     ylab="", xaxt="n")
axis(1, at=c(0), cex=0.7,
     labels=c(expression(mu)))
abline(v=0, lwd=3, lty=2, col="red")
text(-4.9,0.9*max(y),"The center of the sampling distribution\nis the true population parameter", pos=4)
```


## The Normal Distribution
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-5,to=5,by=0.1)
y <- (1/(sqrt(2*pi)))*exp(-1*(x)^2/(2^2))
par(mar=c(4,0.1,0.1,0.1))
plot(x,y, type="l", lwd=3, yaxt="n", xlab="",
     ylab="", xaxt="n")
axis(1, at=c(-1,0,1), cex=0.7,
     labels=c(expression(mu-sigma/sqrt(n)),
              expression(mu),
              expression(mu+sigma/sqrt(n))))
polygon(c(x[41],x[41:61],x[61]),c(0,y[41:61],0), col="grey")
text(0, max(y)/2, label="68%")
text(-4.9,0.9*max(y),"68% of all possible samples are\nwithin one standard deviation\nof the center", pos=4)
```

## The Normal Distribution
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-5,to=5,by=0.1)
y <- (1/(sqrt(2*pi)))*exp(-1*(x)^2/(2^2))
par(mar=c(4,0.1,0.1,0.1))
plot(x,y, type="l", lwd=3, yaxt="n", xlab="",
     ylab="", xaxt="n")
axis(1, at=c(-2,0,2), cex=0.7,
     labels=c(expression(mu-1.96*sigma/sqrt(n)),
              expression(mu),
              expression(mu+1.96*sigma/sqrt(n))))
polygon(c(x[31],x[31:71],x[71]),c(0,y[31:71],0), col="grey")
text(0, max(y)/2, label="95%")
text(-4.9,0.9*max(y),"95% of all possible samples are\nwithin 1.96 standard deviations\nof the center", pos=4)
```

## The Normal Distribution
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-5,to=5,by=0.1)
y <- (1/(sqrt(2*pi)))*exp(-1*(x)^2/(2^2))
par(mar=c(4,0.1,0.1,0.1))
plot(x,y, type="l", lwd=3, yaxt="n", xlab="",
     ylab="", xaxt="n")
axis(1, at=c(-3,0,3), cex=0.7,
     labels=c(expression(mu-3*sigma/sqrt(n)),
              expression(mu),
              expression(mu+3*sigma/sqrt(n))))
polygon(c(x[21],x[21:81], x[81]),c(0,y[21:81],0), col="grey")
text(0, max(y)/2, label="99.7%")
text(-4.9,0.9*max(y),"99.7% of all possible samples are\nwithin three standard deviations\nof the center", pos=4)
```

## Sampling distributions are the key concept
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

>- When we draw a sample and calculate the sample mean we are effectively drawing a value from the sampling distribution for the sample mean. 
>- If we know what that distribution looks like then we can know the probability of drawing a sample close to or far from the true population parameter. 

## Reach into this distribution and pull out a sample
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
h <- hist(means4,breaks=seq(from=minHeight,to=maxHeight,by=1), 
          col="red", 
          main="The sampling distribution of class height\nfor samples of size 4", 
          xlab="mean height", las=1)
```

## Reach into the distribution and pull out a sample
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
h <- hist(means5,breaks=seq(from=minHeight,to=maxHeight,by=1), col="red", main="The sampling distribution of class height\nfor samples of size 5", xlab="mean height", las=1)
arrows(75, 0.85*max(h$counts), mean(means5), 0.5*max(h$counts))
text(75, 0.85*max(h$counts), "You will likely draw a value\nclose to the center", pos=4, cex=0.8)
arrows(77, 0.2*max(h$counts), mean(means5)+5.5*sd(means5)/sqrt(5), 0.01*max(h$counts))
text(77, 0.2*max(h$counts), "But you could draw a\nreally unusual value", pos=4, cex=0.8)
```


## The Catch
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

>- We don't know the true population mean or standard deviation so we know the shape is normal but not its actual center and spread. 
>- Therefore, we don't know if our particular sample was close to the center or out in the tail.
>- What can we do? 

## First Fix
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

>- We can solve the standard deviation problem by just using our sample standard deviation $s$ as an estimate of $\sigma$ to calculate $s\sqrt{n}$ as an approximation of the standard error.
>- This has consquences. Because we are using a sample value which can also be subject to random bias, this substitution creates greater uncertainty in our estimate which we will address later. 
>- We still don't know the population mean though, which is a more fundamental problem. 

## Two Approaches
<div class="footer">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

>- **Confidence Intervals**: Provide a range of values within which you feel confident that the true population mean resides. 
>- **Hypothesis tests**: Play a game of make believe. If the true population mean was a given value, what is the probability that I would get the sample mean value that I actually did? 

# Confidence Intervals
Statistical Inference

## Consider this logic
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r echo=FALSE, fig.width=7, fig.height=4.5, out.width='700px', out.height='450px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-5,to=5,by=0.1)
y <- (1/(sqrt(2*pi)))*exp(-1*(x)^2/(2^2))
par(mar=c(4,0.1,0.1,0.1))
plot(x,y, type="l", lwd=3, yaxt="n", xlab="",
     ylab="", xaxt="n")
axis(1, at=c(-2,0,2), cex=0.7,
     labels=c(expression(mu-2*sigma/sqrt(n)),
              expression(mu),
              expression(mu+2*sigma/sqrt(n))))
polygon(c(x[31],x[31:71],x[71]),c(0,y[31:71],0), col="grey")
text(0, max(y)/3, "95% probability that I will\ndraw a sample mean\nthat is within two (1.96)\nstandard deviations of\nthe true population mean")
```

## Reverse the logic
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

There is a 95% probability that I will draw a sample mean $\bar{x}$ within 1.96 standard errors ($\sigma/\sqrt{n}$) of the true population mean $\mu$. 

I can reverse the logic of this statement and say that if I construct the following interval:

$$\bar{x}\pm1.96*\sigma/\sqrt{n}$$

95% of *all possible samples* will contain the true population mean $\mu$ within this interval. 

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: The concept of the sampling distribution</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
par(mar=c(0,0,3,0))
plot(-2,-2, xlim=c(-1,20), ylim=c(-4,4.5), xaxt="n", yaxt="n", bty="n", xlab="", ylab="", main="On average, one in twenty samples will have confidence\nintervals that do not contain the true population mean", cex.main=1.5)
x <- rnorm(19,0,1)
while(sum(abs(x)>1.96)>0) {
  x[abs(x)>1.96] <- rnorm(sum(abs(x)>1.96),0,1) 
}
x <- c(x[1:10],-2.3,x[11:19])
upper <- x + 1.96
lower <- x - 1.96
abline(h=0, lwd=3, col="blue")
segments(1:20,lower,1:20,upper)
segments(11,lower[11],11,upper[11], col="red", lwd=2)
points(1:20, x, pch=19, cex=1.2)
text(0,0, expression(mu), col="blue", cex=2, pos=3)
legend(0,4.5,legend=c("sample mean", "confidence interval"), pch=c(19,NA), lty=c(NA,1), bty="n")
```

## What does confidence mean?
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

>- We refer to the interval $\bar{x}\pm1.96*\sigma/\sqrt{n}$ as the **confidence interval**.
>- It is tempting to claim that there is a 95% probability that the true population mean is in this interval, **BUT THIS IS INCORRECT**. 
>- The true population mean does not vary. It just is what it is, even if it is unknown. Either your interval contains it or the interval does not contain it. There is no probability.
>- The correct interpretation is that "95% of all possible confidence intervals will contain the true population mean." We don't know if we have one that does or doesn't but we know that 95% would.
>- If that is confusing to you, its perfectly normal. Its weird. 

## We still have a problem
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

The confidence interval is given by:

$$\bar{x}\pm1.96*\sigma/\sqrt{n}$$

However, we still can't calculate this confidence interval because we don't know $\sigma$. Remember that $\sigma$ is the population standard deviation. Since we don't have the population, we don't know this value.

What can we do? Lets just substitute in our sample standard deviation $s$ for $\sigma$ since this value is itself an estimate of $\sigma$:

$$\bar{x} \pm t*s/\sqrt{n}$$

## The t-statistic
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

Notice that the 1.96 is no longer in my formula for the confidence interval.

$$\bar{x} \pm t*s/\sqrt{n}$$

Instead, I now have a $t$. This is the **t-statistic**. We have to use the t-statistic here because of our use of the sample standard deviation. Basically $t$ will be some number slightly bigger than 1.96 that adjusts for the greater uncertainty in our estimate as a result of using the sample standard deviation. 

## How do you get the t-statistic? {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

The t-statistic you get depends on two characteristics:

- What level of confidence you want (we will always use 95%).
- The number of **degrees of freedom** which for sample means is given by $n-1$. 

In *R*, you can calculate the t-statistic with the `qt` command. Lets say we wanted the t-statistic for our crime data with 51 observations:

```{r}
qt(.975, 51-1)
```

The first number is a little hard to explain. We want the 95% confidence interval but we need to put in 0.975. This is because we are getting the upper tail of the distribution which has only 2.5% of the area outside. The second number is the degrees of freedom. 

## Example: Property crime rates {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
mean(crimes$Property)
sd(crimes$Property)
nrow(crimes)
```

$\bar{x}=2894$, $s=641.5$, $n=51$

## Example: Property crime rates, continued {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
se <- 641.5/sqrt(51)
se
```

We got the t-statistic on the previous slide, so put it all together.

```{r}
2894+2.0086*se
2894-2.0086*se
```

We are 95% confident that the true mean property crime rate across states is between 2731.6 and 3074.4 crimes per 100,000. 

## Does this even make sense?
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

It doesn't actually make a lot of sense to calculate a confidence interval for this dataset, because the crime data are not a sample. We have all 50 states, so there is no larger population to which we want to infer. The mean crime rate across states is the true population value already.

The movie, crime, and titanic data all share this feature that they are not samples that we can infer to larger populations. The sexual frequency and politics data, however, are both samples of the US population. 

## Example: Sexual frequency {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
barx <- mean(sex$sexf)
s <- sd(sex$sexf)
n <- nrow(sex)
se <- s/sqrt(n)
t <- qt(.975, n-1)
results <- c(barx, s, n, se, t)
names(results) <- c("sample mean","sd","n","SE","t")
results
```

Note that because the sample size is large here, the t-statistic is very close to 1.96. 


## Example: Sexual frequency, continued {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
barx+t*se
barx-t*se
```

We are 95% confident that the mean sexual frequency in the US is between 47.8 and 52.4 times per year. 

## General form of the confidence interval
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

We can construct confidence intervals for any statistic whose sampling distribution is a normal distribution. This includes means, mean differences, proportions, and regression slopes. The general form of the confidence interval is given by:

$$\texttt{(sample statistic)} \pm t*(\texttt{standard error})$$ 
The only trick is knowing how to calculate the standard error and degrees of freedom for the t-statistic in each case. 

## Cheat sheet for SE and df
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

Type              SE                                                                  df for $t$
------            ------                                                              ------
Mean              $s/\sqrt{n}$                                                        $n-1$
Proportion        $\sqrt\frac{\hat{p}*(1-\hat{p})}{n}$                                $n-1$
Mean Difference   $\sqrt{\frac{s_1^2}{n_1}+\frac{s_2^2}{n_2}}$                        min($n_1-1$,$n_2-1$)
Regression Slope  $\frac{\sqrt{\sum (y_i-\hat{y}_i)^2/(n-1)}}{\sum (x_i-\bar{x})^2}$  $n-2$

Notes: $\hat{p}$ is the sample proportion, $s_1$ and $s_2$ are the standard deviation of group 1 and 2, respectively, and $n_1$ and $n_2$ are the standard deviation of group 1 and 2. 

Don't worry, we won't generally have to calculate these things by hand. 

## Mean age difference by soccer fandom {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
tapply(students$age, students$soccer, mean)
diff <- 20.78571-22.39286
tapply(students$age, students$soccer, sd)
table(students$soccer)
```

## Mean difference by soccer fandom {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
se <- sqrt(7.67^2/28+1.58^2/14)
t <- qt(0.975,14-1)
diff+t*se
diff-t*se
```

I am 95% confident that, among sociology majors, avid soccer fans are on average between 1.65 years older to 4.87 years younger than those who are not avid soccer fans.

## Regression slope of age and sexual frequency {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
nrow(sex)
model <- lm(sexf~age, data=sex)
summary(model)$coef
```

The command above gives me both the slope (-1.2989) and its standard error (0.0654). 

## Regression slope of age and sexual frequency {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Confidence Intervals</body>
</div>

```{r}
t <- qt(0.975, 2103-2)
-1.2989+t*0.0654
-1.2989-t*0.0654
```

I am 95% confident that the true regression slope of sexual frequency predicted by age is between -1.17 and -1.43 fewer sexual encounters per year for every year of age. 

# Hypothesis Tests
Statistical Inference

## The logic of hypothesis testing
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

>- We know what the sampling distribution should look like, but we don't know its center (the true population parameter).
>- Play a game of make believe
>- Assume that the true parameter is some value
>- If my assumption is correct, what is the probability that I would have gotten the sample statistic that I got? 
>- If this probability is really low, then I reject my assumption. 

## Example: Coca-Cola Winners
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

>- Coca-Cola claims that 1 in 12 bottle caps will earn a free coke. ($1/12=0.083$), 8.3%
>- I buy 100 bottles of coke and only get 5 winners. ($5/100=0.05$), 5%
>- Is my winning percentage low enough to cast doubt on Coca-Cola's claims?

## Set up a null hypothesis
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

>- The **null hypothesis** ($H_0$) is your assumption about the true parameter value. It is your prior assumption unless the data can prove you wrong. 
>- In this case, I assume that the true population proportion is 0.083. 
>- Some people will tell you there is also an alternative hypothesis ($H_A$). These people are wrong. 

## Null hypothesis stated mathematically
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

$$H_0: \rho=0.083$$

I use the greek $\rho$ to indicate the population proportion of winners. I will use $\hat{p}$ to represent the proportion observed in my sample. 

## How do we test?
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

>- I have a sample proportion of 0.05 in a sample of size 100.
>- **Assuming the null hypothesis is true**, what would the sampling distribution for my sampling proportion look like? 
>- The sample size is big enough that it should be normally distributed.
>- The center of the sampling distribution should be the assumed true parameter under $H_0$ of 0.083.
>- The standard error of 0.028 is calculated based on the formula presented in the prior section. 

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-0.02,to=0.2,by=0.001)
p <- 0.083
se <- sqrt(p*(1-p)/100)
y <- (1/(se*sqrt(2*pi)))*exp(-1*(x-p)^2/(2*se^2))

plot(x,y,type="l",xlab="sample proportion",yaxt="n",ylab="", main="Game of make believe", cex.main=2)
text(0.14,12, "Sampling distribution for sample proportion\nassuming null hypothesis is true")
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-0.02,to=0.2,by=0.001)
p <- 0.083
se <- sqrt(p*(1-p)/100)
y <- (1/(se*sqrt(2*pi)))*exp(-1*(x-p)^2/(2*se^2))

plot(x,y,type="l",xlab="sample proportion",yaxt="n",ylab="", main="Game of make believe", cex.main=2)
text(0.14,12, "Sampling distribution for sample proportion\nassuming null hypothesis is true")
abline(v=p,lwd=4,col="blue")
text(p,1,"true proportion = 0.083", col="blue", pos=4)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-0.02,to=0.2,by=0.001)
p <- 0.083
se <- sqrt(p*(1-p)/100)
y <- (1/(se*sqrt(2*pi)))*exp(-1*(x-p)^2/(2*se^2))

plot(x,y,type="l",xlab="sample proportion",yaxt="n",ylab="", main="Game of make believe", cex.main=2)
text(0.14,12, "Sampling distribution for sample proportion\nassuming null hypothesis is true")
abline(v=p,lwd=4,col="blue")
text(p,1,"true proportion = 0.083", col="blue", pos=4)
abline(v=0.05,lwd=4,col="red")
text(0.05,1,"our sample      \nproportion = 0.05", col="red", pos=2)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-0.02,to=0.2,by=0.001)
p <- 0.083
se <- sqrt(p*(1-p)/100)
y <- (1/(se*sqrt(2*pi)))*exp(-1*(x-p)^2/(2*se^2))

plot(x,y,type="l",xlab="sample proportion",yaxt="n",ylab="", main="Game of make believe", cex.main=2)
text(0.14,12, "Sampling distribution for sample proportion\nassuming null hypothesis is true")
abline(v=p,lwd=4,col="blue")
text(p,1,"true proportion = 0.083", col="blue", pos=4)
abline(v=0.05,lwd=4,col="red")
text(0.05,1,"our sample      \nproportion = 0.05", col="red", pos=2)
arrows(p,1,0.05,1)
text(0.05,3,"How far is our sample proportion from\nwhere the center would be if the\nnull hypothesis was true?", pos=4)
```

## The central question
<div class="footer"> 
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

How far is our sample proportion (0.05) from where the center would be if the null hypothesis was true (0.083)?

>- If our sample proportion is far way and unlikely to be drawn, then we **reject the null hypothesis**.
>- If our sample proportion is not far away and reasonably likely to be drawn, then we **fail to reject the null hypothesis**. We never accept the null hypothesis, since it is already our working assumption. 

## How far is far enough? 
<div class="footer"> 
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

We determine how far our sample proportion is from the center in terms of the number of standard errors.

$$\frac{\hat{p}-\rho}{SE}=\frac{0.05-0.083}{0.028}=-1.18$$

The sample proportion of 0.05 is 1.18 standard errors below the center of the sampling distribution, assuming the null hypothesis is true. Is this far or not far? 

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x2 <- (x-p)/0.028
#x2 <- c(x2,-1.18)
#x2 <- sort(x2)

plot(x2,y,type="l",xlab="SEs from center",yaxt="n",ylab="", main="Game of make believe")
abline(h=0)
abline(v=-1.18,lwd=4,col="red")
text(-1.18,1,"1.18 standard errors below the mean", col="red", pos=4)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x2 <- (x-p)/0.028

plot(x2,y,type="l",xlab="SEs from center",yaxt="n",ylab="", main="Game of make believe")
abline(h=0)
abline(v=-1.18,lwd=4,col="red")
text(-1.18,1,"1.18 standard errors below the mean", col="red", pos=4)
polygon(c(x2[1:71],-1.18),c(y[1:71],0), col="grey")
arrows(-3,3,-1.5,2)
text(-2.5,3,"What proportion of all\nsample means are equal to\nor lower than this value?", pos=3)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
plot(x2,y,type="l",xlab="SEs from center",yaxt="n",ylab="", main="Game of make believe")
abline(h=0)
abline(v=-1.18,lwd=4,col="red")
text(-1.18,1,"1.18 standard errors below the mean", col="red", pos=4)
polygon(c(x2[1:71],-1.18),c(y[1:71],0), col="grey")
arrows(-3,3,-1.5,2)
text(-2.5,3.5,"0.12 (12%)",pos=3)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
plot(x2,y,type="l",xlab="SEs from center",yaxt="n",ylab="", main="Game of make believe")
abline(h=0)
polygon(c(x2[1:71],-1.18),c(y[1:71],0), col="grey")
polygon(c(x2[221:137],1.18),c(y[221:137],0), col="grey")
abline(v=-1.18,lwd=4,col="red")
abline(v=1.18,lwd=4,col="red")
arrows(-3,3,-1.5,2)
text(-2.5,3.5,"0.12 (12%)",pos=3)
arrows(3,3,1.5,2)
text(2.5,3.5,"But what about the probability of\nbeing this far off in the other direction?",pos=3)
```

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
plot(x2,y,type="l",xlab="sample proportion",yaxt="n",ylab="", main="Game of make believe")
abline(h=0)
polygon(c(x2[1:71],-1.18),c(y[1:71],0), col="grey")
polygon(c(x2[221:137],1.18),c(y[221:137],0), col="grey")
abline(v=-1.18,lwd=4,col="red")
abline(v=1.18,lwd=4,col="red")
arrows(-3,3,-1.5,2)
text(-2.5,3.5,"0.12 (12%)",pos=3)
arrows(3,3,1.5,2)
text(2.5,3.5,"0.12 (12%)",pos=3)
arrows(-2.5,4.5,-0.5,10)
arrows(2.5,4.5,0.5,10)
text(0,11,"p-value=0.12+0.12=0.24 (24%)",pos=3)
```

## Endgame: the p-value
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

The p-value is the ultimate product of a hypothesis test.

>- The p-value tells you the probability of getting a statistic this far away or farther from the the assumed true population parameter, assuming the null hypothesis is true. 
>- I got a p-value of 0.24. Assuming the null hypothesis is true, there is a 24% probability that I would have gotten a sample proportion (0.05) this far or farther from the true poplation paramter (0.083).
>- The lower the p-value, the less likely that we could have gotten the data we got under the assumption of the null hypothesis. Thus, we will reject the null hypothesis if our p-value is low enough.
>- Is 0.24 low enough to reject the null hypothesis? 

## The critical value
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

The critical value is the level of the p-value where we will reject the null hypothesis.

>- If our p-value is less than or equal to the critical value, we **reject the null hypothesis**.
>- If our p-value is greater than the critical value, we **fail to reject the null hypothesis**. 
>- The standard critical value is 0.05 (5%). This number is totally arbitrary, but nonetheless accepted. 
>- 0.24>0.05, so I fail to reject the null hypothesis that 1 in 12 bottlecaps are indeed winners. I cannot confidently claim that Coca-Cola is lying. However, I have also not accepted their claim, just failed to reject it. 

## How do you calculate the p-value? 
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

The p-value is given by the area in the tails of the normal distribution. However, as for confidence intervals, we have to adjust this area for the use of the sample standard deviation in most of our calculations. The command `pt` is very similar to `qt` except you feed in the number of standard errors:

```{r}
2*pt(-1.18,99)
```

I multiply by two to get the area of both tails. 

Note that `pt` always gives the lower tail, so you should always put in negative standard errors, regardless of whether your statistic is above or below the center. 

## The general procedure of hypothesis testing
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

>- State a **null hypothesis**.
>- Calculate a **test statistic** that tells you how different your sample is from what you would expect under the null hypothesis. For our purposes, this test statistic is always the number of standard errors above or below the center of the sampling distribution. 
>- Calculate the **p-value** for the given test statistic.
>- Based on the p-value, either **reject** or **fail to reject** the null hypothesis.


## Hypothesis tests of relationships
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

The hypothesis test we are most interested in is whether the association we observe between two variables in our sample holds in the population. 

>- Mean differences: Are the means between two groups in the population different? In other words, is the mean difference non-zero?
>- Regression slopes: is the regression slope in the population non-zero? 

## Example: mean differences {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r}
tapply(politics$age, politics$party, mean)
```

In my ANES sample, Republicans are older (51.5) than Democrats (49.3). Do I think this reflects a real age difference in the population? My null hypothesis is:

$$H_0: \mu_R-\mu_D=0$$

Where $\mu_R$ is the population mean age of Republicans and $\mu_D$ is the population mean age of Democrats. 

The sample mean difference is `r 51.5-49.3` years of age. Is this difference far enough from zero to reject the null hypothesis? 

## Example: mean differences, continued {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

First, I need the standard error of the sample mean difference, as per the formula in the previous section, which requires the sample SD's and sample size of both groups.

```{r}
tapply(politics$age, politics$party, sd)
table(politics$party)
se <- sqrt(16.50332^2/2361+16.88751^2/1389)
se
```

## Example: mean differences, continued {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>


I can now calculate how many standard errors `r 51.5-49.3` is from zero and use that to calculate my p-value. Remember that I use the smaller of the two sample sizes as my degrees of freedom for the t. 

```{r}
(51.5-49.3)/se
2*pt(-3.88,1389-1)
```

The p-value is tiny. Assuming that the true population age difference between Republicans and Democrats is zero, there is a .02% chance of observing an age difference of `r 51.5-49.3` years or larger in a sample of this size. Thus, I **reject** the null hypothesis and conclude that Democrats are younger than Republicans, on average. 

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-2.5,to=2.5,by=0.01)
se <- 0.566
y <- (1/(se*sqrt(2*pi)))*exp(-1*(x)^2/(2*se^2))

plot(x,y,type="l",xlab="sample mean difference",yaxt="n",ylab="", main="Sampling distribution of mean difference,\nassuming null hypothesis is true", cex.main=2)
#abline(v=0,lwd=3,lty=2,col="blue")
abline(v=2.1,lwd=3,col="red")
abline(v=-2.1,lwd=1,col="red", lty=2)
polygon(c(x[1],x[1:41],-2.1),c(-0.005, y[1:41],-0.005), col="grey")
polygon(c(x[500],x[500:461],2.1),c(-0.005, y[500:461],-0.005), col="grey")
text(0,max(y)/2,"p-value=0.00023")
arrows(x[20],y[20]+0.005,-0.01,max(y)/2-0.02, length=0.1)
arrows(x[480],y[480]+0.005,0.01,max(y)/2-0.02, length=0.1)

```

## Example: regression slopes {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

Let me treat the ordinal responses to whether respondents were in favor of treating immigrants as citizens as a score from 0 to 2. Lets look at the relationship between this score and income. 

```{r}
politics$immigscore <- as.numeric(politics$immigcitizen)-1
model <- lm(immigscore~income, data=politics)
summary(model)$coef
```

The model predicts that a thousand dollar increase in income is associated with a 0.0006 increase in the score of support for immigrant citizenship. Note that this is a small substantive effect. Do we think it is a real effect in the population? 

Our null hypothesis is that the true slope in the population is zero:

$$H_0: \beta_1=0$$

## Example: regression slopes, continued {.smaller}
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

Note that the `summary(model)$coef` command actually gave us all the information we need, including the p-value.

```{r}
summary(model)$coef
```

The standard error is 0.00018. If we take our regression slope and divide by this standard error, we get a t-statistic of 3.11. A slope this far or farther from zero has a 0.18% probability of being drawn, assuming the null hypothesis is true. 

This is a very small p-value, so we **reject** the null hypothesis. We conclude that there is a very small positive relationship between income and support for immigrant citizenship.

----
<div class="footer" style="top:575px;">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

```{r echo=FALSE, fig.width=8, fig.height=5.5, out.width='800px', out.height='550px', dpi=300, fig.align='center', dev.args = list(bg = 'transparent')}
x <- seq(from=-0.00075,to=0.00075,by=0.00001)
se <- 0.00018
y <- (1/(se*sqrt(2*pi)))*exp(-1*(x)^2/(2*se^2))

plot(x,y,type="l",xlab="sample regression slope",yaxt="n",ylab="", main="Sampling distribution of regression slope,\nassuming null hypothesis is true", cex.main=2)
#abline(v=0,lwd=3,lty=2,col="blue")
abline(v=0.00058,lwd=3,col="red")
abline(v=-0.00058,lwd=1,col="red", lty=2)
polygon(c(x[1],x[1:18],-0.00058),c(-0.005, y[1:18],-0.005), col="grey")
polygon(c(x[151],x[151:134],0.00058),c(-0.005, y[151:134],-0.005), col="grey")
text(0,max(y)/2,"p-value=0.0024")
arrows(x[9],y[9]+0.005,-0.0001,0.95*max(y)/2, length=0.1)
arrows(x[140],y[140]+0.005,0.0001,0.95*max(y)/2, length=0.1)
```

## Statistical Significance
<div class="footer"> 
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

If you reject the null hypothesis of "no relationship," then the relationship you observe in the sample is said to be "statistically significant."

>- Don't confuse statistical and substantive significance. In a large sample, even very small substantive associations can be found to be statistically significant. On the flip side, in small samples, very large substantive associations can fail to be statistically significant. 
>- A better term might be "statistically distinguishable" as in "the association observed in the sample is statistically distinguishable from zero."

##  Statistical Inference in Regression Tables {.smaller}
<div class="footer"> 
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

<div class="stargazer">
```{r echo=FALSE, results="asis", warning=FALSE, message=FALSE}
library(stargazer, quietly=TRUE, verbose=FALSE)
m1 <- lm(Violent~PctMale, data=crimes)
m2 <- lm(Violent~PctMale+Gini, data=crimes)
m3 <- lm(Violent~PctMale+Gini+MedianAge, data=crimes)
stargazer(m1,m2,m3, type="html",
          omit.stat=c("adj.rsq","ser","f"), 
          digits=2, 
          star.cutoffs=c(0.05,0.01,0.001),
          dep.var.labels="Violent crime rate",
          covariate.labels = c("Percent male", 
                               "Gini coefficient",
                               "Median Age"),
          notes=c("<em>&#42;p&lt;0.05;&#42;&#42;p&lt;0.01;&#42;&#42;&#42;p&lt;0.001</em>",
                  "Standard errors in parenthesis"),
          notes.append=FALSE,
          title="OLS regression models predicting violent crime rates for US states",
          dep.var.labels.include=FALSE,
          dep.var.caption="")
```
</div>

>- The number in parenthesis is the standard error. If you divide this number by the estimate, you will get a t-statistic and you could use this to calculate a p-value.
>- The asterisks provide an easy visual cue as to whether the p-value for a given estimate is below a certain benchmark. Since one asterisks indicates a p-value less than 0.05, any result with an asterisk is "statistically significant."


## What is a p-value? {.centered}
<div class="footer"> 
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

![](images/slides/pvalues.jpg)

## What is a p-value?
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

P-values are widely misunderstood, even by well-informed researchers. Suppose that you conduct a study of a difference between two groups and find a p-value on your hypothesis test of 0.01. Which of the following statements is true? 

- You have absolutely disproved the null hypothesis.
- You have found the probability of the null hypothesis being true. 
- You know, if you decide to reject the null hypothesis, the probability that you are making the wrong decision. 
- You have a reliable finding in the sense that if, hypothetically, the experiment were repeated a great number of times, you would obtain a significant result 99% of the time. 

>- ALL OF THESE STATEMENTS ARE FALSE. 


## Misunderstanding the p-value
<div class="footer">
<body>Sociology 312, Statistical Inference: Hypothesis Tests</body>
</div>

The p-value is the **probability of the observed data (or more extreme data), assuming the null hypothesis is correct**.

>- The p-value is NOT a statement about the probability of a hypothesis, given the data. It is a statement about the probability of the data, **given a hypothesis**.
>- The reason your brain wants to turn it around to the probability of a hypothesis given the data is because that is intuitively what you want. 
>- According to the classical view of probability as objective, the hypothesis is either true or it is not true and we cannot assign a probability to its truth. 
>- Don't report your conclusions about the p-value in terms of statements about the probability of a hypothesis. 