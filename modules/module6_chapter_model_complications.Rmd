
# Model Complications

In this chapter, we will expand our understanding of the linear model to address many issues that the practical researcher must face. We begin with a review and reformulation of the linear model. We then move on to discuss how to address violations of assumptions such as non-linearity and heteroskedasticity (yes, this is a real word), sample design and weithing, missing values, multicollinearity, and model selection. By the end of this chapter, you will be well-supplied with the tools for conducting a real-world analysis using the linear model framework. 

---

## The Linear Model, Revisited

### Reformulating the linear model

Up until now, we have used the following equation to describe the linear model mathematically:

$$\hat{y}_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}$$
In this formulation, $\hat{y}_i$ is the predicted value of the dependent variable for the $i$th observation and $x_{i1}$, $x_{i2}$. through $x_{ip}$ are the values of $p$ independent variables that predict $\hat{y}_i$ by a linear function. $\beta_0$ is the y-intercept which is the predicted value of dependent variable when all the independent variables are zero. $\beta_1$ through $\beta_p$ are the slopes giving the predicted change in the dependent variable for a one unit increase in a given independent variable holding all of the other variables constant. 

This formulation is useful, but we can now expand and re-formulate it in a way that will help us understand some of the more advanced topics we will discuss in this and later chapters. The formula is above is only the "structural" part of the full linear model. The full linear model is given by:

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}+\epsilon_i$$

I have changed two things in this new formula. On the left-hand side, we have the actual value of the dependent variable for the $i$th observation. In order to make things balance on the right-hand side of the equation, I have added $\epsilon_i$ which is simply the residual or error term for the $i$th observation. We now have a full model that predicts the actual values of $y$ from the actual values of $x$. 

If we compare this to the first formula, it should become clear that every term except the residual term can be substituted for by $\hat{y}_i$. So, we can restate our linear model using two separate formulas as follows:

$$\hat{y}_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}$$

$$y_i=\hat{y}_i+\epsilon_i$$

By dividing up the formula into two separate components, we can begin to think of our linear model as containing a structural component and a random (or stochastic) component. The structural component is the linear equation predicting $\hat{y}_i$. This is the formal relationship we are trying to fit between the dependent and independent variables. The stochastic component on the other hand is given by the $\epsilon_i$ term in the second formula. In order to get back to the actual values of the dependent variable, we have to add in the residual component that is not accounted for by the linear model. 

From this perspective, we can rethink our entire linear model as a partition of the total variation in the dependent variable into the structural component that can be accounted for by our linear model and the residual component that is unnaccounted for by the model. This is exactly as we envisioned things when we learned to calculate $R^2$ in previous chapters. 

### Estimating a linear model

Up until now, we have not discussed how R actually calculates all of the slopes and intercept for a linear model with multiple independent variables. We only know the equations for a linear model with one independent variable. 

Even though we don't know the math yet behind how linear model parameters are estimated, we do know the rationale for why they are selected. We choose the parameters that minimized the sum of squared residuals given by:

$$\sum_{i=1}^n (y_i-\hat{y}_i)^2$$
In this section, we will learn the formal math that underlies how this esimtation occurs, but first a warning: there is a lot of math ahead. In normal everyday practice, you don't have to do any of this math, because R will do it for you. However, it is useful to know how linear model estimating works "under the hood" and it will help with some of the techniques we will learn later in the book. 

#### Matrix algebra crash course

In order to learn how to estimate linear model parameters, we will need to learn a little bit of matrix algebra. In matrix algebra we can collect numbers into **vectors** which are single dimension arrays of numbrers and **matrices** which are two-dimensional arrays of numbers. We can use matrix algebra to represent our linear regression model equation using one-dimensional **vectors** and two-dimensional **matrices**. We can imagine $y$ below as a vector of dimension 3x1 and $\mathbf{X}$ as a matrix of dimension 3x3. 

$$
\mathbf{y}=\begin{pmatrix}
4\\
5\\
3\\
\end{pmatrix}
\mathbf{X}=
\begin{pmatrix}
1 & 7 & 4\\
1 & 3 & 2\\
1 & 1 & 6\\
\end{pmatrix}
$$

We can multiply vectors and matrices together by taking each element in the row of a matrix by the corresponding element in the vector and summing them up:

$$\begin{pmatrix}
1 & 7 & 4\\
1 & 3 & 2\\
1 & 1 & 6\\
\end{pmatrix}
\begin{pmatrix}
4\\
5\\
3\\
\end{pmatrix}=
\begin{pmatrix}
1*4+7*5+3*4\\
1*4+3*5+2*3\\
1*4+1*5+6*3\\
\end{pmatrix}=
\begin{pmatrix}
51\\
25\\
27\\
\end{pmatrix}$$

You can also transpose a vector or matrix by flipping its rows and columns. My transposed version of $\mathbf{X}$ is $\mathbf{X}'$ which is:

$$\mathbf{X}=
\begin{pmatrix}
1 & 1 & 1\\
7 & 3 & 1\\
4 & 2 & 6\\
\end{pmatrix}$$

You can also multiple matrices by each other using the same pattern as for multiplying vectors and matrices but now you start a new column each time you move down a row of the first matrix. So to "square" my \mathbf{X} matrix:

$$
\begin{eqnarray*}
\mathbf{X}'\mathbf{X}&=&
\begin{pmatrix}
1 & 1 & 1\\
7 & 3 & 1\\
4 & 2 & 6\\
\end{pmatrix}
\begin{pmatrix}
1 & 7 & 4\\
1 & 3 & 2\\
1 & 1 & 6\\
\end{pmatrix}&\\
& =& \begin{pmatrix}
1*1+1*1+1*1 & 1*7+1*3+1*1 & 1*4+1*2+1*6\\
7*1+3*1+1*1 & 7*7+3*3+1*1 & 7*4+3*2+1*6\\
4*1+2*1+6*1 & 4*7+2*3+6*1 & 4*4+2*2+6*6\\
\end{pmatrix}\\
& =&  \begin{pmatrix}
3 & 11 & 12\\
11 & 59 & 40\\
12 & 40 & 56\\
\end{pmatrix}
\end{eqnarray*}
$$

R can help us with these calculations. the `t` command will transpose a matrix or vector and the `%*%` operator will to matrix algebra multiplation.

```{r matrix-algebra}
y <- c(4,5,3)
X <- rbind(c(1,7,4),c(1,3,2),c(1,1,6))
X
t(X)
X%*%y
t(X)%*%X
#a shortcut for squaring a matrix
crossprod(X)
```

We can also calculate the **inverse** of a matrix. The inverse of a matrix ($\mathbf{X}^{-1}$) is the matrix that when multiplied by the original matrix produces the **identity** matrix which is just a matrix of ones along the diagonal cells and zeroes elsewhere. Anything multiplied by the identity matrix is just itself, so the identity matrix is like 1 at the matrix algebra level. Calculating an inverse is a difficult calculation that I won't go through here, but R can do it for us easily with the `solve` command:

```{r inverse-matrix}
inverse.X <- solve(X)
inverse.X
round(inverse.X %*% X,0)
```

#### Linear model in matrix form

We now have enough matrix algebra under our belt that we can re-specify the linear model equation in matrix algebra format: 

$$\mathbf{y}=\mathbf{X\beta+\epsilon}$$

$\begin{gather*}
\mathbf{y}=\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{pmatrix}
\end{gather*}$,
$\begin{gather*}
\mathbf{X}=
\begin{pmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1p}\\
1 & x_{21} & x_{22} & \ldots & x_{2p}\\
\vdots & \vdots & \ldots & \vdots\\
1 & x_{n1} & x_{n2} & \ldots & x_{np}\\
\end{pmatrix}
\end{gather*}$,
$\begin{gather*}
\mathbf{\epsilon}=\begin{pmatrix}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{n}\\
\end{pmatrix}
\end{gather*}$,
$\begin{gather*}
\mathbf{\beta}=\begin{pmatrix}
\beta_{1}\\
\beta_{2}\\
\vdots\\
\beta_{p}\\
\end{pmatrix}
\end{gather*}$

Where:

- $\mathbf{y}$ is a vector of known values of the independent variable of length $n$.
- $\mathbf{X}$ is a matrix of known values of the independent variables of dimensions $n$ by $p+1$. This matrix is sometimes referred to as the **design matrix**. 
- $\mathbf{\beta}$ is a vector of to-be-estimated values of intercepts and slopes of length $p+1$.
- $\mathbf{\epsilon}$ is a vector of residuals of length $n$ that will be equal to $\mathbf{y-X\beta}$.

Note that we only know $\mathbf{y}$ and $\mathbf{X}$. We need to estimate a $\mathbf{\beta}$ vector from these known quantities. Once we know the $\mathbf{\beta}$ vector, we can calculate $\mathbf{\epsilon}$ by just taking $\mathbf{y-X\beta}$. 

We want to esimate $\mathbf{\beta}$ in order to minimize the sum of squared residuals. We can represent this sum of squared residuals in matrix algebra format as a function of $\mathbf{\beta}$: 

$$\begin{eqnarray*}
SSR(\beta)&=&(\mathbf{y}-\mathbf{X\beta})'(\mathbf{y}-\mathbf{X\beta})\\
&=&\mathbf{y}'\mathbf{y}-2\mathbf{y}'\mathbf{X\beta}+\mathbf{\beta}'\mathbf{X'X\beta}
\end{eqnarray*}$$

If you remember the old FOIL technique from high school algebra (first, outside, inside, last) that is exactly what we are doing here, in matrix algebra form. 

We now have a function that defines our sum of squared residuals. We want to choose the values of $\mathbf{\beta}$ that minimize the value of this function. In order to do that, I need to introduce a teensy bit of calculus. To find the minimum (or maximum) value of a function, you calculate the derivative of the function with respect to the variable you care about and then solve for zero. Technically, you also need to calculate second derivatives in order to determine if its a minimum or maximum, but since this function has no maximum value, we know that the result has to be a minimum. I don't expect you to learn calculus for this course, so I will just mathemagically tell you that the derivative of $SSR(\beta)$ with respect to $\mathbf{\beta}$ is given by: 

$$-2\mathbf{X'y}+2\mathbf{X'X\beta}$$

We can now set this to zero and solve for $\mathbf{\beta}$:

$$\begin{eqnarray*}
0&=&-2\mathbf{X'y}+2\mathbf{X'X\beta}\\
-2\mathbf{X'X\beta}&=&-2\mathbf{X'y}\\
(\mathbf{X'X})^{-1}\mathbf{X'X\beta}&=&(\mathbf{X'X})^{-1}\mathbf{X'y}\\
\mathbf{\beta}&=&(\mathbf{X'X})^{-1}\mathbf{X'y}\\
\end{eqnarray*}$$

Ta-dah! We have arrived at the matrix algebra solution for the best fitting parameters for a linear model with any number of independent variables. Lets try this formula out in R:

```{r matrix-lm}
X <- as.matrix(cbind(rep(1, nrow(movies)), movies[,c("Runtime","BoxOffice")]))
head(X)
y <- movies$TomatoMeter 
beta <- solve(crossprod(X))%*%crossprod(X,y) 
beta
model <- lm(TomatoMeter~Runtime+BoxOffice, data=movies)
coef(model)
```

It works!

We can also estimate standard errors using this matrix algebra format. To get the standard errors, we first need to calculate the **covariance matrix**. 

$$\sigma^{2}(\mathbf{X'X})^{-1}$$

The $\sigma^2$ here is the variance of our residuals. Technically, this is the variance of the residuals in the population but since we usually only have a sample we actually calculate:

$$s^2=\frac{\sum(y_i-\hat{y}_i)^2}{n-p-1}$$

based on the fitted values of $\hat{y}_i$ from our linear model. $p$ is the number of independent variables in our model. 

The covariance matrix actually provides us with some interesting information about the correlation of our independent variables. For our purposes we just want the square root of the diagonal elements, which gives us the standard error for our model. Continuing with the example estimating tomato meter ratings by run time and box office returns in our movies dataset, lets calculate all the numbers we need for an inference test:

```{r matrix-se}
#get the predicted values of y
y.hat <- X%*%beta 
df <- length(y)-ncol(X) 
#calculate our sample variance of the residual terms
s.sq <- sum((y-y.hat)^2)/df 
#calculate the covariance matrix
covar.matrix <- s.sq*solve(crossprod(X)) 
#extract SEs from the square root of the diagonal
se <- sqrt(diag(covar.matrix)) 
## calculate t-stats from our betas and SEs
t.stat <- beta/se 
#calculate p-values from the t-stat
p.value <- 2*pt(-1*abs(t.stat), df) 
data.frame(beta,se,t.stat,p.value)
#compare to the lm command
summary(model)$coef
```

And it all works. As I said at the beginning, this is a lot of math and not something you need to think about every day. I am not expecting all of this will stick the first time around, but I want it to be here for you as a handy reference for the future.

### Linear model assumptions

Two important assumptions underlie the linear model. The first assumption is the assumption of **linearity**. In short, we assume that the the relationship between the dependent and independent variables is best described by a linear function. In prior chapters, we have seen examples where this assumption of linearity was clearly violated. When we choose the wrong model for the given relationship, we have made a *specification error*. Fitting a linear model to a non-linear relationship is one of the most common forms of specification error. However, this assumption is not as deadly as it sounds. Provided that we correctly diagnose the problem, there are a variety of tools that we can use to fit a non-linear relationship within the linear model framework. We will cover this techniques in the next section. 

The second assumption of linear models is that the residual or error terms $\epsilon_i$ are **independent** of one another and **identically distributed**. This is often called the **i.i.d. assumption**, for short. This assumption is a little more subtle. In order to understand, we need to return to this equation from earlier:

$$y_i=\hat{y}_i+\epsilon_i$$

One way to think about what is going on here is that in order to get an actual value of $y_i$ you feed in all of the independent variables into your linear model equation to get $\hat{y}_i$ and then you reach into some distribution of numbers (or a bag of numbers if you need something more concrete to visualize) to pull out a random value of $\epsilon_i$ which you add to the end of your predicted value to get the actual value. When we think about the equation this way, we are thinking about it as a **data generating process** in which we get $y_i$ values from completing the equation on the right. 

The i.i.d. assumption comes into play when we reach into that distribution (or bag) to draw out our random values. **Independence** assumes that what we drew previously won't affect what we draw in the future. The most common violation of this assumption is in time series data in which peaks and valleys in the time series tend to come clustered in time, so that when you have a high (or low) error in one year, you are likely to have a similar error in the next year. **Identically distributed** means that you are drawn from the same distribution (or bag) each time you make a draw. One of the most common violation of this assumption is when error terms tend to get larger in absolute size as the predicted value grows in size. 

In later sections of this chapter, we will cover the i.i.d. assumption in more detail including the consequences of violating it, diagnostics for detecting it, and some corrections for when it does occur. 

---

## Modeling Non-Linearity

You **transform** your data when you apply a mathematical function to a variable to transform its values into different values. There are a variety of different transformations that are commonly used in statistics, but for this class we will focus on the one transformation that is most common in the social sciences: the log transformation.

Why would you want to transform your data? There are two important potential benefits that transformations can provide. First, a transformation can often resolve the problem of non-linear relationships. If the relationship between $x$ and $y$ is non-linear, then by transforming one or both variables, you may be able to recover a linear relationship. Second, transformation can reduce skewness in a variable and will pull in extreme outliers so that they are less influential. For these reasons, transformations can be useful in regression models. However, it is important to also remember that a transformation changes the way in which x and y relate to one another and thus requires us to adjust our interpretation of results. 

### The natural log transformation

When I talk about the "log" transformation, I am talking about what you probably learned as the "natural log" transformation. This transformation is given to you by the "ln" button on your calculator. For our purposes, this is the only "log" we care about. Just remember that in Eugene, we go natural. 

Any positive number can be logged. For example, I can calculate the log of the number 7:

```{r}
log(7)
```
OK, the natural log of 7 is `r log(7)`. But what does that mean? The natural log of a number is defined as the value you would have to raise the constant $e$ (2.718282) to in order to get back the original number. To raise $e$ by some number, you can use the `exp` function in R ("exp" for "exponential"):

```{r}
exp(log(7))
```

Ta-da! If I raise $e$ to the log of 7, I get back 7. The number $e$ is a very special number like $\pi$ having to do with what happens when you compound interest continually over time, but none of that matters for our purposes. For our purposes, what matters is that by logging a number you can make a multiplicative relationship into an additive relationship. This is because of a basic mathematical relationship where:

$$e^a*a^b=e^{a+b}$$
$$log(x+y) = log(x)+log(y)$$
You can try this out in R to see that it works:

```{r}
exp(2)*exp(3)
exp(2+3)
log(5*4)
log(5)+log(4)
```
This is really all you need to know about the log transformation to understand this section.

### Log-transformations allow us to estimate multiplicative models

Take a look at the histogram of the income variable from our politics dataset. 

```{r echo=FALSE}
hist(politics$income, breaks=seq(from=0,to=300,by=10), col="red", las=1, 
     xlab="income in 1000s", main="Histogram of income")
```

This variable is heavily right skewed, with a few very high earners at the top end of the distribution, and the vast majority of individuals making less than \$100,000 per year. The heavy skew makes this distribution is an ideal candidate for the log transformation. Lets go ahead and log-transform it and save our log-transformation as another variable called "lincome."

```{r}
politics$lincome = log(politics$income)
```

Now lets look at the distribution of this log-transformed income data:

```{r echo=FALSE}
hist(politics$lincome, col="red", las=1, breaks=20, 
     xlab="income in 1000s (log-scale)", main="Histogram of income (logged)")
```

Now we are getting a slight outlier in the opposite direction, but in general the tail ends of our distribution are pulled in considerably. This suggests that we have to worry less about how outliers might affect our results. 

Now lets try putting this log-transformed income variable in as the dependent variable and lets predict it by the age of the respondent. 

```{r}
summary(lm(lincome~age, data=politics))$coef
```
OK, so what does that mean? It might be tempting to interpret the results here as you normally would. We can see age has a positive effect. So, a one year increase in age is associated with a 0.005 increase in ... what? Remember that our dependent variable here is log-income. We could literally say that it is a 0.005 increase in log-income, but that is not a very helpful or intuitive way to think about the result. Similarly the intercept gives us the predicted log-income when age is zero. Thats not helpful for two reasons: its outside the scope of the data, and we don't really know how to think about a log-income of 3.29. 

In order to translate this into something meaningful, lets try looking at this in our equation format. Here is what we have:

$$\log(\hat{y}_i)=3.29+0.005*x_i$$

What we really want is to be able to understand this equation back on the original scale of the dependent variable, which in this case is income. Remember that taking $e^{\log(y)}$ just gives us back $y$. We can use that logic here. If we "exponentiate" (take $e$ to the power of the values) the left-hand side of the equation, then we can get back to $\hat{y}_i$. However, remember from algebra, that what we do to one side of the equation, we have to do to both sides. That means:

$$e^{\log(\hat{y}_i)}=e^{3.29+0.005*x_i}$$
$$\hat{y}_i=(e^{3.29})*(e^{0.005})^{x_i}$$
The good news is that we now just have our predicted income on the left-hand side. The bad news is that the right hand side looks a bit complex. Since $e$ is just a number we can go ahead and calculate the values in those parentheses (called "exponentiating"):

```{r}
exp(3.29)
exp(0.005)
```

That means:

$$\hat{y}_i=(26.8)*(1.005)^{x_i}$$
What we have here is a **multiplicative** relationship rather than an additive relationship. How does this changes things? Well, to see lets plug in some values for $x_i$ and see how it changes our predicted income value. An age of zero is outside the scope of our data, but lets plug it in for instructional purposes anyway:

$$\hat{y}_i=(26.8)*(1.005)^{0}=(26.8)(1)=26.8$$
So, the predicted income when $x$ is zero is just given by exponentiating the intercept. Lets try increasing age by one year:

$$\hat{y}_i=(26.8)*(1.005)^{1}=(26.8)(1.005)$$
I could go ahead and finish that multiplication, but I want to leave it here to better show the change. A one year increase increases income by a **multiplicative** factor of 1.005. In other words, a one year increase in age is associated with a 0.5% increase in income, on average. What happens if I add another year?

$$\hat{y}_i=(26.8)*(1.005)^{2}=(26.8)(1.005)(1.005)$$
Each additional year leads to a 0.5% increase in predicted income. This is what I mean by a multiplicative increase. We are no longer talking about the predicted change in income in terms of **absolute** numbers of dollars, but rather in **relative** terms of percentage increase. 

###  General form and interpretation

In general, you have the following equation when you transform your dependent variable:

$$\log(\hat{y}_i)=b_0+b_1(x_{i1})+b_2(x_{i2})+\ldots+b_p(x_{ip})$$

In order to properly interpret your results, you must exponentiate all of your slopes and the intercept. Your slopes can be interpreted as:

> The model predicts that a one-unit increase in $x_j$ is associated with a $e^{b_j}$ multiplicative increase in $y$, on average while holding all other independent variables constant. 
> The model predicts that $y$ will be $e^{b_0}$ on average when all independent variables are zero. 

Of course, just like all of our prior examples, you are responsible for converting this into sensible English. 

Lets try a fuller example where we predict log income by age, education and race at the same time:

```{r}
round(summary(lm(lincome~age+educ+race, data=politics))$coef,3)
```

Lets start by interpreting the intercept. Two of our variables, education and race, are categorical with reference categories of less than high school and white, respectively and age is a quantitative variable. In order to interpret our intercept of 2.73, we first need to exponentiate it:

```{r}
exp(2.73)
```

So we would say:

> The model predicts that a zero-year old white person with less than a high school degree will make \$15,333, on average. 

Of course, because an age of zero is outside the scope of our data, we don't put much any stock in this prediction. The more interesting numbers are the various slopes. 

Lets interpret, the age slope of 0.004. First, exponentiate:

```{r}
exp(0.004)
```

So, we would say:

> The model predicts that a one year increase in age is associated with a 0.4% increase in income, on average, among individuals of the same race and educational level. 

What about the effect for a BA degree? This is a categorical variable with less than high school as the reference, but again we need to exponentiate to get a meaningful number:

```{r}
exp(1.19)
```

This is a pretty big increase! We might say:

> The model predicts that the incomes of college graduates are 3.28 times higher than the incomes of those with less than a high school diploma, on average, holding constant race and age. 

How would we describe negative effects. Lets look at the effect of -0.23 for Hispanics. First, lets exponentiate it:

```{r}
exp(-0.23)
```

There are two ways we could describe this. Let me try both ways:

> The model predicts that the incomes of Hispanics will be 79% as high as the incomes of whites, on average, holding constant age and education.

Keep in mind that 79% as high means you are making less. If your boss comes and offers you to change your salary to 90% of what it is now, don't be fooled -- thats not a good deal. Alternatively, I could have taken 100-79=21 and said:

> The model predicts that the income of Hispanics is 21% less than the income of whites, on average, holding constant age and education. 

Always keep in mind that regardless of the type of variable involved and the direction of the relationship, we are always talking about a relative, multiplicative change in the dependent variable. In some cases, it may make more sense to describe this as a percentage gain or loss (as in the age and Hispanic case), while in other cases it may make sense to describe it in terms of how many "times more or less" (as in the case of the BA degree).

*Note: from here on out, I am throwing together information discussed in class, so the descriptions may be a bit rough. Apologies in advance*

###  Logging the independent variable 

Lets revisit the data from Preston that we looked at earlier in the term. 

```{r preston2, echo=FALSE}
par(mar=c(4,4,0.1,1))
plot(preston$inc, preston$lifeexp,
     xlab="national income, 1960",
     ylab="life expectancy, 1960", pch=21, bg="grey", las=1)
```
These data show a clearly non-linear relationship. More specifically, this is a "diminishing returns" relationship where a positive effect gets smaller in magnitude at higher levels of the independent variable. Logging the dependent variable will not help us make this relationship look more linear, but logging the independent variable will:

```{r preston3, echo=FALSE}
par(mar=c(4,4,0.1,1))
plot(log(preston$inc), preston$lifeexp,
     xlab="national income, 1960",
     ylab="life expectancy, 1960", pch=21, bg="grey", las=1)
```

How does this transformation of the independent variable affect how we interpret the results? Lets run the model:

```{r prestonmodel}
summary(lm(lifeexp~log(inc), data=preston))
```

Interpreting the slope here can be tricky. The basic point is that by logging the independent variable, the change in the independent variable is now relative as measured by a 1% increase in the independent variable (national income per capita here). The change in the dependent variable is still in absolute terms (years of life expectancy here), but in order to interpret the slope correctly you must divide it by 100 (i.e. move the decimal place two places to the left). In this case, I would say:

> The model predicts that a 1% increase in national income per capita in a country is associated with a 0.075 year increase in life expectancy on average. 

Why did I have to move the decimal place two to the left? Lets use the model to compare the predicted values of two cases, where one case has exactly 1% higher national income per capita, to see how this works. I will start the lower country at a national income per capita of \$20,000, but the results here apply regardless of what number I choose here. To see the change I subtract one predicted value from the other. 

$$
\begin{aligned} 
((14.93+7.54*\log(20200))-(14.93+7.54*\log(20000))&=7.54(\log(20200)-\log(20000))\\
&=7.54*\log(20200/20000)\\ 
&=7.54*\log(1.01)
\end{aligned}
$$

Now, it turns out that $log(1.01)$ almost exactly equals 0.01, so this is roughly equivalent to $7.54*0.01=0.0754$. This same math will be true regardless of the starting value of income chosen, so roughly speaking a 1% increase in $x$ is associated with a $b_1/100$ change in $y$. 

###  Logging both independent and dependent variables: The elasticity model

So, now we have seen that logging the dependent variable will make change in the dependent relative, and logging the independent variable will make change in the independent variable relative. It makes sense to think that if you log them both, you would get relative change in $x$ predicting relative change in $y$. Correct! This is what is called an "elasticity" model because the predicted slopes are equivalent to the concept of elasticity in economics: how much does of a percent change in $y$ results from a 1% increase in $x$. 

To show you how this works, lets try to predict movie box office returns by tomato ratings where we apply all three types of models.

```{r}
model.logy <- lm(log(BoxOffice)~TomatoRating, data=movies)
model.logx <- lm(BoxOffice~log(TomatoRating), data=movies)
model.logboth <- lm(log(BoxOffice)~log(TomatoRating), data=movies)
coef(model.logy)[2]
coef(model.logx)[2]
coef(model.logboth)[2]
```

In the first model, I am logging $y$ so I nee to exponentiate the result to interpret it. 

```{r}
exp(0.2012)
```

So, I would say:

> The model predicts that a one point increase in a movie's tomato rating is associated with a 22% increase in box office returns on average. 

In the second model, I need to move over the decimal place to the left and then say:

> The model predicts that a 1% increase in a movie's tomato rating is associated with a \$500,000 increase in box office returns. 

The third model (the elasticity model) is the easiest to interpret. It turns out that the number can be interpreted directly as the percentage change in $y$ expected for a 1 percent increase in $x$, so:

> The model predicts that a 1% increase in a movie's tomato rating is associated with a 0.85% increase in box office returns. 

Why is this the case? We can work this out from a similar mathematical exercise above. On the independent variable side a 1% increase in $x$ is still associated with a $b_1/100$ increase in the dependent variable, but that independent variable is still the log of $y$. Thus, technically you should get the result by taking $e^{b_1/100}$. However, since you are dividing by 100 that number will almost always be small enough that you can use the approximation that $b_1$ itself is the percentage increase in $y$ for the given change in $x$. 

###  The square root transformation

The log transformation is very flexible and solves multiple problems at once (non-linearity, outliers, skewness), which explains its popularity. But it breaks down in one important situation: you cannot log a variable that has zero or negative values. The negative case is not as important because generally the log transformation fixes things for variables that only take non-negative values. However, there are numerous cases where a quantitative variable can be zero as well as a positive. Lets run the same elasticity model as above on box office returns, but this time lets predict returns by the Tomato Meter rather than the Tomato Rating.

```{r error=TRUE}
summary(lm(log(BoxOffice)~log(TomatoMeter), data=movies))
```
Oh no! We got an error. The problem is that the Tomato Meter has a few cases of zero values (when a movie received zero positive reviews). The log of zero is negative infinity and that simply won't work when fitting a linear model. What can you do?

Well it turns out that the square root transformation can do much the same work as the natural logarithm. It will pull in skewness and can make non-linear relationships more linear. Since the square root of zero is a real number (zero to be precise), it will also work on variables that have legitimate zeroes. So,

```{r}
summary(lm(log(BoxOffice)~sqrt(TomatoMeter), data=movies))
```

Now we can get a result. The downside, however, is that there is [no clear and easy interpretation of how to intepret this effect](http://stats.stackexchange.com/questions/35982/how-to-interpret-regression-coefficients-when-response-was-transformed-by-the-4t).  

###  Other methods to deal with non-linearity

Transformations are one way to handle non-linearity in the relationship between $x$ and $y$. There are other common ways that this non-linearity can also be handled. We will cover the case of smoothing and polynomial regression here. Another case that we will not cover here is the use of splines, although some of the smoothing techniques covered here will use splines implicitly. 

####  Smoothing

Smoothing is primarily a graphical technique that can be used to diagnostically detect non-linearity in a relationship. Lets plot the relationship between movie tomato rating and box office returns. 

```{r echo=FALSE}
plot(movies$TomatoRating, movies$BoxOffice, pch=21, bg="grey", col=NULL,
     las=1, xlab="Tomato Rating", ylab="Box Office Returns (millions)")
```
There is so much overplotting and box office returns are so skewed here that it is quite difficult to visually see the relationship. Smoothing will help us do that. There are numerous ways that one can smooth data, but the basic idea is that you replace the $y$ value for an observation with a substitute value that incorporates information about the adjacent neighbors (in terms of $x$) for this observation. 

To see how this works, the figure below picks out one movie that had particularly anomalous box office returns given its tomato rating (shown in red) and the two movies that were most immediately adjacent to this value in terms of their tomato rating. It then takes the mean box office returns between the three movies and plots this as the smoothed mean value for the selected movie in green. 

```{r echo=FALSE}
movies2 <- movies[order(movies$TomatoRating),]
plot(movies$TomatoRating, movies$BoxOffice, pch=21, bg="grey90", col=NULL,
     las=1, xlab="Tomato Rating", ylab="Box Office Returns (millions)")
points(movies2$TomatoRating[450], movies2$BoxOffice[450], pch=21, bg="red")
text(movies2$TomatoRating[450], movies2$BoxOffice[450], movies2$Title[450], cex=0.5, pos=1)
points(movies2$TomatoRating[c(449,451)], movies2$BoxOffice[c(449,451)], pch=21, bg="grey10")
text(movies2$TomatoRating[c(449,451)], movies2$BoxOffice[c(449,451)], movies2$Title[c(449,451)], cex=0.5, pos=2)
points(movies2$TomatoRating[450], mean(movies2$BoxOffice[449:451]), pch=21, bg="green")
```

In practice this kind of mean smoothing (also called a "running average") or median smoothing only works well for time series values where there is only one unique value of $x$ for each observation, whereas we have many movies with the exact same tomato rating. In practice, a better smoothing approach for data like this is to use more complex methods for smoothing that involve splines and local polynomial regression to get predicted values. Three methods that will do this in R are shown below. For the `loess` and `smooth.spline` functions, you have to declare how many adjacent observations you want to consider as a proportion of the total dataset. I have chosen 10% and 75%. The wider you make this span, the smoother the line will get at the cost of potentially losing important spikes and dips. The "supersmoother" function `supsmu` is simpler and tries to determine the best span internally. 

```{r}
#for loess its important to first order the movies by x
movies <- movies[order(movies$TomatoRating),]
plot(movies$TomatoRating, movies$BoxOffice, pch=21, bg="grey90", col=NULL,
     las=1, xlab="Tomato Rating", ylab="Box Office Returns (millions)")
smooth.loess <- loess(BoxOffice~TomatoRating, data=movies, span=0.1)
lines(smooth.loess$x, smooth.loess$fitted, col="red", lwd=3)
smooth.loess <- loess(BoxOffice~TomatoRating, data=movies, span=0.75)
lines(smooth.loess$x, smooth.loess$fitted, col="orange", lwd=3)
lines(smooth.spline(movies$TomatoRating, movies$BoxOffice, spar=0.1), lwd=2, col="blue")
lines(smooth.spline(movies$TomatoRating, movies$BoxOffice, spar=0.75), lwd=2, col="darkgreen")
lines(supsmu(movies$TomatoRating, movies$BoxOffice), col="yellow", lwd=2)
legend(2, 700, legend=c("Lowess, 10% span", "Lowess, 75% span", "Spline smoothing, 10%", 
                        "Spline smoothing, 75%", "Supersmoother"),
       lwd=1, lty=1, col=c("red","orange","blue","darkgreen","yellow"), cex=0.6)
```

All the smoothers here indicate an exponential type relationship that would be better fit by logging the dependent variable. 

####  Polynomial Regression

A final method that can be used to fit non-linear relationships is to fit *polynomial* terms. Recall, for example, the formula:

$$y=a+bx+cx^2$$

This function defines a parabola which fits not a straight line but a curve with one point of inflection. We can fit this sort of curve in an OLS regression model by simply including the square of a variable as an additional term in the model. Before we do this it is usually a good idea to center the variable to be squared somewhere around the mean because this will reduce collinearity between the original term and its square. 

For example, I could fit a polynomial term to a model that predicts tomato meter by movie runtime. like so:

```{r}
summary(lm(TomatoMeter~I(Runtime-90)+I((Runtime-90)^2), data=movies))
```

Interpreting these numbers directly can be quite tricky. The easiest approach is often to simply graph the resulting parabola for reasonable values of $x$. I can do that here for movies:

```{r}
x <- 70:620-90
fitted <- 42.256+0.406*x-0.0004635*x^2
plot(x, fitted, type="l", lwd=2, xaxt="n", las=1,
     xlab="runtime (in minutes)", ylab="Predicted tomato meter")
ticks <- seq(from=70, to=620, by=50)-90
axis(1, at=ticks, labels=ticks+90)
abline(v=528-90, lty=2)
```

Note that we now get a curvilinear relationship where the positive effect of runtime gets smaller at higher values of runtime much like a diminishing returns relationship. In this case, the effect of runtime can even reverse direction and become negative which is not possible with a log transformation on $x$. However, its worth noting that the effect of runtime doesn't become negative until we are well outside the range of real movie values. 

You can actually mathematically figure out the exact inflection point based on the two "slopes" for the original term and its square. Given the following model:

$$y=b_0+b_1*x+b_2*x^2$$

The inflection point is given by:

$$b_1/(-2 * b_2)$$

In this case, that gives us an inflection point of:

$$0.4060329/(2*0.0004635)=438$$

However, because we subtracted 90 from each value, the actual runtime minutes for the inflection is equal to $438+90=528$. Note that this value is show on the graph above. 

---

## The i.i.d. Violation and Robust Standard Errors

---

## Sample Design and Weighting

---

## Missing Values

---

## Multicollinearity and Scale Creation

---

## Model Selection